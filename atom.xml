<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>TianYong&#39;s Blog</title>
  
  <subtitle>比你优秀的人都努力，有什么理由不努力！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://tianyong.fun/"/>
  <updated>2023-03-27T17:50:58.306Z</updated>
  <id>http://tianyong.fun/</id>
  
  <author>
    <name>TTYONG</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>大数据开发工程师-第十一周 Spark性能优化的道与术-4</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-4.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-4.html</id>
    <published>2023-03-27T17:50:58.000Z</published>
    <updated>2023-03-27T17:50:58.306Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十一周 Spark性能优化的道与术-3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-3.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-3.html</id>
    <published>2023-03-27T11:31:29.000Z</published>
    <updated>2023-03-27T17:50:18.694Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十一周-Spark性能优化的道与术-3"><a href="#第十一周-Spark性能优化的道与术-3" class="headerlink" title="第十一周 Spark性能优化的道与术-3"></a>第十一周 Spark性能优化的道与术-3</h1><h2 id="性能优化分析"><a href="#性能优化分析" class="headerlink" title="性能优化分析"></a>性能优化分析</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">一个计算任务的执行主要依赖于CPU、内存、带宽</span><br><span class="line">Spark是一个基于内存的计算引擎，所以对它来说，影响最大的可能就是内存，一般我们的任务遇到了性能瓶颈大概率都是内存的问题，当然了CPU和带宽也可能会影响程序的性能，这个情况也不是没有的，只是比较少。</span><br><span class="line">Spark性能优化，其实主要就是在于对内存的使用进行调优。通常情况下，如果你的Spark程序计算的数据量比较小，并且你的内存足够使用，那么只要网络不至于卡死，一般是不会有大的性能问题的。但是Spark程序的性能问题往往出现在针对大数据量进行计算（比如上亿条数的数据，或者上T规模的数据），这个时候如果内存分配不合理就会比较慢，所以，Spark性能优化，主要是对内存进行优化。</span><br></pre></td></tr></table></figure><h2 id="内存都去哪了"><a href="#内存都去哪了" class="headerlink" title="内存都去哪了"></a>内存都去哪了</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. 每个Java对象，都有一个对象头，会占用16个字节，主要是包括了一些对象的元信息，比如指向它的类的指针。如果一个对象本身很小，比如就包括了一个int类型的field，那么它的对象头实际上比对象自身还要大。</span><br><span class="line">2. Java的String对象的对象头，会比它内部的原始数据，要多出40个字节。因为它内部使用char数组来保存内部的字符序列，并且还要保存数组长度之类的信息。</span><br><span class="line">3. Java中的集合类型，比如HashMap和LinkedList，内部使用的是链表数据结构，所以对链表中的每一个数据，都使用了Entry对象来包装。Entry对象不光有对象头，还有指向下一个Entry的指针，通常占用8个字节。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">所以把原始文件中的数据转化为内存中的对象之后，占用的内存会比原始文件中的数据要大</span><br><span class="line"></span><br><span class="line">那我如何预估程序会消耗多少内存呢？</span><br><span class="line">通过cache方法，可以看到RDD中的数据cache到内存中之后占用多少内存，这样就能看出了</span><br><span class="line">代码如下：这个测试代码就只写一个scala版本的了</span><br></pre></td></tr></table></figure><h3 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：测试内存占用情况</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestMemoryScala</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">     <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">     conf.setAppName(<span class="string">"TestMemoryScala"</span>)</span><br><span class="line">     .setMaster(<span class="string">"local"</span>)</span><br><span class="line">     <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">     <span class="keyword">val</span> dataRDD = sc.textFile(<span class="string">"hdfs://bigdata01:9000/hello_10000000.dat"</span>).cache()</span><br><span class="line">     <span class="keyword">val</span> count = dataRDD.count()</span><br><span class="line">         println(count)</span><br><span class="line">     <span class="comment">//while循环是为了保证程序不结束，方便在本地查看4040页面(在本地运行时的spark web页面)中的storage信息</span></span><br><span class="line">     <span class="keyword">while</span>(<span class="literal">true</span>)&#123;</span><br><span class="line">     ;</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271945127.png" alt="image-20230327194520543"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271944394.png" alt="image-20230327194456852"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">执行代码，访问localhost的4040端口界面</span><br><span class="line">这个界面其实就是spark的任务界面，在本地运行任务的话可以直接访问4040界面查看</span><br><span class="line">点击stages可以看到任务的原始输入数据是多大</span><br><span class="line"></span><br><span class="line">点击storage可以看到将数据加载到内存，生成RDD之后的大小</span><br><span class="line"></span><br><span class="line">这样我们就能知道这一份数据在RDD中会占用多少内存了，这样在使用的时候，如果想要把数据全部都加载进内存，就需要给这个任务分配这么多内存了，当然了你分配少一些也可以，只不过这样计算效率会变低，因为RDD中的部分数据内存放不下就会放到磁盘了。</span><br></pre></td></tr></table></figure><h2 id="性能优化方案"><a href="#性能优化方案" class="headerlink" title="性能优化方案"></a>性能优化方案</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">下面我们通过这几个方式来实现对Spark程序的性能优化</span><br><span class="line">    高性能序列化类库</span><br><span class="line">    持久化或者checkpoint</span><br><span class="line">    JVM垃圾回收调优</span><br><span class="line">    提高并行度</span><br><span class="line">    数据本地化</span><br><span class="line">    算子优化</span><br></pre></td></tr></table></figure><h3 id="高性能序列化类库"><a href="#高性能序列化类库" class="headerlink" title="高性能序列化类库"></a>高性能序列化类库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">在任何分布式系统中，序列化都是扮演着一个重要的角色的。</span><br><span class="line">如果使用的序列化技术，在执行序列化操作的时候很慢，或者是序列化后的数据还是很大，那么会让分布式应用程序的性能下降很多。所以，进行Spark性能优化的第一步，就是进行序列化的性能优化。</span><br><span class="line"></span><br><span class="line">Spark默认会在一些地方对数据进行序列化，如果我们的算子函数使用到了外部的数据（比如Java中的自定义类型），那么也需要让其可序列化，否则程序在执行的时候是会报错的，提示没有实现序列化，这个一定要注意。</span><br><span class="line"></span><br><span class="line">原因是这样的：</span><br><span class="line">因为Spark的初始化工作是在Driver进程中进行的，但是实际执行是在Worker节点的Executor进程中进行的；当Executor端需要用到Driver端封装的对象时，就需要把Driver端的对象通过序列化传输到Executor端，这个对象就需要实现序列化。</span><br><span class="line">否则会报错，提示对象没有实现序列化</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意了，其实遇到这种没有实现序列化的对象，解决方法有两种</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. 如果此对象可以支持序列化，则将其实现Serializable接口，让它支持序列化</span><br><span class="line">2. 如果此对象不支持序列化，针对一些数据库连接之类的对象，这种对象是不支持序列化的，所以可以把这个代码放到算子内部，这样就不会通过driver端传过去了，它会直接在executor中执行。</span><br><span class="line">Spark对于序列化的便捷性和性能进行了一个取舍和权衡。默认情况下，Spark倾向于序列化的便捷性，使用了Java自身提供的序列化机制——基于ObjectInputStream和 ObjectOutputStream的序列化机制，因为这种方式是Java原生提供的，使用起来比较方便，但是Java序列化机制的性能并不高。序列化的速度相对较慢，而且序列化以后的数据，相对来说还是比较大，比较占空间。所以，如果你的Spark应用程序对内存很敏感，那默认的Java序列化机制并不是最好的选择。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Spark实际上提供了两种序列化机制：</span><br><span class="line">Java序列化机制和Kryo序列化机制</span><br><span class="line">Spark只是默认使用了java这种序列化机制</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Java序列化机制：默认情况下，Spark使用Java自身的ObjectInputStream和ObjectOutputStream机制进行对象的序列化。只要你的类实现了Serializable接口，那么都是可以序列化的。Java序列化机制的速度比较慢，而且序列化后的数据占用的内存空间比较大，这是它的缺点</span><br><span class="line"></span><br><span class="line">Kryo序列化机制：Spark也支持使用Kryo序列化。Kryo序列化机制比Java序列化机制更快，而且序列化后的数据占用的空间更小，通常比Java序列化的数据占用的空间要小10倍左右。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Kryo序列化机制之所以不是默认序列化机制的原因：</span><br><span class="line"></span><br><span class="line">第一点：因为有些类型虽然实现了Seriralizable接口，但是它也不一定能够被Kryo进行序列化；</span><br><span class="line">第二点：如果你要得到最佳的性能，Kryo还要求你在Spark应用程序中，对所有你需要序列化的类型都进行手工注册，这样就比较麻烦了</span><br><span class="line"></span><br><span class="line">如果要使用Kryo序列化机制</span><br><span class="line">首先要用SparkConf设置spark.serializer的值为 org.apache.spark.serializer.KryoSerializer，就是将Spark的序列化器设置为KryoSerializer。这样，Spark在进行序列化时，就会使用Kryo进行序列化</span><br><span class="line">了。使用Kryo时针对需要序列化的类，需要预先进行注册，这样才能获得最佳性能——如果不注册的话，Kryo也能正常工作，只是Kryo必须时刻保存类型的全类名，反而占用不少内存。</span><br><span class="line">Spark默认对Scala中常用的类型在Kryo中做了注册，但是，如果在自己的算子中，使用了外部的自定义类型的对象，那么还是需要对其进行注册。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">注册自定义的数据类型格式：</span><br><span class="line">conf.registerKryoClasses(...)</span><br><span class="line"></span><br><span class="line">注意：如果要序列化的自定义的类型，字段特别多，此时就需要对Kryo本身进行优化，因为Kryo内部的缓存可能不够存放那么大的class对象</span><br><span class="line"></span><br><span class="line">需要调用SparkConf.set()方法，设置spark.kryoserializer.buffer.mb参数的值，将其调大，默认值为2，单位是MB ，也就是说最大能缓存2M的对象，然后进行序列化。可以在必要时将其调大。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">什么场景下适合使用Kryo序列化？</span><br><span class="line"></span><br><span class="line">一般是针对一些自定义的对象，例如我们自己定义了一个对象，这个对象里面包含了几十M，或者上百M的数据，然后在算子函数内部，使用到了这个外部的大对象</span><br><span class="line">如果默认情况下，让Spark用java序列化机制来序列化这种外部的大对象，那么就会导致序列化速度比较慢，并且序列化以后的数据还是比较大。</span><br><span class="line"></span><br><span class="line">所以，在这种情况下，比较适合使用Kryo序列化类库，来对外部的大对象进行序列化，提高序列化速度，减少序列化后的内存空间占用。</span><br><span class="line">用代码实现一个案例：</span><br><span class="line">scala代码如下：</span><br></pre></td></tr></table></figure><h4 id="使用kryo实现序列化"><a href="#使用kryo实现序列化" class="headerlink" title="使用kryo实现序列化"></a>使用kryo实现序列化</h4><h5 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> com.esotericsoftware.kryo.<span class="type">Kryo</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.serializer.<span class="type">KryoRegistrator</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.<span class="type">StorageLevel</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：Kryo序列化的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KryoSerScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">     <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">     conf.setAppName(<span class="string">"KryoSerScala"</span>)</span><br><span class="line">     .setMaster(<span class="string">"local"</span>)</span><br><span class="line">     <span class="comment">//指定使用kryo序列化机制，注意：如果使用了registerKryoClasses，其实这一行设置可以省略的</span></span><br><span class="line">     .set(<span class="string">"spark.serializer"</span>,<span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line">     .registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">Person</span>]))<span class="comment">//注册自定义的数据类型(Array里也可以传多个)，注册了性能高一些</span></span><br><span class="line">     <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">     <span class="keyword">val</span> dataRDD = sc.parallelize(<span class="type">Array</span>(<span class="string">"hello you"</span>,<span class="string">"hello me"</span>))</span><br><span class="line">     <span class="keyword">val</span> wordsRDD = dataRDD.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">     <span class="keyword">val</span> personRDD = wordsRDD.map(word=&gt;<span class="type">Person</span>(word,<span class="number">18</span>)).persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY_SER</span>)</span><br><span class="line">     personRDD.foreach(println(_))</span><br><span class="line">     <span class="comment">//while循环是为了保证程序不结束，方便在本地查看4040页面中的storage信息</span></span><br><span class="line">     <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">     ;</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">                                                                 <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>,age: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Serializable</span></span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">执行任务，然后访问localhost的4040界面</span><br><span class="line">在界面中可以看到cache的数据大小是 31 字节。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303272100115.png" alt="image-20230327210029655"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">那我们把kryo序列化设置去掉，使用默认的java序列化看一下效果</span><br><span class="line">修改代码，注释掉这两行代码即可</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;.set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">&#x2F;&#x2F;.registerKryoClasses(Array(classOf[Person]))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">运行任务，再访问4040界面</span><br><span class="line">发现此时占用的内存空间是 138 字节，比使用kryo的方式内存空间多占用了将近5倍。</span><br><span class="line">所以从这可以看出来，使用 kryo 序列化方式对内存的占用会降低很多。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303272101635.png" alt="image-20230327210118413"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：如果我们只是将spark的序列化机制改为了kryo序列化，但是没有对使用到的自定义类型手工进行注册，那么此时内存的占用会介于前面两种情况之间</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">修改代码，只注释掉registerKryoClasses这一行代码</span><br><span class="line"></span><br><span class="line">.set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">&#x2F;&#x2F;.registerKryoClasses(Array(classOf[Person]))&#x2F;&#x2F;注册自定义的数据类型</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">运行任务，再访问4040界面</span><br><span class="line">发现此时的内存占用为123字节，介于前面的31字节和138字节之间。</span><br><span class="line">所以从这可以看出来，在使用kryo序列化的时候，针对自定义的类型最好是手工注册一下，否则就算开启了kryo序列化，性能的提升也是有限的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303272103931.png" alt="image-20230327210309583"></p><h5 id="java"><a href="#java" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> com.esotericsoftware.kryo.Kryo;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.serializer.KryoRegistrator;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.StorageLevel;</span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：Kryo序列化的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KryoSerjava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         <span class="comment">//创建SparkContext：</span></span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setAppName(<span class="string">"KryoSerjava"</span>)</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         .set(<span class="string">"spark.serializer"</span>, <span class="string">"org.apache.spark.serializer.KryoSer</span></span><br><span class="line"><span class="string">         .set("</span>spark.kryo.classesToRegister<span class="string">", "</span>com.imooc.java.Person<span class="string">");</span></span><br><span class="line"><span class="string">         JavaSparkContext sc = new JavaSparkContext(conf);</span></span><br><span class="line"><span class="string">         JavaRDD&lt;String&gt; dataRDD = sc.parallelize(Arrays.asList("</span>hello you<span class="string">", "</span>hello me<span class="string">"));</span></span><br><span class="line"><span class="string">         JavaRDD&lt;String&gt; wordsRDD = dataRDD.flatMap(new FlatMapFunction&lt;String,String&gt;()&#123;</span></span><br><span class="line"><span class="string">         @Override</span></span><br><span class="line"><span class="string">         public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span></span><br><span class="line"><span class="string">         return Arrays.asList(line.split("</span> <span class="string">")).iterator();</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">         &#125;);</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">         JavaRDD&lt;Person&gt; personRDD = wordsRDD.map(new Function&lt;String, Person&gt;</span></span><br><span class="line"><span class="string">         @Override</span></span><br><span class="line"><span class="string">         public Person call(String word) throws Exception &#123;</span></span><br><span class="line"><span class="string">         return new Person(word, 18);</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">         &#125;).persist(StorageLevel.MEMORY_ONLY_SER());</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">         personRDD.foreach(new VoidFunction&lt;Person&gt;() &#123;</span></span><br><span class="line"><span class="string">         @Override</span></span><br><span class="line"><span class="string">         public void call(Person person) throws Exception &#123;</span></span><br><span class="line"><span class="string">         System.out.println(person);</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">         &#125;);</span></span><br><span class="line"><span class="string">         while (true)&#123;</span></span><br><span class="line"><span class="string">         ;</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">class Person implements Serializable&#123;</span></span><br><span class="line"><span class="string">    private String name;</span></span><br><span class="line"><span class="string">    private int age;</span></span><br><span class="line"><span class="string">    Person(String name,int age)&#123; // 这里讲可以通过什么自动生成，没听清</span></span><br><span class="line"><span class="string">        this.name = name;</span></span><br><span class="line"><span class="string">        this.age = age;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    @Override</span></span><br><span class="line"><span class="string">    public String toString() &#123; // alt+shift+s,右键generate-&gt;toString</span></span><br><span class="line"><span class="string">        return "</span>Person&#123;<span class="string">" +</span></span><br><span class="line"><span class="string">            "</span>name=<span class="string">'" + name + '</span>\<span class="string">''</span> +</span><br><span class="line">            <span class="string">", age="</span> + age +</span><br><span class="line">            <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="持久化或者checkpoint"><a href="#持久化或者checkpoint" class="headerlink" title="持久化或者checkpoint"></a>持久化或者checkpoint</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">针对程序中多次被transformation或者action操作的RDD进行持久化操作，避免对一个RDD反复进行计算，再进一步优化，使用Kryo序列化的持久化级别，减少内存占用</span><br><span class="line">为了保证RDD持久化数据在可能丢失的情况下还能实现高可靠，则需要对RDD执行Checkpoint操作</span><br><span class="line">这两个操作我们前面讲过了，在这就不再演示了</span><br></pre></td></tr></table></figure><h3 id="JVM垃圾回收调优"><a href="#JVM垃圾回收调优" class="headerlink" title="JVM垃圾回收调优"></a>JVM垃圾回收调优</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">由于Spark是基于内存的计算引擎，RDD缓存的数据，以及算子执行期间创建的对象都是放在内存中的，所以针对Spark任务如果内存设置不合理会导致大部分时间都消耗在垃圾回收上</span><br><span class="line">对于垃圾回收来说，最重要的就是调节RDD缓存占用的内存空间，和算子执行时创建的对象占用的内存空间的比例。</span><br><span class="line">默认情况下，Spark使用每个executor 60%的内存空间来缓存RDD，那么只有40%的内存空间来存放算子执行期间创建的对象</span><br><span class="line">在这种情况下，可能由于内存空间的不足，并且算子对应的task任务在运行时创建的对象过大，那么一旦发现40%的内存空间不够用了，就会触发Java虚拟机的垃圾回收操作。</span><br><span class="line"></span><br><span class="line">因此在极端情况下，垃圾回收操作可能会被频繁触发。</span><br><span class="line">在这种情况下，如果发现垃圾回收频繁发生。那么就需要对这个比例进行调优了， spark.storage.memoryFraction参数的值默认是0.6。</span><br><span class="line">使用SparkConf().set(&quot;spark.storage.memoryFraction&quot;, &quot;0.5&quot;) 可以进行修改，就是将RDD缓存占用内存空间的比例降低为 50% ，从而提供更多的内存空间来保存task运行时创建的对象。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">因此，对于RDD持久化而言，完全可以使用Kryo序列化，加上降低其executor内存占比的方式，来减少其内存消耗。给task提供更多的内存，从而避免task在执行时频繁触发垃圾回收。</span><br><span class="line">我们可以对task的垃圾回收进行监测，在spark的任务执行界面，可以查看每个task执行消耗的时间，以及task gc消耗的时间。</span><br><span class="line"></span><br><span class="line">重新向集群中提交checkpoint的代码，查看spark任务的task指标信息</span><br><span class="line">确保Hadoop集群、yarn的historyserver进程以及spark的historyserver进程是正常运行的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">删除checkpoint任务的输出目录</span><br><span class="line">[root@bigdata04 sparkjars]# hdfs dfs -rm -r &#x2F;out-chk001</span><br><span class="line"></span><br><span class="line">[root@bigdata04 sparkjars]# sh -x checkPointJob.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">点击生成的第一个job，再点击进去查看这个job的stage，进入第一个stage，查看task的执行情况，看这里面的GC time的数值会不会比较大，最直观的就是如果gc time这里标红了，则说明gc时间过长。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303272230037.png" alt="image-20230327223033714"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">上面这个是分任务查看，其实还可以查看全局的，看Executor进程中整个任务执行总时间和gc的消耗时间。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303272232315.png" alt="image-20230327223148318"></p><h4 id="java-GC"><a href="#java-GC" class="headerlink" title="java GC"></a>java GC</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">既然说到了Java中的GC，那我们就需要说道说道了。</span><br><span class="line">Java堆空间被划分成了两块空间：一个是年轻代，一个是老年代。</span><br><span class="line">年轻代放的是短时间存活的对象</span><br><span class="line">老年代放的是长时间存活的对象。</span><br><span class="line">年轻代又被划分了三块空间， Eden、Survivor1、Survivor2</span><br><span class="line"></span><br><span class="line">来看一下这个内存划分比例图</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303272234159.png" alt="image-20230327223408841"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">年轻代占堆内存的1&#x2F;3，老年代占堆内存的2&#x2F;3</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">其中年轻代又被划分了三块， Eden，Survivor1，Survivor2 的比例为 8:1:1</span><br><span class="line">Eden区域和Survivor1区域用于存放对象，Survivor2区域备用。</span><br><span class="line"></span><br><span class="line">我们创建的对象，首先会放入Eden区域，如果Eden区域满了，那么就会触发一次Minor GC，进行年轻代的垃圾回收(其实就是回收Eden区域内没有人使用的对象)，然后将存活的对象存入Survivor1区域，再创建对象的时候继续放入Eden区域。</span><br><span class="line"></span><br><span class="line">第二次Eden区域满了，那么Eden和Survivor1区域中存活的对象，会一块被移动到Survivor2区域中。然后Survivor1和Survivor2的角色调换，Survivor1变成了备用。</span><br><span class="line"></span><br><span class="line">当第三次Eden区域再满了的时候，Eden和Survivor2区域中存活的对象，会一块被移动到Survivor1区域中，按照这个规律进行循环</span><br><span class="line"></span><br><span class="line">如果一个对象，在年轻代中，撑过了多次垃圾回收(默认是15次)，都没有被回收掉，那么会被认为是长时间存活的，此时就会被移入老年代。此外，如果在将Eden和Survivor1中的存活对象，尝试放入Survivor2中时，发现Survivor2放满了，那么会直接放入老年代。此时就出现了，短时间存活的对象，也会进入老年代的问题。</span><br><span class="line"></span><br><span class="line">如果老年代的空间满了，那么就会触发Full GC，进行老年代的垃圾回收操作，如果执行Full GC也释放不了内存空间，就会报内存溢出的错误了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意了，Full GC是一个重量级的垃圾回收，Full GC执行的时候，程序是处于暂停状态的，这样会非常影响性能。</span><br></pre></td></tr></table></figure><h4 id="spark-GC调优方案"><a href="#spark-GC调优方案" class="headerlink" title="spark GC调优方案"></a>spark GC调优方案</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Spark中，垃圾回收调优的目标就是，只有真正长时间存活的对象，才能进入老年代，短时间存活的对象，只能呆在年轻代。不能因为某个Survivor区域空间不够，在Minor GC时，就进入了老年代，从而造成短时间存活的对象，长期呆在老年代中占据了空间，这样Full GC时要回收大量的短时间存活的对象，导致Full GC速度缓慢。</span><br><span class="line"></span><br><span class="line">如果发现，在task执行期间，大量full gc发生了，那么说明，年轻代的Eden区域，给的空间不够大。此时可以执行一些操作来优化垃圾回收行为</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1：最直接的就是提高Executor的内存</span><br><span class="line">在spark-submit中通过参数指定executor的内存</span><br><span class="line">--executor-memory 1G </span><br><span class="line"></span><br><span class="line">2：调整Eden与s1和s2的比值【一般情况下不建议调整这块的比值】</span><br><span class="line">-XX:NewRatio&#x3D;4：设置年轻代(包括Eden和两个Survivor区)与年老代的比值(除去持久代).设置为4,则年轻代与年老代所占比值为1:4,年轻代占整个堆栈的1&#x2F;5</span><br><span class="line">-XX:SurvivorRatio&#x3D;4：设置年轻代中Eden区与Survivor区的大小比值.设置为4,则两个Survivor区与一个Eden区的比值为2:4,一个Survivor区占整个年轻代的1&#x2F;6</span><br><span class="line">具体使用的时候在 spark-submit 脚本中通过 --conf 参数设置即可</span><br><span class="line">--conf &quot;spark.executor.extraJavaOptions&#x3D; -XX:SurvivorRatio&#x3D;4 -XX:NewRatio&#x3D;4&quot;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">其实最直接的就是增加Executor的内存，如果这个内存上不去，其它的修改都是徒劳。</span><br><span class="line">举个例子就是说，一个20岁的成年人和一个3岁的小孩</span><br><span class="line">3岁的小孩掌握再多的格斗技巧都没有用，在绝对的实力面前一切都是花架子。</span><br><span class="line">所以说我们一般很少需要去调整Eden、s1、s2的比值，一般都是直接增加Executor的内存比较靠谱。</span><br></pre></td></tr></table></figure><h3 id="提高并行度"><a href="#提高并行度" class="headerlink" title="提高并行度"></a>提高并行度</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">实际上Spark集群的资源并不一定会被充分利用到，所以要尽量设置合理的并行度，来充分地利用集群的资源，这样才能提高Spark程序的性能。</span><br><span class="line"></span><br><span class="line">Spark会自动设置以文件作为输入源的RDD的并行度，依据其大小，比如HDFS，就会给每一个block创建一个partition，也依据这个设置并行度。对于reduceByKey等会发生shuffle操作的算子，会使用并行度最大的父RDD的并行度</span><br><span class="line"></span><br><span class="line">可以手动使用textFile()、parallelize()等方法的第二个参数来设置并行度；也可以使用spark.default.parallelism参数，来设置统一的并行度。Spark官方的推荐是，给集群中的每个cpu core设置2~3个task。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">下面来举个例子</span><br><span class="line">我在spark-submit 脚本中给任务设置了5个executor，每个executor，设置了2个cpu core</span><br><span class="line">spark-submit \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--num-executors 5 \   &#x2F;&#x2F; 为job设置5个executor</span><br><span class="line">--executor-cores 2 \   &#x2F;&#x2F; 分配两个CPU</span><br><span class="line">.....</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">此时，如果我在代码中设置了默认并行度为5</span><br><span class="line">conf.set(&quot;spark.default.parallelism&quot;,&quot;5&quot;)</span><br><span class="line"></span><br><span class="line">这个参数设置完了以后，也就意味着所有RDD的partition都被设置成了5个，针对RDD的每一个partition，spark会启动一个task来进行计算，所以对于所有的算子操作，都只会创建5个task来处理对应的RDD中的数据。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">但是注意了，我们前面在spark-submit脚本中设置了5个executor，每个executor 2个cpu core，所以这个时候spark其实会向yarn集群申请10个cpu core，但是我们在代码中设置了默认并行度为5，只会产生5个task，一个task使用一个cpu core，那也就意味着有5个cpu core是空闲的，这样申请的资源就浪费了一半。</span><br><span class="line"></span><br><span class="line">其实最好的情况，就是每个cpu core都不闲着，一直在运行，这样可以达到资源的最大使用率，其实让一个cpu core运行一个task都是有点浪费的，官方也建议让每个cpu core运行2~3个task，这样可以充分压榨CPU的性能</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">为什么这样说呢？</span><br><span class="line"></span><br><span class="line">是这样的，因为每个task执行的顺序和执行结束的时间很大概率是不一样的，如果正好有10个cpu，运行10个taks，那么某个task可能很快就执行完了，那么这个CPU就空闲下来了，这样资源就浪费了。</span><br><span class="line">所以说官方推荐，给每个cpu分配2~3个task是比较合理的，可以充分利用CPU资源，发挥它最大的价值。</span><br></pre></td></tr></table></figure><h4 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">下面我们来实际写个案例看一下效果</span><br><span class="line">Scala代码如下：</span><br><span class="line"></span><br><span class="line">package com.imooc.scala</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line">&#x2F;**</span><br><span class="line"> * 需求：设置并行度</span><br><span class="line"> * 1：可以在textFile或者parallelize等方法的第二个参数中设置并行度</span><br><span class="line"> * 2：或者通过spark.default.parallelism参数统一设置并行度</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">object MoreParallelismScala &#123;</span><br><span class="line">     def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">     val conf &#x3D; new SparkConf()</span><br><span class="line">     conf.setAppName(&quot;MoreParallelismScala&quot;)</span><br><span class="line">     &#x2F;&#x2F;.setMaster(&quot;local&quot;)</span><br><span class="line">    &#x2F;&#x2F;设置全局并行度</span><br><span class="line">     conf.set(&quot;spark.default.parallelism&quot;,&quot;5&quot;)</span><br><span class="line">     val sc &#x3D; new SparkContext(conf)</span><br><span class="line">     val dataRDD &#x3D; sc.parallelize(Array(&quot;hello&quot;,&quot;you&quot;,&quot;hello&quot;,&quot;me&quot;,&quot;hehe&quot;,&quot;hello&quot;,&quot;you&quot;,&quot;hello&quot;,&quot;me&quot;,&quot;hehe&quot;))</span><br><span class="line">     dataRDD.map((_,1))</span><br><span class="line">     .reduceByKey(_ + _)</span><br><span class="line">     .foreach(println(_))</span><br><span class="line">     sc.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：设置并行度</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MoreParallelismJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">     SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">     conf.setAppName(<span class="string">"MoreParallelismJava"</span>);</span><br><span class="line">     <span class="comment">//设置全局并行度</span></span><br><span class="line">     conf.set(<span class="string">"spark.default.parallelism"</span>,<span class="string">"5"</span>);</span><br><span class="line">     JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">     JavaRDD&lt;String&gt; dataRDD = sc.parallelize(Arrays.asList(<span class="string">"hello"</span>,<span class="string">"you"</span>,<span class="string">"hello"</span>,<span class="string">"me"</span>,<span class="string">"hehe"</span>,<span class="string">"hello"</span>,<span class="string">"you"</span>,<span class="string">"hello"</span>,<span class="string">"me"</span>,<span class="string">"hehe"</span>));</span><br><span class="line">     dataRDD.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">     <span class="meta">@Override</span></span><br><span class="line">     <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception </span></span><br><span class="line"><span class="function">     return new Tuple2&lt;String, Integer&gt;<span class="params">(word,<span class="number">1</span>)</span></span>;</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;).reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">     <span class="meta">@Override</span></span><br><span class="line">     <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer i1, Integer i2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">     <span class="keyword">return</span> i1 + i2;</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;).foreach(<span class="keyword">new</span> VoidFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">     <span class="meta">@Override</span></span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Tuple2&lt;String, Integer&gt; tup)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">     System.out.println(tup);</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;);</span><br><span class="line">     sc.stop();                                                      &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">对代码编译打包</span><br><span class="line">spark-submit脚本内容如下：</span><br><span class="line"></span><br><span class="line">[root@bigdata04 sparkjars]# vi moreParallelismJob.sh </span><br><span class="line">spark-submit \</span><br><span class="line">--class com.imooc.scala.MoreParallelismScala \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--num-executors 5 \</span><br><span class="line">--executor-cores 2 \</span><br><span class="line">db_spark-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280039960.png" alt="image-20230328003937453"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">任务提交到集群运行之后，查看spark的任务界面(job)</span><br><span class="line">先看executors，这里显示了4个executor和1个driver进程，为什么不是5个executor进程呢？</span><br><span class="line"></span><br><span class="line">是因为我们现在使用的是yarn-cluster模式，driver进程运行在集群内部，所以它占了一个executor，如果使用的是yarn-client模式，就会产生5个executor和1个单独的driver进程。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280042154.png" alt="image-20230328004230697"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">然后去看satges界面，两个Stage都是5个task并行执行，这5个task会使用5个cpu，但是我们给这个任务申请了10个cpu，所以就有5个是空闲的了(这里没考虑driver的占用)。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280057687.png" alt="image-20230328005706629"></p><h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280054889.png" alt="image-20230328005414569"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">当在sparkContext生成对象后，再设置默认并行度会出现问题</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280055569.png" alt="image-20230328005531042"></p><h4 id="提高性能"><a href="#提高性能" class="headerlink" title="提高性能"></a>提高性能</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">如果想要最大限度利用CPU的性能，至少将spark.default.parallelism的值设置为10，这样可以实现一个cpu运行一个task，其实官方推荐是设置为20或者30。</span><br><span class="line">其实这个参数也可以在spark-submit脚本中动态设置，通过--conf参数设置，这样就比较灵活了。</span><br><span class="line"></span><br><span class="line">注意：此时需要将代码中设置spark.default.parallelism的配置注释掉</span><br><span class="line">&#x2F;&#x2F;conf.set(&quot;spark.default.parallelism&quot;,&quot;5&quot;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">为了看起来更清晰，在这我们使用 yarn-client 模式，这样driver就不会占用我们的分配的executor了</span><br><span class="line"></span><br><span class="line">[root@bigdata04 sparkjars]# vi moreParallelismJob2.sh </span><br><span class="line">spark-submit \</span><br><span class="line">--class com.imooc.scala.MoreParallelismScala \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--num-executors 5 \</span><br><span class="line">--executor-cores 2 \</span><br><span class="line">--conf &quot;spark.default.parallelism&#x3D;10&quot; \</span><br><span class="line">db_spark-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">由于修改了代码，所以需要重新编译，打包，执行</span><br><span class="line">执行结束后再来查看spark的任务界面，可以看到此时有10个task并行执行</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280117859.png" alt="image-20230328011738873"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280118186.png" alt="image-20230328011801559"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280116405.png" alt="image-20230328011600636"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这就是并行度相关的设置</span><br><span class="line">接下来我们来看一个图，加深一下理解</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280126634.png" alt="image-20230328012605704"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个图中描述的就是刚才我们演示的两种情况下Executor和Task之间的关系</span><br></pre></td></tr></table></figure><h4 id="spark-submit常用参数"><a href="#spark-submit常用参数" class="headerlink" title="spark-submit常用参数"></a>spark-submit常用参数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">最后我们来分析总结一下spark-submit脚本中经常配置的一些参数</span><br><span class="line"></span><br><span class="line">--name mySparkJobName：指定任务名称(代码里也可以设置)</span><br><span class="line">--class com.imooc.scala.xxxxx ：指定入口类</span><br><span class="line">--master yarn ：指定集群地址(standalone)，on yarn模式指定yarn</span><br><span class="line">--deploy-mode cluster ：client代表yarn-client，cluster代表yarn-cluster</span><br><span class="line">--executor-memory 1G ：executor进程的内存大小，实际工作中设置2~4G即可</span><br><span class="line">--num-executors 2 ：分配多少个executor进程</span><br><span class="line">--executor-cores 2 : 一个executor进程分配多少个cpu core</span><br><span class="line">--driver-cores 1 ：(如果使用yarn-cluster模式，这里不用设置也行，直接按executor的配置来)driver进程分配多少cpu core，默认为1即可</span><br><span class="line">--driver-memory 1G：(如果使用yarn-cluster模式，这里不用设置也行，直接按executor的配置来)driver进程的内存，如果需要使用类似于collect之类的action算子向Driver端拉取数据，则这里可以设置大一些</span><br><span class="line">--jars fastjson.jar,abc.jar 在这里可以设置job依赖的第三方jar包(pom里spark-core没提供的)【不建议把第三方依赖打入程序的jar包中，一方面会导致jar变大；另一方面，同一个项目组同事用的第三方依赖版本问题；还有这里可以使用本地路径，或者hdfs路径(建议使用hdfs路径，因为使用本地路径依赖，还是会读取到hdfs上)】</span><br><span class="line">--conf &quot;spark.default.parallelism&#x3D;10&quot;：可以动态指定一些spark任务的参数，指定多个参</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">最后注意一点：针对 --num-executors 和 --executor-cores 的设置</span><br><span class="line">大家看这两种方式设置有什么区别：</span><br><span class="line">第一种方式：</span><br><span class="line">--num-executors 2</span><br><span class="line">--executor-cores 1</span><br><span class="line">第二种方式：</span><br><span class="line">--num-executors 1</span><br><span class="line">--executor-cores 2</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">这两种设置最终都会向集群申请2个cpu core，可以并行运行两个task，但是这两种设置方式有什么区别呢？</span><br><span class="line"></span><br><span class="line">第一种方法：多executor模式</span><br><span class="line">由于每个executor只分配了一个cpu core，我们将无法利用在同一个JVM中运行多个任务的优点。我们假设这两个executor是在两个节点中启动的，那么针对广播变量这种操作，将在两个节点的中都复制1份，最终会复制两份</span><br><span class="line"></span><br><span class="line">第二种方法：多core模式</span><br><span class="line">此时一个executor中会有2个cpu core，这样可以利用同一个JVM中运行多个任务的优点，并且针对广播变量的这种操作，只会在这个executor对应的节点中复制1份即可。</span><br><span class="line"></span><br><span class="line">那是不是我可以给一个executor分配很多的cpu core，也不是的，因为一个executor的内存大小是固定的，如果在里面运行过多的task可能会导致内存不够用，所以这块一般在工作中我们会给一个executor分配 2~4G 内存，对应的分配 2~4个cpu core。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十一周 Spark性能优化的道与术-2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-checkpoint.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-checkpoint.html</id>
    <published>2023-03-27T06:42:14.000Z</published>
    <updated>2023-03-27T12:03:46.618Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十一周-Spark性能优化的道与术-2"><a href="#第十一周-Spark性能优化的道与术-2" class="headerlink" title="第十一周 Spark性能优化的道与术-2"></a>第十一周 Spark性能优化的道与术-2</h1><h2 id="checkpoint概述"><a href="#checkpoint概述" class="headerlink" title="checkpoint概述"></a>checkpoint概述</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">checkpoint，是Spark提供的一个比较高级的功能。有时候，我们的Spark任务，比较复杂，从初始化RDD开始，到最后整个任务完成，有比较多的步骤，比如超过10个transformation算子。而且，整个任务运行的时间也特别长，比如通常要运行1~2个小时。在这种情况下，就比较适合使用checkpoint功能了。</span><br><span class="line"></span><br><span class="line">因为对于特别复杂的Spark任务，有很高的风险会出现某个要反复使用的RDD因为节点的故障导致丢失，虽然之前持久化过，但是还是导致数据丢失了。那么也就是说，出现失败的时候，没有容错机制，所以当后面的transformation算子，又要使用到该RDD时，就会发现数据丢失了，此时如果没有进行容错处理的话，那么就需要再重新计算一次数据了。</span><br><span class="line">所以针对这种Spark Job，如果我们担心某些关键的，在后面会反复使用的RDD，因为节点故障导致数据丢失，那么可以针对该RDD启动checkpoint机制，实现容错和高可用</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">那如何使用checkPoint呢？</span><br><span class="line">首先要调用SparkContext的setCheckpointDir()方法，设置一个容错的文件系统的目录，比如HDFS；然后，对RDD调用checkpoint()方法。</span><br><span class="line">最后，在RDD所在的job运行结束之后，会启动一个单独的job，将checkpoint设置过的RDD的数据写入之前设置的文件系统中。</span><br><span class="line"></span><br><span class="line">这是checkpoint使用的基本步骤，很简单，那我们下面先从理论层面分析一下当我们设置好checkpoint之后，Spark底层都做了哪些事情</span><br></pre></td></tr></table></figure><h2 id="RDD之checkpoint流程"><a href="#RDD之checkpoint流程" class="headerlink" title="RDD之checkpoint流程"></a>RDD之checkpoint流程</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271450477.png" alt="image-20230327144700422"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1：SparkContext设置checkpoint目录，用于存放checkpoint的数据；</span><br><span class="line">对RDD调用checkpoint方法，然后它就会被RDDCheckpointData对象进行管理，此时这个RDD的checkpoint状态会被设置为Initialized</span><br><span class="line">2：待RDD所在的job运行结束，会调用job中最后一个RDD的doCheckpoint方法，该方法沿着RDD的血缘关系向上查找被checkpoint()方法标记过的RDD， 并将其checkpoint状态从Initialized设置为CheckpointingInProgress</span><br><span class="line">3：启动一个单独的job，来将血缘关系中标记为CheckpointInProgress的RDD执行checkpoint操作，也就是将其数据写入checkpoint目录</span><br><span class="line">4：将RDD数据写入checkpoint目录之后，会将RDD状态改变Checkpointed；</span><br><span class="line">并且还会改变RDD的血缘关系，即会清除掉RDD所有依赖的RDD；最后还会设置其父RDD为新创建的CheckpointRDD</span><br></pre></td></tr></table></figure><h2 id="checkpoint与持久化的区别"><a href="#checkpoint与持久化的区别" class="headerlink" title="checkpoint与持久化的区别"></a>checkpoint与持久化的区别</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">那这里所说的checkpoint和我们之前讲的RDD持久化有什么区别吗？</span><br><span class="line">lineage是否发生改变linage（血缘关系）说的就是RDD之间的依赖关系</span><br><span class="line"></span><br><span class="line">持久化，只是将数据保存在内存中或者本地磁盘文件中，RDD的lineage(血缘关系)是不变的；</span><br><span class="line"></span><br><span class="line">Checkpoint执行之后，RDD就没有依赖的RDD了，也就是它的lineage改变了</span><br><span class="line">丢失数据的可能性持久化的数据丢失的可能性较大，如果采用persist把数据存在内存中的话，虽然速度最快但是也是最不可靠的，就算放在磁盘上也不是完全可靠的，因为磁盘也会损坏。Checkpoint的数据通常是保存在高可用文件系统中(HDFS),丢失的可能性很低</span><br><span class="line"></span><br><span class="line">建议：对需要checkpoint的RDD，先执行persist(StorageLevel.DISK_ONLY)</span><br><span class="line">为什么呢？</span><br><span class="line"></span><br><span class="line">因为默认情况下，如果某个RDD没有持久化，但是设置了checkpoint，那么这个时候，本来Spark任务已经执行结束了，但是由于中间的RDD没有持久化，在进行checkpoint的时候想要将这个RDD的数据写入外部存储系统的话，就需要重新计算这个RDD的数据，再将其checkpoint到外部存储系统中。</span><br><span class="line">如果对需要checkpoint的rdd进行了基于磁盘的持久化，那么后面进行checkpoint操作时，就会直接从磁盘上读取rdd的数据了，就不需要重新再计算一次了，这样效率就高了。</span><br><span class="line"></span><br><span class="line">那在这能不能使用基于内存的持久化呢？当然是可以的，不过没那个必要。</span><br></pre></td></tr></table></figure><h2 id="checkPoint的使用"><a href="#checkPoint的使用" class="headerlink" title="checkPoint的使用"></a>checkPoint的使用</h2><h3 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">那我们来演示一下：将一个RDD的数据持久化到HDFS上面</span><br><span class="line">scala代码如下：</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：checkpoint的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CheckPointOpScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         conf.setAppName(<span class="string">"CheckPointOpScala"</span>)</span><br><span class="line">         <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">         <span class="keyword">if</span>(args.length==<span class="number">0</span>)&#123;</span><br><span class="line">         <span class="type">System</span>.exit(<span class="number">100</span>)</span><br><span class="line">     &#125;</span><br><span class="line">         <span class="keyword">val</span> outputPath = args(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">//1：设置checkpint目录</span></span><br><span class="line">         sc.setCheckpointDir(<span class="string">"hdfs://bigdata01:9000/chk001"</span>)</span><br><span class="line">         <span class="keyword">val</span> dataRDD = sc.textFile(<span class="string">"hdfs://bigdata01:9000/hello_10000000.dat"</span>)</span><br><span class="line">        </span><br><span class="line">         <span class="comment">//2：对rdd执行checkpoint操作</span></span><br><span class="line">         dataRDD.checkpoint()</span><br><span class="line">         dataRDD.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">         .map((_,<span class="number">1</span>))</span><br><span class="line">         .reduceByKey(_ + _)</span><br><span class="line">         .saveAsTextFile(outputPath)</span><br><span class="line">         sc.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="java"><a href="#java" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.StorageLevel;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：checkpoint的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CheckPointOpJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setAppName(<span class="string">"CheckPointOpJava"</span>);</span><br><span class="line">         JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">         <span class="keyword">if</span>(args.length==<span class="number">0</span>)&#123;</span><br><span class="line">         System.exit(<span class="number">100</span>);</span><br><span class="line">         &#125;</span><br><span class="line">         String outputPath = args[<span class="number">0</span>];</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//1：设置checkpint目录</span></span><br><span class="line">         sc.setCheckpointDir(<span class="string">"hdfs://bigdata01:9000/chk002"</span>);</span><br><span class="line">         JavaRDD&lt;String&gt; dataRDD = sc.textFile(<span class="string">"hdfs://bigdata01:9000/hello_100000.dat"</span>);</span><br><span class="line">         <span class="comment">//2: 对rdd执行checkpoint操作</span></span><br><span class="line">         dataRDD.checkpoint();</span><br><span class="line">         dataRDD.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> Arrays.asList(line.split(<span class="string">" "</span>)).iterator();</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">          <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception </span></span><br><span class="line"><span class="function">         return new Tuple2&lt;String, Integer&gt;<span class="params">(word,<span class="number">1</span>)</span></span>;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer i1, Integer i2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> i1 + i2;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).saveAsTextFile(outputPath);</span><br><span class="line">         sc.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="提交到集群执行"><a href="#提交到集群执行" class="headerlink" title="提交到集群执行"></a>提交到集群执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">下面我们把这个任务打包提交到集群上运行一下，看一下效果。</span><br><span class="line">代码master部分注释掉</span><br><span class="line"></span><br><span class="line">先确保hadoop集群是正常运行的，以及hadoop中的historyserver进程和spark的historyserver进程也是正常运行的。</span><br><span class="line">测试数据之前已经上传到了hdfs上面，如果没有则需要上传</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271517969.png" alt="image-20230327151632675"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">将pom.xml中的spark-core的依赖设置为provided，然后编译打包</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line"> &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line"> &lt;version&gt;2.4.3&lt;&#x2F;version&gt;</span><br><span class="line"> &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">将打包的jar包上传到bigdata04的&#x2F;data&#x2F;soft&#x2F;sparkjars目录，创建一个新的spark-submit脚本</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271519124.png" alt="image-20230327151903625"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">执行成功之后可以到 setCheckpointDir 指定的目录中查看一下，可以看到目录中会生成对应的文件保存rdd中的数据，只不过生成的文件不是普通文本文件，直接查看文件中的内容显示为乱码。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271523654.png" alt="image-20230327152317696"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271523426.png" alt="image-20230327152338046"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271525661.png" alt="image-20230327152509680"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来进到YARN的8088界面查看</span><br><span class="line">点击Tracking UI进入spark的ui界面看第一个界面jobs</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271526703.png" alt="image-20230327152656935"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在这可以看出来产生了2个job，</span><br><span class="line">第一个job是我们正常的任务执行，执行了39s，一共产生了28个task任务</span><br><span class="line">第二个job是checkpoint启动的job，执行了35s，一共产生了14个task任务</span><br><span class="line"></span><br><span class="line">看第二个界面Stages，这里面的3个Stage是前面2个job产生的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">具体想知道哪些Stage属于哪个job任务的话，可以在任务界面，点击Description中的链接就可以看到job对应的Stage</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271528015.png" alt="image-20230327152845211"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第一个job其实就是我们实现的单词计数的功能，这个任务产生了两个stage，这两个stage具体是如何划分的呢？</span><br><span class="line">咱们前面讲过，stage的划分是由宽依赖决定的，在这个任务中reduceByKey这个过程会产生宽依赖，所以会产生2个Stage</span><br><span class="line">这里面显示的有这两个stage的一些基本信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271531710.png" alt="image-20230327153130946"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">stage id：stage的编号，从0开始</span><br><span class="line">Duration：stage执行消耗的时间</span><br><span class="line">Tasks：Successed&#x2F;Total：task执行成功数量&#x2F;task总量</span><br><span class="line">Input：输入数据量</span><br><span class="line">ouput：输出数据量</span><br><span class="line">shuffle read&#x2F;shuffle read：shuffle过程传输数据量</span><br><span class="line">点击这个界面中的DAG Visualization可以看到当前这个任务stage的划分情况，可以看到每个Stage包含哪些算子</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271534605.png" alt="image-20230327153403130"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">进到Stage内部看一下</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271541155.png" alt="image-20230327154126374"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271542549.png" alt="image-20230327154248904"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面可以看到每个task的具体执行情况，执行状态，执行消耗的时间，GC消耗的时间，处理的数据量和数据条数、通过shuffle输出的数据量和数据条数</span><br><span class="line">其实从这里也可以看出来文件的每一个block块会产生一个task</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271546781.png" alt="image-20230327154559437"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这就是这个Stage执行的基本信息了。</span><br><span class="line">加下来看一下第二个Job，这个job是checkpoint启动的任务，查看它的stage的信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271546799.png" alt="image-20230327154637511"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个job只会产生一个stage，因为我们只针对textFile的结果设置了checkpoint</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271547533.png" alt="image-20230327154722955"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这个stage执行消耗了35s，说明这份数据是重新通过textFile读取过来的。</span><br><span class="line">针对Storage这块，显示的其实就是持久化的数据，如果对RDD做了持久化，那么在任务执行过程中能看到，任务执行结束就看不到了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271548520.png" alt="image-20230327154816099"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">下面我们来验证一下在开启持久化的情况下执行checkpoint操作时的区别</span><br><span class="line">在代码中针对RDD开启持久化</span><br><span class="line">1：对比此时产生的两个job总的消耗的时间，以及job中的Stage消耗的时间</span><br><span class="line">其实你会发现开启持久化之后，checkpoint的那个job消耗的时间就变少了</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271553264.png" alt="image-20230327155305626"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2：查看DAG Visualization，你会发现stage里面也会有有一些不一样的地方</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271556557.png" alt="image-20230327155658220"></p><h2 id="checkpoint源码分析"><a href="#checkpoint源码分析" class="headerlink" title="checkpoint源码分析"></a>checkpoint源码分析</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">前面我们通过理论层面分析了checkpoint的原理，以及演示了checkpoint的使用</span><br><span class="line">下面我们通过源码层面来对我们前面分析的理论进行验证</span><br><span class="line">先下载spark源码，下载流程和下载spark安装包的流程一样</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271953796.png" alt="image-20230327195356368"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">把下载的安装包解压到idea项目目录中</span><br><span class="line"></span><br><span class="line">打开spark-2.4.3源码目录，进入core目录，这个是spark的核心代码，我们要查看的checkpoint的源码就在这个项目中</span><br><span class="line">在idea中打开core这个子项目</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271954921.png" alt="image-20230327195430272"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面我们就来分析一下RDD的checkpoint功能：</span><br><span class="line">checkpoint功能可以分为两块</span><br><span class="line">1：checkpoint的写操作</span><br><span class="line">将指定RDD的数据通过checkpoint存储到指定外部存储中</span><br><span class="line">2：checkpoint的读操作</span><br><span class="line">任务中RDD数据在使用过程中丢失了，正好这个RDD之前做过checkpoint，所以这时就需要通过checkpoint来恢复数据</span><br></pre></td></tr></table></figure><h3 id="checkpoint的写操作"><a href="#checkpoint的写操作" class="headerlink" title="checkpoint的写操作"></a>checkpoint的写操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.1 ：当我们在自己开发的spark任务中先调用 sc.setCheckpointDir时，底层其实就会调用</span><br><span class="line">SparkContext中的setCheckpointDir方法</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def setCheckpointDir(directory: String) &#123;</span><br><span class="line">     &#x2F;&#x2F; If we are running on a cluster, log a warning if the directory is local.</span><br><span class="line">     &#x2F;&#x2F; Otherwise, the driver may attempt to reconstruct the checkpointed RDD fr</span><br><span class="line">     &#x2F;&#x2F; its own local file system, which is incorrect because the checkpoint fil</span><br><span class="line">     &#x2F;&#x2F; are actually on the executor machines.</span><br><span class="line">     if (!isLocal &amp;&amp; Utils.nonLocalPaths(directory).isEmpty) &#123;</span><br><span class="line">         logWarning(&quot;Spark is not running in local mode, therefore the checkpoint </span><br><span class="line">         s&quot;must not be on the local filesystem. Directory &#39;$directory&#39; &quot; +</span><br><span class="line">         &quot;appears to be on the local filesystem.&quot;)</span><br><span class="line">     &#125;</span><br><span class="line">     &#x2F;&#x2F;根据我们传过来的目录，后面再拼上一个子目录，子目录使用一个UUID随机字符串</span><br><span class="line">     &#x2F;&#x2F;使用HDFS的javaAPI 在HDFS上创建目录</span><br><span class="line">     checkpointDir &#x3D; Option(directory).map &#123; dir &#x3D;&gt;</span><br><span class="line">     val path &#x3D; new Path(dir, UUID.randomUUID().toString)</span><br><span class="line">     val fs &#x3D; path.getFileSystem(hadoopConfiguration)</span><br><span class="line">     fs.mkdirs(path)</span><br><span class="line">     fs.getFileStatus(path).getPath.toString</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1.2：接着我们会调用RDD.checkpoint方法，此时会执行RDD这个class中的 checkpoint方法</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;这里相当于是checkpoint的一个标记，并没有真正执行checkpoint</span><br><span class="line">def checkpoint(): Unit &#x3D; RDDCheckpointData.synchronized &#123;</span><br><span class="line">     &#x2F;&#x2F; NOTE: we use a global lock here due to complexities downstream with ensu</span><br><span class="line">     &#x2F;&#x2F; children RDD partitions point to the correct parent partitions. In the f</span><br><span class="line">     &#x2F;&#x2F; we should revisit this consideration.</span><br><span class="line">     &#x2F;&#x2F;如果SparkContext没有设置checkpointDir，则抛出异常</span><br><span class="line">     if (context.checkpointDir.isEmpty) &#123;</span><br><span class="line">     throw new SparkException(&quot;Checkpoint directory has not been set in the Sp</span><br><span class="line">     &#125; else if (checkpointData.isEmpty) &#123;</span><br><span class="line">     &#x2F;&#x2F;如果设置了，则创建RDDCheckpointData的子类，这个子类主要负责管理RDD的checkpoi</span><br><span class="line">     &#x2F;&#x2F;并且会初始化checkpoint状态为Initialized</span><br><span class="line">     checkpointData &#x3D; Some(new ReliableRDDCheckpointData(this))</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">这个checkpoint方法执行完成之后，这个流程就结束了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.3：剩下的就是在这个设置了checkpint的RDD所在的job执行结束之后，Spark会调用job中最后一个RDD的doCheckpoint方法</span><br><span class="line">这个逻辑是在SparkContext这个class的runJob方法中，当执行到Spark中的action算子时，这个runJob方法会被触发，开始执行任务。</span><br><span class="line">这个runJob的最后一行会调用rdd中的 doCheckpoint 方法</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;在有action动作时，会触发sparkcontext对runJob的调用</span><br><span class="line">def runJob[T, U: ClassTag](</span><br><span class="line">     rdd: RDD[T],</span><br><span class="line">     func: (TaskContext, Iterator[T]) &#x3D;&gt; U,</span><br><span class="line">     partitions: Seq[Int],</span><br><span class="line">     resultHandler: (Int, U) &#x3D;&gt; Unit): Unit &#x3D; &#123;</span><br><span class="line">     if (stopped.get()) &#123;</span><br><span class="line">     throw new IllegalStageException(&quot;SparkContext has been shutdown&quot;)</span><br><span class="line">     &#125;</span><br><span class="line">     val callSite &#x3D; getCallSite</span><br><span class="line">     val cleanedFunc &#x3D; clean(func)</span><br><span class="line">     logInfo(&quot;Starting job: &quot; + callSite.shortForm)</span><br><span class="line">     if (conf.getBoolean(&quot;spark.logLineage&quot;, false)) &#123;</span><br><span class="line">     logInfo(&quot;RDD&#39;s recursive dependencies:\n&quot; + rdd.toDebugString)</span><br><span class="line">     &#125;</span><br><span class="line">     dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, </span><br><span class="line">     progressBar.foreach(_.finishAll())</span><br><span class="line">     &#x2F;&#x2F;在这里会执行doCheckpoint()</span><br><span class="line">     rdd.doCheckpoint()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.4：接着会进入到RDD中的 doCheckpoint 方法</span><br><span class="line">这里面最终会调用 RDDCheckpointData 的 checkpoint 方法</span><br><span class="line">checkpointData.get.checkpoint()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">private[spark] def doCheckpoint(): Unit &#x3D; &#123;</span><br><span class="line">     RDDOperationScope.withScope(sc, &quot;checkpoint&quot;, allowNesting &#x3D; false, ignoreP</span><br><span class="line">     &#x2F;&#x2F;该rdd是否已经调用doCheckpoint，如果还没有，则开始处理</span><br><span class="line">     if (!doCheckpointCalled) &#123;</span><br><span class="line">     doCheckpointCalled &#x3D; true</span><br><span class="line">     &#x2F;&#x2F;若已经被checkpoint()标记过，则checkpointData.isDefined为true</span><br><span class="line">     if (checkpointData.isDefined) &#123;</span><br><span class="line">     &#x2F;&#x2F;查看是否需要把该rdd的所有依赖全部checkpoint</span><br><span class="line">     &#x2F;&#x2F;checkpointAllMarkedAncestors取自配置&quot;spark.checkpoint.checkpointAllM</span><br><span class="line">     &#x2F;&#x2F;默认不配时值为false</span><br><span class="line">     if (checkpointAllMarkedAncestors) &#123;</span><br><span class="line">     &#x2F;&#x2F; TODO We can collect all the RDDs that needs to be checkpointed, </span><br><span class="line">     &#x2F;&#x2F; them in parallel.</span><br><span class="line">     &#x2F;&#x2F; Checkpoint parents first because our lineage will be truncated af</span><br><span class="line">     &#x2F;&#x2F; checkpoint ourselves</span><br><span class="line">     &#x2F;&#x2F; 血缘上的每一个父rdd递归调用该方法</span><br><span class="line">     dependencies.foreach(_.rdd.doCheckpoint())</span><br><span class="line">     &#125;</span><br><span class="line">     &#x2F;&#x2F;调用RDDCheckpointData的checkpoint方法</span><br><span class="line">     checkpointData.get.checkpoint()</span><br><span class="line">     &#125; else &#123;</span><br><span class="line">     &#x2F;&#x2F;沿着rdd的血缘关系向上查找被checkpoint()标记过的RDD</span><br><span class="line">     dependencies.foreach(_.rdd.doCheckpoint())</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">1.5：接下来进入到 RDDCheckpointData 的 checkpoint 方法中</span><br><span class="line">这里面会调用子类 ReliableCheckpointRDD 中的 doCheckpoint()方法</span><br><span class="line"></span><br><span class="line">final def checkpoint(): Unit &#x3D; &#123;</span><br><span class="line">     &#x2F;&#x2F; Guard against multiple threads checkpointing the same RDD by</span><br><span class="line">     &#x2F;&#x2F; atomically flipping the Stage of this RDDCheckpointData</span><br><span class="line">     &#x2F;&#x2F;&#x2F;&#x2F;将checkpoint的状态从Initialized置为CheckpointingInProgress</span><br><span class="line">     RDDCheckpointData.synchronized &#123;</span><br><span class="line">     if (cpStage &#x3D;&#x3D; Initialized) &#123;</span><br><span class="line">     cpStage &#x3D; CheckpointingInProgress</span><br><span class="line">     &#125; else &#123;</span><br><span class="line">     return</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     &#x2F;&#x2F;调用子类的doCheckpoint，默认会使用ReliableCheckpointRDD子类，创建一个新的Chec</span><br><span class="line">     val newRDD &#x3D; doCheckpoint()</span><br><span class="line">     &#x2F;&#x2F; Update our Stage and truncate the RDD lineage</span><br><span class="line">     &#x2F;&#x2F;将checkpoint状态置为Checkpointed状态，并且改变rdd之前的依赖，设置父rdd为新创建</span><br><span class="line">     RDDCheckpointData.synchronized &#123;</span><br><span class="line">     cpRDD &#x3D; Some(newRDD)</span><br><span class="line">     cpStage &#x3D; Checkpointed</span><br><span class="line">     rdd.markCheckpointed()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.6：接着来进入 ReliableCheckpointRDD 中的 doCheckpoint() 方法</span><br><span class="line">这里面会调用 ReliableCheckpointRDD 中的 writeRDDToCheckpointDirectory 方法将rdd的数据写入HDFS</span><br><span class="line">中的 checkpoint 目录，并且返回创建的 CheckpointRDD</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">protected override def doCheckpoint(): CheckpointRDD[T] &#x3D; &#123;</span><br><span class="line">     &#x2F;&#x2F;将rdd的数据写入HDFS中的checkpoint目录，并且创建的CheckpointRDD</span><br><span class="line">     val newRDD &#x3D; ReliableCheckpointRDD.writeRDDToCheckpointDirectory(rdd, cpDir</span><br><span class="line">     &#x2F;&#x2F; Optionally clean our checkpoint files if the reference is out of scope</span><br><span class="line">     if (rdd.conf.getBoolean(&quot;spark.cleaner.referenceTracking.cleanCheckpoints&quot;, </span><br><span class="line">     rdd.context.cleaner.foreach &#123; cleaner &#x3D;&gt;</span><br><span class="line">     cleaner.registerRDDCheckpointDataForCleanup(newRDD, rdd.id)</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line">logInfo(s&quot;Done checkpointing RDD $&#123;rdd.id&#125; to $cpDir, new parent is RDD $&#123;n</span><br><span class="line"> newRDD</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.7：接下来进入 ReliableCheckpointRDD 的 writeRDDToCheckpointDirectory 方法</span><br><span class="line">这里面最终会启动一个job，将checkpoint的数据写入到指定的HDFS目录中</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;将rdd的数据写入HDFS中checkpoint目录，并且创建CheckpointRDD</span><br><span class="line">def writeRDDToCheckpointDirectory[T: ClassTag](</span><br><span class="line"> originalRDD: RDD[T],</span><br><span class="line"> checkpointDir: String,</span><br><span class="line"> blockSize: Int &#x3D; -1): ReliableCheckpointRDD[T] &#x3D; &#123;</span><br><span class="line"> val checkpointStartTimeNs &#x3D; System.nanoTime()</span><br><span class="line"> val sc &#x3D; originalRDD.sparkContext</span><br><span class="line"> &#x2F;&#x2F;Create the output path for the checkpoint</span><br><span class="line"> &#x2F;&#x2F;创建checkpoint输出目录</span><br><span class="line"> val checkpointDirPath &#x3D; new Path(checkpointDir)</span><br><span class="line"> &#x2F;&#x2F;获取HDFS文件系统API接口</span><br><span class="line"> val fs &#x3D; checkpointDirPath.getFileSystem(sc.hadoopConfiguration)</span><br><span class="line"> &#x2F;&#x2F;创建目录</span><br><span class="line"> if (!fs.mkdirs(checkpointDirPath)) &#123;</span><br><span class="line"> throw new SparkException(s&quot;Failed to create checkpoint path $checkpointDi</span><br><span class="line"> &#125;</span><br><span class="line"> &#x2F;&#x2F; Save to file, and reload it as an RDD</span><br><span class="line"> &#x2F;&#x2F;将Hadoop配置文件信息广播到所有节点</span><br><span class="line"> val broadcastedConf &#x3D; sc.broadcast(</span><br><span class="line"> new SerializableConfiguration(sc.hadoopConfiguration))</span><br><span class="line"> &#x2F;&#x2F; TODO: This is expensive because it computes the RDD again unnecessarily </span><br><span class="line"> &#x2F;&#x2F;这里强调了checkpoint是一个昂贵的操作，主要是说它昂贵在需要沿着血缘关系重新计算该</span><br><span class="line"> &#x2F;&#x2F;重新启动一个job,将rdd的分区数据写入HDFS</span><br><span class="line"> sc.runJob(originalRDD,</span><br><span class="line"> writePartitionToCheckpointFile[T](checkpointDirPath.toString, broadcasted</span><br><span class="line"> &#x2F;&#x2F;如果rdd的partitioner不为空，则将partitioner写入checkpoint目录</span><br><span class="line"> if (originalRDD.partitioner.nonEmpty) &#123;</span><br><span class="line"> writePartitionerToCheckpointDir(sc, originalRDD.partitioner.get, checkpoi</span><br><span class="line"> &#125;</span><br><span class="line"> val checkpointDurationMs &#x3D;</span><br><span class="line"> TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - checkpointStartTimeNs)</span><br><span class="line"> logInfo(s&quot;Checkpointing took $checkpointDurationMs ms.&quot;)</span><br><span class="line"> &#x2F;&#x2F;创建一个CheckpointRDD,该RDD的分区数目和原始的rdd的分区数是一样的</span><br><span class="line"> val newRDD &#x3D; new ReliableCheckpointRDD[T](</span><br><span class="line"> sc, checkpointDirPath.toString, originalRDD.partitioner)</span><br><span class="line"> if (newRDD.partitions.length !&#x3D; originalRDD.partitions.length) &#123;</span><br><span class="line"> throw new SparkException(</span><br><span class="line"> &quot;Checkpoint RDD has a different number of partitions from original RDD. </span><br><span class="line"> s&quot;RDD [ID: $&#123;originalRDD.id&#125;, num of partitions: $&#123;originalRDD.partit</span><br><span class="line"> s&quot;Checkpoint RDD [ID: $&#123;newRDD.id&#125;, num of partitions: &quot; +</span><br><span class="line"> s&quot;$&#123;newRDD.partitions.length&#125;].&quot;)</span><br><span class="line"> &#125;</span><br><span class="line"> newRDD</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">执行到这，其实调用过checkpoint方法的RDD就被保存到HDFS上了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">注意：在这里通过checkpoint操作将RDD中的数据写入到HDFS中的时候，会调用RDD中的</span><br><span class="line">iterator方法，遍历RDD中所有分区的数据。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">那我们来分析一下这块的代码</span><br><span class="line">此时我们没有对RDD进行持久化，所以走else中的代码</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">final def iterator(split: Partition, context: TaskContext): Iterator[T] &#x3D; &#123;</span><br><span class="line"> &#x2F;&#x2F;如果StorageLevel不为空，表示该RDD已经持久化过了,可能是在内存，也有可能是在磁盘</span><br><span class="line"> if (storageLevel !&#x3D; StorageLevel.NONE) &#123;</span><br><span class="line"> &#x2F;&#x2F;直接从持久化存储中读取或者计算(如果数据丢失)</span><br><span class="line"> getOrCompute(split, context)</span><br><span class="line"> &#125; else &#123;</span><br><span class="line"> &#x2F;&#x2F;进行rdd partition的计算或者从checkpoint中读取数据</span><br><span class="line"> computeOrReadCheckpoint(split, context)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">进入 computeOrReadCheckpoint(split, context) 中</span><br><span class="line">此时这个RDD是将要进行checkpoint，还没有完成checkpoint，所以走 else ，会执行 compute 方法</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskCont</span><br><span class="line">&#123;</span><br><span class="line"> &#x2F;&#x2F;当前rdd是否已经checkpoint并且物化，如果已经checkpoint并且物化</span><br><span class="line"> &#x2F;&#x2F;则调用firstParent的iterator方法获取</span><br><span class="line"> if (isCheckpointedAndMaterialized) &#123;</span><br><span class="line"> &#x2F;&#x2F;注意：针对checpoint操作，它的血缘关系已经被切断了，那么它的firstParent就是前面</span><br><span class="line"> firstParent[T].iterator(split, context)</span><br><span class="line"> &#125; else &#123;</span><br><span class="line"> &#x2F;&#x2F;如果没有，则表示持久化数据丢失，或者根本就没有持久化，</span><br><span class="line"> &#x2F;&#x2F;需要调用rdd的compute方法开始重新计算，返回一个Iterator对象</span><br><span class="line"> compute(split, context)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在这会执行RDD的子类 HadoopRDD 中的 compute 方法</span><br><span class="line">在这里会通过 RecordReader 获取RDD中指定分区的数据</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line">override def compute(theSplit: Partition, context: TaskContext): Interruptibl</span><br><span class="line"> val iter &#x3D; new NextIterator[(K, V)] &#123;</span><br><span class="line"> private val split &#x3D; theSplit.asInstanceOf[HadoopPartition]</span><br><span class="line"> logInfo(&quot;Input split: &quot; + split.inputSplit)</span><br><span class="line"> private val jobConf &#x3D; getJobConf()</span><br><span class="line"> private val inputMetrics &#x3D; context.taskMetrics().inputMetrics</span><br><span class="line"> private val existingBytesRead &#x3D; inputMetrics.bytesRead</span><br><span class="line"> &#x2F;&#x2F; Sets InputFileBlockHolder for the file block&#39;s information</span><br><span class="line"> split.inputSplit.value match &#123;</span><br><span class="line"> case fs: FileSplit &#x3D;&gt;</span><br><span class="line"> InputFileBlockHolder.set(fs.getPath.toString, fs.getStart, fs.getLengt</span><br><span class="line"> case _ &#x3D;&gt;</span><br><span class="line"> InputFileBlockHolder.unset()</span><br><span class="line"> &#125;</span><br><span class="line"> &#x2F;&#x2F; Find a function that will return the FileSystem bytes read by this thr</span><br><span class="line"> &#x2F;&#x2F; creating RecordReader, because RecordReader&#39;s constructor might read s</span><br><span class="line"> private val getBytesReadCallback: Option[() &#x3D;&gt; Long] &#x3D; split.inputSplit.v</span><br><span class="line"> case _: FileSplit | _: CombineFileSplit &#x3D;&gt;</span><br><span class="line"> Some(SparkHadoopUtil.get.getFSBytesReadOnThreadCallback())</span><br><span class="line"> case _ &#x3D;&gt; None</span><br><span class="line"> &#125;</span><br><span class="line"> &#x2F;&#x2F; We get our input bytes from thread-local Hadoop FileSystem statistics.</span><br><span class="line"> &#x2F;&#x2F; If we do a coalesce, however, we are likely to compute multiple partit</span><br><span class="line"> &#x2F;&#x2F; task and in the same thread, in which case we need to avoid override v</span><br><span class="line"> &#x2F;&#x2F; previous partitions (SPARK-13071).</span><br><span class="line"> private def updateBytesRead(): Unit &#x3D; &#123;</span><br><span class="line"> getBytesReadCallback.foreach &#123; getBytesRead &#x3D;&gt;</span><br><span class="line"> inputMetrics.setBytesRead(existingBytesRead + getBytesRead())</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> private var reader: RecordReader[K, V] &#x3D; null</span><br><span class="line"> private val inputFormat &#x3D; getInputFormat(jobConf)</span><br><span class="line"> HadoopRDD.addLocalConfiguration(</span><br><span class="line"> new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;, Locale.US).format(createTime),</span><br><span class="line"> context.stageId, theSplit.index, context.attemptNumber, jobConf)</span><br><span class="line"> reader &#x3D;</span><br><span class="line"> try &#123;</span><br><span class="line"> inputFormat.getRecordReader(split.inputSplit.value, jobConf, Reporter</span><br><span class="line"> &#125; catch &#123;</span><br><span class="line"> case e: FileNotFoundException if ignoreMissingFiles &#x3D;&gt;</span><br><span class="line"> logWarning(s&quot;Skipped missing file: $&#123;split.inputSplit&#125;&quot;, e)</span><br><span class="line"> finished &#x3D; true</span><br><span class="line"> null</span><br><span class="line"> &#x2F;&#x2F; Throw FileNotFoundException even if &#96;ignoreCorruptFiles&#96; is true</span><br><span class="line"> case e: FileNotFoundException if !ignoreMissingFiles &#x3D;&gt; throw e</span><br><span class="line"> case e: IOException if ignoreCorruptFiles &#x3D;&gt;</span><br><span class="line"> logWarning(s&quot;Skipped the rest content in the corrupted file: $&#123;split</span><br><span class="line"> finished &#x3D; true</span><br><span class="line"> null</span><br><span class="line"> &#125;</span><br><span class="line"> &#x2F;&#x2F; Register an on-task-completion callback to close the input stream.</span><br><span class="line"> context.addTaskCompletionListener[Unit] &#123; context &#x3D;&gt;</span><br><span class="line"> &#x2F;&#x2F; Update the bytes read before closing is to make sure lingering bytes</span><br><span class="line"> &#x2F;&#x2F; this thread get correctly added.</span><br><span class="line"> updateBytesRead()</span><br><span class="line"> closeIfNeeded()</span><br><span class="line"> &#125;</span><br><span class="line"> private val key: K &#x3D; if (reader &#x3D;&#x3D; null) null.asInstanceOf[K] else reader</span><br><span class="line"> private val value: V &#x3D; if (reader &#x3D;&#x3D; null) null.asInstanceOf[V] else read</span><br><span class="line"> override def getNext(): (K, V) &#x3D; &#123;</span><br><span class="line"> try &#123;</span><br><span class="line"> finished &#x3D; !reader.next(key, value)</span><br><span class="line"> &#125; catch &#123;</span><br><span class="line"> case e: FileNotFoundException if ignoreMissingFiles &#x3D;&gt;</span><br><span class="line"> logWarning(s&quot;Skipped missing file: $&#123;split.inputSplit&#125;&quot;, e)</span><br><span class="line"> finished &#x3D; true</span><br><span class="line"> &#x2F;&#x2F; Throw FileNotFoundException even if &#96;ignoreCorruptFiles&#96; is true</span><br><span class="line"> case e: FileNotFoundException if !ignoreMissingFiles &#x3D;&gt; throw e</span><br><span class="line"> case e: IOException if ignoreCorruptFiles &#x3D;&gt;</span><br><span class="line"> logWarning(s&quot;Skipped the rest content in the corrupted file: $&#123;split</span><br><span class="line"> finished &#x3D; true</span><br><span class="line"> &#125;</span><br><span class="line"> if (!finished) &#123;</span><br><span class="line"> inputMetrics.incRecordsRead(1)</span><br><span class="line"> &#125;</span><br><span class="line"> if (inputMetrics.recordsRead % SparkHadoopUtil.UPDATE_INPUT_METRICS_INT</span><br><span class="line"> updateBytesRead()</span><br><span class="line"> &#125;</span><br><span class="line"> (key, value)</span><br><span class="line"> &#125;</span><br><span class="line"> override def close(): Unit &#x3D; &#123;</span><br><span class="line"> if (reader !&#x3D; null) &#123;</span><br><span class="line"> InputFileBlockHolder.unset()</span><br><span class="line"> try &#123;</span><br><span class="line"> reader.close()</span><br><span class="line"> &#125; catch &#123;</span><br><span class="line"> case e: Exception &#x3D;&gt;</span><br><span class="line"> if (!ShutdownHookManager.inShutdown()) &#123;</span><br><span class="line"> logWarning(&quot;Exception in RecordReader.close()&quot;, e)</span><br><span class="line"> &#125;</span><br><span class="line"> &#125; finally &#123;</span><br><span class="line"> reader &#x3D; null</span><br><span class="line"> &#125;</span><br><span class="line"> if (getBytesReadCallback.isDefined) &#123;</span><br><span class="line"> updateBytesRead()</span><br><span class="line"> &#125; else if (split.inputSplit.value.isInstanceOf[FileSplit] ||</span><br><span class="line"> split.inputSplit.value.isInstanceOf[CombineFileSplit]) &#123;</span><br><span class="line"> &#x2F;&#x2F; If we can&#39;t get the bytes read from the FS stats, fall back to t</span><br><span class="line"> &#x2F;&#x2F; which may be inaccurate.</span><br><span class="line"> try &#123;</span><br><span class="line"> inputMetrics.incBytesRead(split.inputSplit.value.getLength)</span><br><span class="line"> &#125; catch &#123;</span><br><span class="line"> case e: java.io.IOException &#x3D;&gt;</span><br><span class="line"> logWarning(&quot;Unable to get input size to set InputMetrics for ta</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> new InterruptibleIterator[(K, V)](context, iter)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这样经过几次迭代之后就可以获取到RDD中所有分区的数据了，因为这个compute是一次获取一个分区</span><br><span class="line">的数据。获取到之后checkpoint就可以把这个RDD的数据存储到HDFS上了。</span><br><span class="line">这就是checkpoint的写操作</span><br></pre></td></tr></table></figure><h3 id="checkpoint的读操作"><a href="#checkpoint的读操作" class="headerlink" title="checkpoint的读操作"></a>checkpoint的读操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们来分析一下checkpoint读数据这个操作</span><br><span class="line">当RDD中的数据丢失了以后，需要通过checkpoint读取存储在hdfs上的数据，</span><br><span class="line">2.1：这个时候还是会执行RDD中的iterator方法</span><br><span class="line">由于我们没有做持久化，只做了checkpoint，所以还是会走 else</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">final def iterator(split: Partition, context: TaskContext): Iterator[T] &#x3D; &#123;</span><br><span class="line"> &#x2F;&#x2F;如果StorageLevel不为空，表示该RDD已经持久化过了,可能是在内存，也有可能是在磁盘</span><br><span class="line"> if (storageLevel !&#x3D; StorageLevel.NONE) &#123;</span><br><span class="line"> &#x2F;&#x2F;直接从持久化存储中读取或者计算(如果数据丢失)</span><br><span class="line"> getOrCompute(split, context)</span><br><span class="line"> &#125; else &#123;</span><br><span class="line"> &#x2F;&#x2F;进行rdd partition的计算或者从checkpoint中读取数据</span><br><span class="line"> computeOrReadCheckpoint(split, context)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">进入 computeOrReadCheckpoint 方法</span><br><span class="line">此时rdd已经 checkpoint 并且物化，所以 if 分支满足</span><br><span class="line">执行 firstParent[T].iterator(split, context) 这行代码</span><br><span class="line"></span><br><span class="line">这行代码的意思是会找到当前这个RDD的父RDD，其实这个RDD执行过checkpoint之后，血缘关系已经</span><br><span class="line">被切断了，它的父RDD就是我们前面创建的那个 ReliableCheckpointRDD</span><br><span class="line">这个 ReliableCheckpointRDD 中没有覆盖 iterator 方法，所以在调用 iterator 的时候还是执行RDD这个</span><br><span class="line">父类中的 iterator ，重新进来之后再判断，这个 ReliableCheckpointRDD 再执行if判断的时候就不满足</span><br><span class="line">了，因为它的 checkpoint 属性不满足，所以会走 else ，执行 compute</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskCont</span><br><span class="line">&#123;</span><br><span class="line"> &#x2F;&#x2F;当前rdd是否已经checkpoint并且物化，如果已经checkpoint并且物化</span><br><span class="line"> &#x2F;&#x2F;则调用firstParent的iterator方法获取</span><br><span class="line"> if (isCheckpointedAndMaterialized) &#123;</span><br><span class="line"> &#x2F;&#x2F;注意：针对checpoint操作，它的血缘关系已经被切断了，那么它的firstParent就是前面</span><br><span class="line"> firstParent[T].iterator(split, context)</span><br><span class="line"> &#125; else &#123;</span><br><span class="line"> &#x2F;&#x2F;如果没有，则表示持久化数据丢失，或者根本就没有持久化，</span><br><span class="line"> &#x2F;&#x2F;需要调用rdd的compute方法开始重新计算，返回一个Iterator对象</span><br><span class="line"> compute(split, context)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时会执行 ReliableCheckpointRDD 这个子类中的 compute 方法</span><br><span class="line">这里面就会找到之前checkpoint的文件，从HDFS上恢复RDD中的数据。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">override def compute(split: Partition, context: TaskContext): Iterator[T] &#x3D; &#123;</span><br><span class="line"> &#x2F;&#x2F;获取checkpoint文件</span><br><span class="line"> val file &#x3D; new Path(checkpointPath, ReliableCheckpointRDD.checkpointFileNam</span><br><span class="line"> &#x2F;&#x2F;从HDFS上的checkpoint文件中读取checkpoint的数据</span><br><span class="line"> ReliableCheckpointRDD.readCheckpointFile(file, broadcastedConf, context)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这是从checkpoint中读取数据的流程</span><br><span class="line">咱们前面说过，建议对需要做checkpoint的数据先进行持久化，如果我们设置了持久化，针对</span><br><span class="line">checkpoint的写操作，在执行iterator方法的时候会是什么现象呢？</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">此时在最后将RDD中的数据通过checkpoint存储到HDFS上的时候，会调用RDD的iterator方法，不过此</span><br><span class="line">时 storageLevel 就不为 null 了，因为我们对这个RDD做了基于磁盘的持久化，所以会走 if 分支，执行</span><br><span class="line">getOrCompute</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">final def iterator(split: Partition, context: TaskContext): Iterator[T] &#x3D; &#123;</span><br><span class="line"> &#x2F;&#x2F;如果StorageLevel不为空，表示该RDD已经持久化过了,可能是在内存，也有可能是在磁盘</span><br><span class="line"> if (storageLevel !&#x3D; StorageLevel.NONE) &#123;</span><br><span class="line"> &#x2F;&#x2F;直接从持久化存储中读取或者计算(如果数据丢失)</span><br><span class="line"> getOrCompute(split, context)</span><br><span class="line"> &#125; else &#123;</span><br><span class="line"> &#x2F;&#x2F;进行rdd partition的计算或者从checkpoint中读取数据</span><br><span class="line"> computeOrReadCheckpoint(split, context)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">进入 getOrCompute 方法</span><br><span class="line">由于这个RDD的数据已经做了持久化，所以在这就可以从 blockmanager 中读取数据了，就不需要重新从</span><br><span class="line">源头计算或者拉取数据了，所以会提高 checkpoint 的效率</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">private[spark] def getOrCompute(partition: Partition, context: TaskContext): </span><br><span class="line"> val blockId &#x3D; RDDBlockId(id, partition.index)</span><br><span class="line"> var readCachedBlock &#x3D; true</span><br><span class="line"> &#x2F;&#x2F; This method is called on executors, so we need call SparkEnv.get instead </span><br><span class="line"> SparkEnv.get.blockManager.getOrElseUpdate(blockId, storageLevel, elementCla</span><br><span class="line"> readCachedBlock &#x3D; false</span><br><span class="line"> computeOrReadCheckpoint(partition, context)</span><br><span class="line"> &#125;) match &#123;</span><br><span class="line"> case Left(blockResult) &#x3D;&gt;</span><br><span class="line"> if (readCachedBlock) &#123;</span><br><span class="line"> val existingMetrics &#x3D; context.taskMetrics().inputMetrics</span><br><span class="line"> existingMetrics.incBytesRead(blockResult.bytes)</span><br><span class="line"> new InterruptibleIterator[T](context, blockResult.data.asInstanceOf[It</span><br><span class="line"> override def next(): T &#x3D; &#123;</span><br><span class="line"> existingMetrics.incRecordsRead(1)</span><br><span class="line"> delegate.next()</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125; else &#123;</span><br><span class="line"> new InterruptibleIterator(context, blockResult.data.asInstanceOf[Iter</span><br><span class="line"> &#125;</span><br><span class="line"> case Right(iter) &#x3D;&gt;</span><br><span class="line"> new InterruptibleIterator(context, iter.asInstanceOf[Iterator[T]])</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十一周 Spark性能优化的道与术</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-1.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-1.html</id>
    <published>2023-03-27T03:33:38.000Z</published>
    <updated>2023-03-27T17:50:21.029Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十一周-Spark性能优化的道与术"><a href="#第十一周-Spark性能优化的道与术" class="headerlink" title="第十一周 Spark性能优化的道与术"></a>第十一周 Spark性能优化的道与术</h1><h2 id="宽依赖和窄依赖"><a href="#宽依赖和窄依赖" class="headerlink" title="宽依赖和窄依赖"></a>宽依赖和窄依赖</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">先看一下什么是窄依赖：</span><br><span class="line">窄依赖(Narrow Dependency)：指父RDD的每个分区只被子RDD的一个分区所使用，例如map、filter等这些算子</span><br><span class="line">一个RDD，对它的父RDD只有简单的一对一的关系，也就是说，RDD的每个partition仅仅依赖于父RDD中的一个partition，父RDD和子RDD的partition之间的对应关系，是一对一的。</span><br><span class="line"></span><br><span class="line">宽依赖(Shuffle Dependency)：父RDD的每个分区都可能被子RDD的多个分区使用，例如groupByKey、</span><br><span class="line">reduceByKey，sortBykey等算子，这些算子其实都会产生shuffle操作</span><br><span class="line">也就是说，每一个父RDD的partition中的数据都可能会传输一部分到下一个RDD的每个partition中。此时就会出现，父RDD和子RDD的partition之间，具有错综复杂的关系，那么，这种情况就叫做两个RDD之间是宽依赖，同时，他们之间会发生shuffle操作。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面来看图具体分析一个案例</span><br><span class="line">以单词计数案例来分析</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271142255.png" alt="image-20230327114152987"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">最左侧是linesRDD，这个表示我们通过textFile读取文件中的数据之后获取的RDD</span><br><span class="line">接着是我们使用flatMap算子，对每一行数据按照空格切开，然后可以获取到第二个RDD，这个RDD中包含的是切开的每一个单词</span><br><span class="line">在这里这两个RDD就属于一个窄依赖，因为父RDD的每个分区只被子RDD的一个分区所使用，也就是说他们的分区是一对一的，这样就不需要经过shuffle了。</span><br><span class="line">接着是使用map算子，将每一个单词转换成(单词,1)这种形式，</span><br><span class="line">此时这两个RDD也是一个窄依赖的关系，父RDD的分区和子RDD的分区也是一对一的</span><br><span class="line">最后我们会调用reduceByKey算子，此时会对相同key的数据进行分组，分到一个分区里面，并且进行聚合操作，此时父RDD的每个分区都可能被子RDD的多个分区使用，那这两个RDD就属于宽依赖了。</span><br><span class="line"></span><br><span class="line">这就是宽窄依赖的区别，那我们在这区分宽窄依赖有什么意义吗？</span><br><span class="line">不要着急，往下面看</span><br></pre></td></tr></table></figure><h2 id="Stage"><a href="#Stage" class="headerlink" title="Stage"></a>Stage</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">spark job是根据action算子触发的,遇到action算子就会起一个job</span><br><span class="line">Spark Job会被划分为多个Stage，每一个Stage是由一组并行的Task组成的</span><br><span class="line"></span><br><span class="line">注意：stage的划分依据就是看是否产生了shuflle(即宽依赖),遇到一个shuffle操作就划分为前后两个stage</span><br><span class="line">stage是由一组并行的task组成，stage会将一批task用TaskSet来封装，提交给TaskScheduler进行分配，最后发送到Executor执行</span><br><span class="line"></span><br><span class="line">下面来看一张图来具体分析一下</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271146961.png" alt="image-20230327114640278"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">为什么是从后往前呢？因为RDD之间是有血缘关系的，后面的RDD依赖前面的RDD，也就是说后面的RDD要等前面的RDD执行完,才会执行。</span><br><span class="line">所以从后往前遇到宽依赖就划分为两个stage，shuffle前一个,shuffle后一个。如果整个过程没有产生shuffle那就只会有一个stage</span><br><span class="line"></span><br><span class="line">看这个图</span><br><span class="line">RDD G往前推，到RDD B的时候，是窄依赖，所以不切分Stage，再往前到RDD A，此时产生了宽依赖，所以RDD A属于一个Stage、RDD B 和 G属于一个Stage</span><br><span class="line">再看下面，RDD G到RDD F，产生了宽依赖，所以RDD F属于一个Stage，因为RDD F和RDD C、D、E这几个RDD没有产生宽依赖，都是窄依赖，所以他们属于一个Stage。</span><br><span class="line">所以这个图中,RDD A单独一个stage1,RDD C、D、E、F被划分在stage2中,</span><br><span class="line">最后RDD B和RDD G划分在了stage3 里面。</span><br><span class="line"></span><br><span class="line">注意：Stage划分是从后往前划分，但是stage执行时从前往后的，这就是为什么后面先切割的stage为什么编号是3.</span><br></pre></td></tr></table></figure><h2 id="Spark-Job的三种提交模式"><a href="#Spark-Job的三种提交模式" class="headerlink" title="Spark Job的三种提交模式"></a>Spark Job的三种提交模式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1. 第一种，standalone模式，基于Spark自己的standalone集群。</span><br><span class="line">指定spark-submit –master spark:&#x2F;&#x2F;bigdata01:7077</span><br><span class="line"></span><br><span class="line">2. 第二种，是基于YARN的client模式。</span><br><span class="line">指定–master yarn --deploy-mode client</span><br><span class="line">这种方式主要用于测试，查看日志方便一些，部分日志会直接打印到控制台上面，因为driver进程运行在本地客户端，就是提交Spark任务的那个客户端机器，driver负责调度job，会与yarn集群产生大量的通信，一般情况下Spark客户端机器和Hadoop集群的机器是无法内网通信，只能通过外网，这样在大量通信的情况下会影响通信效率，并且当我们执行一些action操作的时候数据也会返回给driver端，driver端机器的配置一般都不高，可能会导致内存溢出等问题。</span><br><span class="line"></span><br><span class="line">3. 第三种，是基于YARN的cluster模式。【推荐】</span><br><span class="line">指定–master yarn --deploy-mode cluster</span><br><span class="line">这种方式driver进程运行在集群中的某一台机器上，这样集群内部节点之间通信是可以通过内网通信的，并且集群内的机器的配置也会比普通的客户端机器配置高，所以就不存在yarn-client模式的一些问题了，只不过这个时候查看日志只能到集群上面看了，这倒没什么影响。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271206035.png" alt="image-20230327120631638"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">左边是standalone模式，现在我们使用的提交方式，driver进程是在客户端机器中的，其实针对standalone模式而言，这个Driver进程也是可以运行在集群中的</span><br><span class="line">来看一下官网文档，standalone模式也是支持的，通过指定deploy-mode 为cluster即可</span><br><span class="line"></span><br><span class="line">中间的值yarn client模式，由于是on yarn模式，所以里面是yarn集群的进程，此时driver进程就在提交spark任务的客户端机器上了</span><br><span class="line">最右边这个是yarn cluster模式，driver进程就会在集群中的某一个节点上面。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271210537.png" alt="image-20230327121044013"></p><h2 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">在MapReduce框架中，Shuffle是连接Map和Reduce之间的桥梁，Map阶段通过shuffle读取数据并输出到对应的Reduce；而Reduce阶段负责从Map端拉取数据并进行计算。在整个shuffle过程中，往往伴随着大量的磁盘和网络I&#x2F;O。所以shuffle性能的高低也直接决定了整个程序的性能高低。Spark也会有自己</span><br><span class="line">的shuffle实现过程。</span><br><span class="line"></span><br><span class="line">我们首先来看一下</span><br><span class="line">在Spark中，什么情况下，会发生shuffle？</span><br><span class="line">reduceByKey、groupByKey、sortByKey、countByKey、join等操作都会产生shuffle。</span><br><span class="line"></span><br><span class="line">那下面我们来详细分析一下Spark中的shuffle过程。</span><br><span class="line">Spark的shuffle历经了几个过程</span><br><span class="line">1. Spark 0.8及以前使用Hash Based Shuffle</span><br><span class="line">2. Spark 0.8.1 为Hash Based Shuffle引入File Consolidation机制</span><br><span class="line">3. Spark1.6之后使用Sort-Base Shuffle，因为Hash Based Shuffle存在一些不足所以就把它替换掉了。</span><br><span class="line"></span><br><span class="line">所以Spark Shuffle 一共经历了这几个过程：</span><br><span class="line">1. 未优化的 Hash Based Shuffle</span><br><span class="line">2. 优化后的Hash Based Shuffle</span><br><span class="line">3. Sort-Based Shuffle</span><br></pre></td></tr></table></figure><h3 id="未优化的Hash-Based-Shuffle"><a href="#未优化的Hash-Based-Shuffle" class="headerlink" title="未优化的Hash Based Shuffle"></a>未优化的Hash Based Shuffle</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">来看一个图，假设我们是在执行一个reduceByKey之类的操作，此时就会产生shuffle</span><br><span class="line">shuffle里面会有两种task，一种是shuffleMapTask，负责拉取前一个RDD中的数据，还有一个ResultTask，负责把拉取到的数据按照规则汇总起来</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271218160.png" alt="image-20230327121800239"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1：假设有1个节点，这个节点上有2个CPU，上面运行了4个ShuffleMapTask，这样的话其实同时只有2个ShuffleMapTask是并行执行的，因为一个cpu core同时只能执行一个ShuffleMapTask。</span><br><span class="line">2：每个ShuffleMapTask都会为每个ResultTask创建一份Bucket缓存，以及对应的ShuffleBlockFile磁盘文件</span><br><span class="line">这样的话，每一个ShuffleMapTask都会产生4份Bucket缓存和对应的4个ShuffleBlockFile文件，分别对应下面的4个ResultTask</span><br><span class="line"></span><br><span class="line">3：假设另一个节点上面运行了4个ResultTask现在等着获取ShuffleMapTask的输出数据，来完成比如ReduceByKey的操作。</span><br><span class="line">这是这个流程，注意了，如果有100个MapTask，100个ResultTask，那么会产生10000个本地磁盘文件，这样需要频繁的磁盘IO，是比较影响性能的。</span><br><span class="line"></span><br><span class="line">注意，那个bucket缓存是非常重要的，ShuffleMapTask会把所有的数据都写入Bucket缓存之后，才会刷写到对应的磁盘文件中，但是这就有一个问题，如果map端数据过多，那么很容易造成内存溢出，所以spark在优化后的Hash Based Shuffle中对这个问题进行了优化，默认这个内存缓存是100kb，当Bucket中的数据达到了阈值之后，就会将数据一点一点地刷写到对应的ShuffleBlockFile磁盘中了。这种操作的优点，是不容易发生内存溢出。缺点在于，如果内存缓存过小的话，那么可能发生过多的磁盘io操作。所以，这里的内存缓存大小，是可以根据实际的业务情况进行优化的。</span><br></pre></td></tr></table></figure><h3 id="优化后的Hash-Based-Shuffle"><a href="#优化后的Hash-Based-Shuffle" class="headerlink" title="优化后的Hash Based Shuffle"></a>优化后的Hash Based Shuffle</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271226135.png" alt="image-20230327122626812"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">看这个优化后的shuffle流程</span><br><span class="line">1：假设机器上有2个cpu，4个shuffleMaptask，这样同时只有2个在并行执行</span><br><span class="line">2：在这个版本中，Spark引入了consolidation机制，一个ShuffleMapTask将数据写入ResultTask数量的本地文件中，这个是不变的，但是当下一个ShuffleMapTask运行的时候，可以直接将数据写入之前产生的本地文件中，相当于对多个ShuffleMapTask的输出进行了合并，从而大大减少了本地磁盘中文件的数量。</span><br><span class="line">此时文件的数量变成了CPU core数量 * ResultTask数量，比如每个节点上有2个CPU，有100个ResultTask，那么每个节点上会产生200个文件</span><br><span class="line">这个时候文件数量就变得少多了。</span><br><span class="line"></span><br><span class="line">但是如果 ResultTask端的并行任务过多的话则 CPU core * Result Task 依旧过大，也会产生很多小文件</span><br></pre></td></tr></table></figure><h3 id="Sort-Based-Shuffle"><a href="#Sort-Based-Shuffle" class="headerlink" title="Sort-Based Shuffle"></a>Sort-Based Shuffle</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">引入Consolidation机制虽然在一定程度上减少了磁盘文件数量，但是不足以有效提高Shuffle的性能，这种情况只适合中小型数据规模的数据处理。</span><br><span class="line">为了让Spark能在更大规模的集群上高性能处理大规模的数据，因此Spark引入了 Sort-Based Shuffle。</span><br><span class="line"></span><br><span class="line">该机制针对每一个ShuffleMapTask都只创建一个文件，将所有的 ShuffleMapTask的数据都写入同一个文件，并且对应生成一个索引文件。</span><br><span class="line">以前的数据是放在内存中，等到数据写完了再刷写到磁盘，现在为了减少内存的使用，在内存不够用的时候，可以将内存中的数据溢写到磁盘，结束的时候，再将这些溢写的文件联合内存中的数据一起进行归并，从而减少内存的使用量。一方面文件数量显著减少，另一方面减少缓存所占用的内存大小，而且同时避免GC的风险和频率。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271434463.png" alt="image-20230327143450475"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>IDEA使用经验积累</title>
    <link href="http://tianyong.fun/IDEA%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E7%A7%AF%E7%B4%AF.html"/>
    <id>http://tianyong.fun/IDEA%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E7%A7%AF%E7%B4%AF.html</id>
    <published>2023-03-24T15:07:36.000Z</published>
    <updated>2023-03-24T16:04:07.886Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="IDEA使用经验积累"><a href="#IDEA使用经验积累" class="headerlink" title="IDEA使用经验积累"></a>IDEA使用经验积累</h1><h2 id="运行程序报错内存不够"><a href="#运行程序报错内存不够" class="headerlink" title="运行程序报错内存不够"></a>运行程序报错内存不够</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">run-&gt;edit configuration-&gt;vm option</span><br><span class="line">-Xms1024m -Xmx1024m</span><br></pre></td></tr></table></figure><h2 id="Maven创建项目"><a href="#Maven创建项目" class="headerlink" title="Maven创建项目"></a>Maven创建项目</h2><h3 id="编辑spark程序时"><a href="#编辑spark程序时" class="headerlink" title="编辑spark程序时"></a>编辑spark程序时</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">本地运行</span><br><span class="line">&lt;!--&lt;scope&gt;provided&lt;&#x2F;scope&gt;--&gt;</span><br><span class="line">这个表示使用相关spark依赖</span><br><span class="line"></span><br><span class="line">在spark集群运行时</span><br><span class="line">要把注释去掉，表示不使用相关spark依赖</span><br></pre></td></tr></table></figure><h2 id="快捷键"><a href="#快捷键" class="headerlink" title="快捷键"></a>快捷键</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctrl+alt+v:快速创建返回类型</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第16章 spark</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC16%E7%AB%A0-spark.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC16%E7%AB%A0-spark.html</id>
    <published>2023-02-28T14:35:14.000Z</published>
    <updated>2023-03-03T16:51:35.890Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第15章 hadoop架构再探讨</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC15%E7%AB%A0-hadoop%E6%9E%B6%E6%9E%84%E5%86%8D%E6%8E%A2%E8%AE%A8.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC15%E7%AB%A0-hadoop%E6%9E%B6%E6%9E%84%E5%86%8D%E6%8E%A2%E8%AE%A8.html</id>
    <published>2023-02-28T14:34:54.000Z</published>
    <updated>2023-03-03T16:51:17.421Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第14章 基于hadoop的数据仓库Hive</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC14%E7%AB%A0-%E5%9F%BA%E4%BA%8Ehadoop%E7%9A%84%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93Hive.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC14%E7%AB%A0-%E5%9F%BA%E4%BA%8Ehadoop%E7%9A%84%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93Hive.html</id>
    <published>2023-02-28T14:34:22.000Z</published>
    <updated>2023-03-03T16:50:57.288Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第11章 大数据在互联网中的应用</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC11%E7%AB%A0-%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9C%A8%E4%BA%92%E8%81%94%E7%BD%91%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC11%E7%AB%A0-%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9C%A8%E4%BA%92%E8%81%94%E7%BD%91%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8.html</id>
    <published>2023-02-28T14:33:23.000Z</published>
    <updated>2023-02-28T14:33:23.315Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第10章 数据可视化</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC10%E7%AB%A0-%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC10%E7%AB%A0-%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.html</id>
    <published>2023-02-28T14:32:57.000Z</published>
    <updated>2023-03-03T16:50:39.828Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第9章 图计算</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.html</id>
    <published>2023-02-28T14:32:32.000Z</published>
    <updated>2023-03-09T15:04:20.844Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第9章-图计算"><a href="#第9章-图计算" class="headerlink" title="第9章 图计算"></a>第9章 图计算</h1><h2 id="图计算简介"><a href="#图计算简介" class="headerlink" title="图计算简介"></a>图计算简介</h2><h3 id="图结构数据"><a href="#图结构数据" class="headerlink" title="图结构数据"></a>图结构数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">•许多大数据都是以大规模图或网络的形式呈现，如社交网络、传染病传播途径、交通事故对路网的影响</span><br><span class="line">•许多非图结构的大数据，也常常会被转换为图模型后进行分析</span><br><span class="line">•图数据结构很好地表达了数据之间的关联性</span><br><span class="line">•关联性计算是大数据计算的核心——通过获得数据的关联性，可以从噪音很多的海量数据中抽取有用的信息–比如，通过为购物者之间的关系建模，就能很快找到口味相似的用户，并为之推荐商品</span><br><span class="line">–或者在社交网络中，通过传播关系发现意见领袖</span><br></pre></td></tr></table></figure><h3 id="传统图计算解决方案的不足之处"><a href="#传统图计算解决方案的不足之处" class="headerlink" title="传统图计算解决方案的不足之处"></a>传统图计算解决方案的不足之处</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">很多传统的图计算算法都存在以下几个典型问题：</span><br><span class="line">（1）常常表现出比较差的内存访问局部性</span><br><span class="line">（2）针对单个顶点的处理工作过少</span><br><span class="line">（3）计算过程中伴随着并行度的改变</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">针对大型图（比如社交网络和网络图）的计算问题，可能的解决方案及其不足之处具体如下：</span><br><span class="line">•（1）为特定的图应用定制相应的分布式实现：通用性不好</span><br><span class="line">•（2）基于现有的分布式计算平台进行图计算：在性能和易用性方面往往无法达到最优</span><br><span class="line">•现有的并行计算框架像MapReduce还无法满足复杂的关联性计算</span><br><span class="line">•MapReduce作为单输入、两阶段、粗粒度数据并行的分布式计算框架，在表达多迭代、稀疏结构和细粒度数据时，力不从心</span><br><span class="line">•比如，有公司利用MapReduce进行社交用户推荐，对于5000万注册用户，50亿关系对，利用10台机器的集群，需要超过10个小时的计算</span><br><span class="line">•（3）使用单机的图算法库：比如BGL、LEAD、NetworkX、JDSL、Standford GraphBase和FGL等，但是，在可以解决的问题的规模方面具有很大的局限性</span><br><span class="line">•（4）使用已有的并行图计算系统：比如，Parallel BGL和CGM Graph，实现了很多并行图算法，但是，对大规模分布式系统非常重要的一些方面（比如容错），无法提供较好的支持</span><br></pre></td></tr></table></figure><h3 id="图计算通用软件"><a href="#图计算通用软件" class="headerlink" title="图计算通用软件"></a>图计算通用软件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">针对大型图的计算，目前通用的图计算软件主要包括两种：</span><br><span class="line">第一种主要是基于遍历算法的、实时的图数据库，如Neo4j、OrientDB、DEX和Infinite Graph</span><br><span class="line">第二种是以图顶点为中心的、基于消息传递批处理的并行引擎，如GoldenOrb、Giraph、Pregel和Hama，这些图处理软件主要是基于BSP模型实现的并行图处理系统</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">一次BSP(整体同步并行计算模型、又称大同步模型)计算过程包括一系列全局超步（所谓的超步就是计算中的一次迭代），每个超步主要包括三个组件：</span><br><span class="line">•局部计算：每个参与的处理器都有自身的计算任务，它们只读取存储在本地内存中的值，不同处理器的计算任务都是异步并且独立的</span><br><span class="line"></span><br><span class="line">•通讯：处理器群相互交换数据，交换的形式是，由一方发起推送(put)和获取(get)操作</span><br><span class="line"></span><br><span class="line">•栅栏同步(Barrier Synchronization)：当一个处理器遇到“路障”（或栅栏），会等到其他所有处理器完成它们的计算步骤；每一次同步也是一个超步的完成和下一个超步的开始。图9-1是一个超步的垂直结构图</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230308202813506.png" alt="image-20230308202813506" style="zoom:67%;"><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303082212630.png" alt="image-20230308221246547"></p><h2 id="Pregel简介"><a href="#Pregel简介" class="headerlink" title="Pregel简介"></a>Pregel简介</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">•谷歌公司在2003年到2004年公布了GFS、MapReduce和BigTable，成为后来云计算和Hadoop项目的重要基石</span><br><span class="line"></span><br><span class="line">•谷歌在后Hadoop时代的新“三驾马车”——Caffeine(大规模网页索引的建立)、Dremel(实时交互式分析产品)和Pregel，再一次影响着圈子与大数据技术的发展潮流</span><br><span class="line"></span><br><span class="line">•Pregel是一种基于BSP模型实现的并行图处理系统</span><br><span class="line">•为了解决大型图的分布式计算问题，Pregel搭建了一套可扩展的、有容错机制的平台，该平台提供了一套非常灵活的API，可以描述各种各样的图计算</span><br><span class="line">•Pregel作为分布式图计算的计算框架，主要用于图遍历、最短路径、PageRank计算等等</span><br></pre></td></tr></table></figure><h2 id="Pregel图计算模型"><a href="#Pregel图计算模型" class="headerlink" title="Pregel图计算模型"></a>Pregel图计算模型</h2><h3 id="有向图和顶点"><a href="#有向图和顶点" class="headerlink" title="有向图和顶点"></a>有向图和顶点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">•Pregel计算模型以有向图作为输入，有向图的每个顶点都有一个String类型的顶点ID，每个顶点都有一个可修改的用户自定义值与之关联，每条有向边都和其源顶点关联，并记录了其目标顶点ID，边上有一个可修改的用户自定义值与之关联</span><br><span class="line">•在每个超步S中，图中的所有顶点都会并行执行相同的用户自定义函数。每个顶点可以接收前一个超步(S-1)中发送给它的消息，修改其自身及其出射边的状态，并发送消息给其他顶点，甚至是修改整个图的拓扑结构。需要指出的是，在这种计算模式中，边并不是核心对象，在边上面不会运行相应的计算，只有顶点才会执行用户自定义函数进行相应计算</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303091325684.png" alt="image-20230309132510569"></p><h3 id="顶点之间的消息传递"><a href="#顶点之间的消息传递" class="headerlink" title="顶点之间的消息传递"></a>顶点之间的消息传递</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">采用消息传递模型主要基于以下两个原因：</span><br><span class="line">（1）消息传递具有足够的表达能力，没有必要使用远程读取或共享内存的方式</span><br><span class="line">（2）有助于提升系统整体性能。大型图计算通常是由一个集群完成的，集群环境中执行远程数据读取会有较高的延迟；Pregel的消息模式采用异步和</span><br><span class="line">批量的方式传递消息，因此可以缓解远程读取的延迟</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230308203709811.png" alt="image-20230308203709811" style="zoom:67%;"><h3 id="Pregel的计算过程"><a href="#Pregel的计算过程" class="headerlink" title="Pregel的计算过程"></a>Pregel的计算过程</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">•Pregel的计算过程是由一系列被称为“超步”的迭代组成的。</span><br><span class="line"></span><br><span class="line">在每个超步中，每个顶点上面都会并行执行用户自定义的函数，该函数描述了一个顶点V在一个超步S中需要执行的操作。</span><br><span class="line"></span><br><span class="line">该函数可以读取前一个超步(S-1)中其他顶点发送给顶点V的消息，执行相应计算后，修改顶点V及其出射边的状态，然后沿着顶点V的出射边发送消息给其他顶点，而且，一个消息可能经过多条边的传递后被发送到任意已知ID的目标顶点上去。</span><br><span class="line"></span><br><span class="line">这些消息将会在下一个超步(S+1)中被目标顶点接收，然后像上述过程一样开始下一个超步(S+1)的迭代过程</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">•在Pregel计算过程中，一个算法什么时候可以结束，是由所有顶点的状态决定的</span><br><span class="line">•在第0个超步，所有顶点处于活跃状态，都会参与该超步的计算过程</span><br><span class="line">•当一个顶点不需要继续执行进一步的计算时，就会把自己的状态设置为“停机”，进入非活跃状态</span><br><span class="line">•一旦一个顶点进入非活跃状态，后续超步中就不会再在该顶点上执行计算，除非其他顶点给该顶点发送消息把它再次激活</span><br><span class="line">•当一个处于非活跃状态的顶点收到来自其他顶点的消息时，Pregel计算框架必须根据条件判断来决定是否将其显式唤醒进入活跃状态</span><br><span class="line">•当图中所有的顶点都已经标识其自身达到“非活跃（inactive）”状态，并且没有消息在传送的时候，算法就可以停止运行</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230308204037030.png" alt="image-20230308204037030" style="zoom:67%;"><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230308204100764.png" alt="image-20230308204100764" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">发送出去的消息，会在下一个超步执行，每一个顶点都有一个输入队列，队列中的值来源于上一个超步，每一个超步执行时去取自己输入队列中的值</span><br></pre></td></tr></table></figure><h2 id="Pregel的C-API"><a href="#Pregel的C-API" class="headerlink" title="Pregel的C++ API"></a>Pregel的C++ API</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Pregel已经预先定义好一个基类——Vertex类：</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">template &lt;typename VertexValue, typename EdgeValue, typename MessageValue&gt;</span><br><span class="line">class Vertex &#123;</span><br><span class="line"> public:</span><br><span class="line">    virtual void Compute(MessageIterator* msgs) &#x3D; 0;</span><br><span class="line">    const string&amp; vertex_id() const; &#x2F;&#x2F; 顶点ID</span><br><span class="line">    int64 superstep() const; &#x2F;&#x2F; 超步是第几步</span><br><span class="line">    const VertexValue&amp; GetValue(); &#x2F;&#x2F; 获得顶点的值</span><br><span class="line">    VertexValue* MutableValue(); &#x2F;&#x2F; 修改顶点值</span><br><span class="line">    OutEdgeIterator GetOutEdgeIterator();&#x2F;&#x2F; 获得该顶点出射边</span><br><span class="line">    void SendMessageTo(const string&amp; dest_vertex, const MessageValue&amp; message);</span><br><span class="line">    void VoteToHalt();&#x2F;&#x2F; 修改状态</span><br><span class="line"> &#125;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">•在Vetex类中，定义了三个值类型参数，分别表示顶点、边和消息。每一个顶点都有一个给定类型的值与之对应</span><br><span class="line">•编写Pregel程序时，需要继承Vertex类，并且覆写Vertex类的虚函数Compute() </span><br><span class="line"></span><br><span class="line">•在Pregel执行计算过程时，在每个超步中都会并行调用每个顶点上定义的Compute()函数</span><br><span class="line">•允许Compute()方法查询当前顶点及其边的信息，以及发送消息到其他的顶点</span><br><span class="line">    –Compute()方法可以调用GetValue()方法来获取当前顶点的值</span><br><span class="line">    –调用MutableValue()方法来修改当前顶点的值</span><br><span class="line">    –通过由出射边的迭代器提供的方法来查看、修改出射边对应的值</span><br><span class="line">•对状态的修改，对于被修改的顶点而言是可以立即被看见的，但是，对于其他顶点而言是不可见的，因此，不同顶点并发进行的数据访问是不存在竞争关系的</span><br><span class="line">整个过程中，唯一需要在超步之间持久化的顶点级状态，是顶点和其对</span><br><span class="line">应的边所关联的值，因而，Pregel计算框架所需要管理的图状态就只包括顶点和边所关联的值，这种做法大大简化了计算流程，同时，也有利于图的分布和故障恢复</span><br></pre></td></tr></table></figure><h3 id="消息传递机制"><a href="#消息传递机制" class="headerlink" title="消息传递机制"></a>消息传递机制</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 顶点之间的通讯是借助于消息传递机制来实现的，每条消息都包含了消息值和需要到达的目标顶点ID。用户可以通过Vertex类的模板参数来设定消息值的数据类型</span><br><span class="line">• 在一个超步S中，一个顶点可以发送任意数量的消息，这些消息将在下一个超步（S+1）中被其他顶点接收</span><br><span class="line">• 一个顶点V通过与之关联的出射边向外发送消息，并且，消息要到达的目标顶点并不一定是与顶点V相邻的顶点，一个消息可以连续经过多条连通的边到达某个与顶点V不相邻的顶点U，U可以从接收的消息中获取到与其不相邻的顶点V的ID</span><br></pre></td></tr></table></figure><h3 id="Combiner"><a href="#Combiner" class="headerlink" title="Combiner"></a>Combiner</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">• Pregel计算框架在消息发出去之前，Combiner可以将发往同一个顶点的多个整型值进行求和得到一个值，只需向外发送这个“求和结果”，从而实现了由多个消息合并成一个消息，大大减少了传输和缓存的开销</span><br><span class="line">• 在默认情况下，Pregel计算框架并不会开启Combiner功能，因为，通常很难找到一种对所有顶点的Compute()函数都合适的Combiner</span><br><span class="line">• 当用户打算开启Combiner功能时，可以继承Combiner类并覆写虚函数Combine()</span><br><span class="line">• 此外，通常只对那些满足交换律和结合律的操作才可以去开启Combiner功能，因为，Pregel计算框架无法保证哪些消息会被合并，也无法保证消息传递给 Combine()的顺序和合并操作执行的顺序</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230308204959085.png" alt="image-20230308204959085" style="zoom:67%;"><h3 id="Aggregator"><a href="#Aggregator" class="headerlink" title="Aggregator"></a>Aggregator</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">• Aggregator提供了一种全局通信、监控和数据查看的机制</span><br><span class="line">• 在一个超步S中，每一个顶点都可以向一个Aggregator提供一个数据，Pregel计算框架会对这些值进行聚合操作产生一个值，在下一个超步（S+1）中，图中的所有顶点都可以看见这个值</span><br><span class="line">• Aggregator的聚合功能，允许在整型和字符串类型上执行最大值、最小值、求和操作，比如，可以定义一个“Sum”Aggregator来统计每个顶点的出射边数量，最后相加可以得到整个图的边的数量</span><br><span class="line">• Aggregator还可以实现全局协同的功能，比如，可以设计“and” Aggregator来决定在某个超步中Compute()函数是否执行某些逻辑分支，只有当“and” Aggregator显示所有顶点都满足了某条件时，才去执行这些逻辑分支</span><br></pre></td></tr></table></figure><h3 id="拓扑改变"><a href="#拓扑改变" class="headerlink" title="拓扑改变"></a>拓扑改变</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• Pregel计算框架允许用户在自定义函数Compute()中定义操作，修改图的拓扑结构，比如在图中增加（或删除）边或顶点</span><br><span class="line">• 对于全局拓扑改变，Pregel采用了惰性协调机制，在改变请求发出时，Pregel不会对这些操作进行协调，只有当这些改变请求的消息到达目标顶点并被执行时，Pregel才会对这些操作进行协调，这样，所有针对某个顶点V的拓扑修改操作所引发的冲突，都会由V自己来处理</span><br><span class="line">• 对于本地的局部拓扑改变，是不会引发冲突的，顶点或边的本地增减能够立即生效，很大程度上简化了分布式编程</span><br></pre></td></tr></table></figure><h3 id="输入和输出"><a href="#输入和输出" class="headerlink" title="输入和输出"></a>输入和输出</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 在Pregel计算框架中，图的保存格式多种多样，包括文本文件、关系数据库或键值数据库等</span><br><span class="line">• 在Pregel中，“从输入文件生成得到图结构”和“执行图计算”这两个过程是分离的，从而不会限制输入文件的格式</span><br><span class="line">• 对于输出，Pregel也采用了灵活的方式，可以以多种方式进行输出</span><br></pre></td></tr></table></figure><h2 id="Pregel的体系结构"><a href="#Pregel的体系结构" class="headerlink" title="Pregel的体系结构"></a>Pregel的体系结构</h2><h3 id="Pregel的执行过程"><a href="#Pregel的执行过程" class="headerlink" title="Pregel的执行过程"></a>Pregel的执行过程</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">•在Pregel计算框架中，一个大型图会被划分成许多个分区，每个分区都包含了一部分顶点以及以其为起点的边</span><br><span class="line">•一个顶点应该被分配到哪个分区上，是由一个函数决定的，系统默认函数为hash(ID) mod N，其中，N为所有分区总数，ID是这个顶点的标识符；当然，用户也可以自己定义这个函数</span><br><span class="line">•这样，无论在哪台机器上，都可以简单根据顶点ID判断出该顶点属于哪个分区，即使该顶点可能已经不存在了</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230308210115360.png" alt="image-20230308210115360" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">在理想的情况下（不发生任何错误），一个Pregel用户程序的执行过程如下：</span><br><span class="line">（1）选择集群中的多台机器执行图计算任务，每台机器上运行用户程序的一个副本，其中，有一台机器会被选为Master，其他机器作为Worker。Master只负责协调多个Worker执行任务，系统不会把图的任何分区分配给它。Worker借助于名称服务系统可以定位到Master的位置，并向Master发送自己的注册信息。</span><br><span class="line">（2）Master把一个图分成多个分区，并把分区分配到多个Worker。一个Worker会领到一个或多个分区，每个</span><br><span class="line">Worker知道所有其他Worker所分配到的分区情况。每个</span><br><span class="line">Worker负责维护分配给自己的那些分区的状态(顶点及边</span><br><span class="line">的增删)，对分配给自己的分区中的顶点执行Compute()函</span><br><span class="line">数，向外发送消息，并管理接收到的消息。</span><br><span class="line">（3）Master会把用户输入划分成多个部分，通常是基于文件边界进行划分。划分后，每个部分都是一系列记录的集合，每条记录都包含一定数量的顶点和边。然后，Master会为每个Worker分配用户输入的一部分。如果一个Worker从输入内容中加载到的顶点，刚好是自己所分配到的分区中的顶点，就会立即更新相应的数据结构。否则，该Worker会根据加载到的顶点的ID，把它发送到其所属的分区所在的Worker上。当所有的输入都被加载后，图中的所有顶点都会被标记为“活跃”状态。</span><br><span class="line">（4）Master向每个Worker发送指令，Worker收到指令后，开始运行一个超步。Worker会为自己管辖的每个分区分配一个线程，对于分区中的每个顶点，Worker会把来自上一个超步的、发给该顶点的消息传递给它，并调用处于“活跃”状态的顶点上的Compute()函数，在执行计算过程中，顶点可以对外发送消息，但是，所有消息的发送工作必须在本超步结束之前完成。当所有这些工作都完成以后，Worker会通知Master，并把自己在下一个超步还处于“活跃”状态的顶点的数量报告给Master。上述步骤会被不断重复，直到所有顶点都不再活跃并且系统中不会有任何消息在传输，这时，执行过程才会结束。</span><br><span class="line">（5）计算过程结束后，Master会给所有的Worker发送指令，通知每个Worker对自己的计算结果进行持久化存储。</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230308210550499.png" alt="image-20230308210550499" style="zoom:67%;"><h3 id="容错性"><a href="#容错性" class="headerlink" title="容错性"></a>容错性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• Pregel采用检查点机制来实现容错。在每个超步的开始，Master会通知所有的Worker把自己管辖的分区的状态（包括顶点值、边值以及接收到的消息），写入到持久化存储设备</span><br><span class="line">• Master会周期性地向每个Worker发送ping消息，Worker收到ping消息后会给Master发送反馈消息。如果Master在指定时间间隔内没有收到某个Worker的反馈消息，就会把该Worker标记为“失效”。同样地，如果一个Worker在指定的时间间隔内没有收到来自Master的ping消息，该Worker也会停止工作</span><br><span class="line">• 每个Worker上都保存了一个或多个分区的状态信息，当一个Worker发生故障时，它所负责维护的分区的当前状态信息就会丢失。Master监测到一个Worker发生故障“失效”后，会把失效Worker所分配到的分区，重新分配到其他处于正常工作状态的Worker集合上，然后，所有这些分区会从最近的某超步S开始时写出的检查点中，重新加载状态信息。很显然，这个超步S可能会比失效Worker上最后运行的超步S1要早好几个阶段，因此，为了恢复到最新的正确状态，需要重新执行从超步S到超步S1的所有操作</span><br></pre></td></tr></table></figure><h3 id="Worker"><a href="#Worker" class="headerlink" title="Worker"></a>Worker</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">在一个Worker中，它所管辖的分区的状态信息是保存在内存中的。分区中的顶点的状态信息包括：</span><br><span class="line">•顶点的当前值</span><br><span class="line">•以该顶点为起点的出射边列表，每条出射边包含了目标顶点ID和边的值</span><br><span class="line">•消息队列，包含了所有接收到的、发送给该顶点的消息</span><br><span class="line">•标志位，用来标记顶点是否处于活跃状态</span><br><span class="line"></span><br><span class="line">在每个超步中，Worker会对自己所管辖的分区中的每个顶点进行遍历，并调用顶点上的Compute()函数，在调用时，会把以下三个参数传递进去：</span><br><span class="line">•该顶点的当前值</span><br><span class="line">•一个接收到的消息的迭代器</span><br><span class="line">•一个出射边的迭代器</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">•在Pregel中，为了获得更好的性能，“标志位”和输入消息</span><br><span class="line">队列是分开保存的</span><br><span class="line">•对于每个顶点而言，Pregel只保存一份顶点值和边值，但是，会保存两份“标志位”和输入消息队列，分别用于当前超步和下一个超步</span><br><span class="line">•在超步S中，当一个Worker在进行顶点处理时，用于当前超步的消息会被处理，同时，它在处理过程中还会接收到来自其他Worker的消息，这些消息会在下一个超步S+1中被处理，因此，需要两个消息队列用于存放作用于当前超步S的消息和作用于下一个超步S+1的消息</span><br><span class="line">•如果一个顶点V在超步S接收到消息，那么，它表示V将会在下一个超步S+1中（而不是当前超步S中）处于“活跃”状态</span><br><span class="line">•当一个Worker上的一个顶点V需要发送消息到其他顶点U时，该Worker会首先判断目标顶点U是否位于自己机器上</span><br><span class="line">•如果目标顶点U在自己的机器上，就直接把消息放入到与目标顶点U对应的输入消息队列中</span><br><span class="line">•如果发现目标顶点U在远程机器上，这个消息就会被暂时缓存到本地，当缓存中的消息数目达到一个事先设定的阈值时，这些缓存消息会被批量异步发送出去，传输到目标顶点所在的Worker上</span><br><span class="line">•如果存在用户自定义的Combiner操作，那么，当消息被加入到输出队列或者到达输入队列时，就可以对消息执行合并操作，这样可以节省存储空间和网络传输开销</span><br></pre></td></tr></table></figure><h3 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">•Master主要负责协调各个Worker执行任务，每个Worker会借助于名称服务系统定位到Master的位置，并向Master发送自己的注册信息，Master会为每个Worker分配一个唯一的ID</span><br><span class="line">•Master维护着关于当前处于“有效”状态的所有Worker的各种信息，包括每个Worker的ID和地址信息，以及每个Worker被分配到的分区信息</span><br><span class="line">•虽然在集群中只有一个Master，但是，它仍然能够承担起一个大规模图计算的协调任务，这是因为Master中保存这些信息的数据结构的大小，只与分区的数量有关，而与顶点和边的数量无关</span><br><span class="line">•一个大规模图计算任务会被Master分解到多个Worker去执行，在每个超步开始时，Master都会向所有处于“有效”状态的Worker发送相同的指令，然后等待这些Worker的回应</span><br><span class="line">•如果在指定时间内收不到某个Worker的反馈，Master就认为这个Worker失效</span><br><span class="line">•如果参与任务执行的多个Worker中的任意一个发生了故障失效，Master就会进入恢复模式</span><br><span class="line">•在每个超步中，图计算的各种工作，比如输入、输出、计算、保存和从检查点中恢复，都会在“路障（barrier）”之前结束</span><br><span class="line">•如果路障同步成功，说明一个超步顺利结束，Master就会进入下一个处理阶段，图计算进入下一个超步的执行</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">•Master在内部运行了一个HTTP服务器来显示图计算过程的各种信息</span><br><span class="line">•用户可以通过网页随时监控图计算执行过程各个细节</span><br><span class="line">•图的大小</span><br><span class="line">•关于出度分布的柱状图</span><br><span class="line">•处于活跃状态的顶点数量</span><br><span class="line">•在当前超步的时间信息和消息流量</span><br><span class="line">•所有用户自定义Aggregator的值</span><br></pre></td></tr></table></figure><h3 id="Aggregator-1"><a href="#Aggregator-1" class="headerlink" title="Aggregator"></a>Aggregator</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">• 每个用户自定义的Aggregator都会采用聚合函数对一个值集合进行聚合计算得到一个全局值</span><br><span class="line">• 每个Worker都保存了一个Aggregator的实例集，其中的每个实例都是由类型名称和实例名称来标识的</span><br><span class="line">• 在执行图计算过程的某个超步S中，每个Worker会利用一个Aggregator对当前本地分区中包含的所有顶点的值进行归约，得到一个本地的局部归约值</span><br><span class="line">• 在超步S结束时，所有Worker会将所有包含局部归约值的Aggregator的值进行最后的汇总，得到全局值，然后提交给Master</span><br><span class="line">• 在下一个超步S+1开始时，Master就会将Aggregator的全局值发送给每个Worker</span><br></pre></td></tr></table></figure><h2 id="Pregel的应用实例"><a href="#Pregel的应用实例" class="headerlink" title="Pregel的应用实例"></a>Pregel的应用实例</h2><h3 id="单源最短路径"><a href="#单源最短路径" class="headerlink" title="单源最短路径"></a>单源最短路径</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303090019737.png" alt="image-20230309001948663"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class ShortestPathVertex</span><br><span class="line"> : public Vertex&lt;int, int, int&gt; &#123;</span><br><span class="line"> void Compute(MessageIterator* msgs) &#123;</span><br><span class="line">     int mindist &#x3D; IsSource(vertex_id()) ? 0 : INF;</span><br><span class="line">     for (; !msgs-&gt;Done(); msgs-&gt;Next())</span><br><span class="line">     mindist &#x3D; min(mindist, msgs-&gt;Value());</span><br><span class="line">     if (mindist &lt; GetValue()) &#123;</span><br><span class="line">     *MutableValue() &#x3D; mindist;</span><br><span class="line">     OutEdgeIterator iter &#x3D; GetOutEdgeIterator();</span><br><span class="line">     for (; !iter.Done(); iter.Next())</span><br><span class="line">     SendMessageTo(iter.Target(),</span><br><span class="line">     mindist + iter.GetValue());</span><br><span class="line"> &#125;</span><br><span class="line"> VoteToHalt();</span><br><span class="line"> &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1 class ShortestPathVertex</span><br><span class="line">2 : public Vertex&lt;int, int, int&gt; &#123;</span><br><span class="line">3 void Compute(MessageIterator* msgs) &#123;</span><br><span class="line">4 int mindist &#x3D; IsSource(vertex_id()) ? 0 : INF;</span><br><span class="line">5 for (; !msgs-&gt;Done(); msgs-&gt;Next())</span><br><span class="line">6 mindist&#x3D; min(mindist, msgs-&gt;Value());</span><br><span class="line">7 if (mindist &lt; GetValue()) &#123;</span><br><span class="line">8 *MutableValue() &#x3D; mindist;</span><br><span class="line">9 OutEdgeIteratoriter &#x3D; GetOutEdgeIterator();</span><br><span class="line">10 for (; !iter.Done(); iter.Next())</span><br><span class="line">11 SendMessageTo(iter.Target(),</span><br><span class="line">12 mindist+ iter.GetValue());</span><br><span class="line">13 &#125;</span><br><span class="line">14 VoteToHalt();</span><br><span class="line">15 &#125;</span><br><span class="line">16 &#125;;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303091405728.png" alt="image-20230309140534677"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303091405163.png" alt="image-20230309140546109"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303091414778.png" alt="image-20230309141445727"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">超步1：</span><br><span class="line">•顶点0：没有收到消息，依然非活跃</span><br><span class="line">•顶点1：收到消息100（唯一消息），被显式唤醒，执行计算，mindist变为100，小于顶点值INF，顶点值修改为100，没有出射边，不需要发送消息，最后变为非活跃</span><br><span class="line">•顶点2：收到消息30，被显式唤醒，执行计算， mindist变为30，小于顶点值INF，顶点值修改为30，有两条出射边，向顶点3发送消息90（即：30+60），向顶点1发送消息90（即：30+60），最后变为非活跃</span><br><span class="line">•顶点3：没有收到消息，依然非活跃</span><br><span class="line">•顶点4：收到消息10，被显式唤醒，执行计算， mindist变为10，小于顶点值INF，顶点值修改为10，向顶点3发送消息60（即：10+50），最后变为非活跃</span><br><span class="line">剩余超步省略……</span><br><span class="line">当所有顶点非活跃，并且没有消息传递，就结束</span><br></pre></td></tr></table></figure><h3 id="二分匹配"><a href="#二分匹配" class="headerlink" title="二分匹配"></a>二分匹配</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">程序的执行过程是由四个阶段组成的多个循环组成的，当程序执行到超步S时，S mod 4就可以得到当前超步处于循环的哪个阶段。每个循环的四个阶段如下：</span><br><span class="line"> （1）阶段0：对于左集合中的任意顶点V，如果V还没有被匹配，就发送消息给它的每个邻居顶点请求匹配，然后，顶点V会调用VoteToHalt()进入“非活跃”状态。如果顶点V已经找到了匹配，或者V没有找到匹配但是没有出射边，那么，顶点V就不会发送消息。当顶点V没有发送消息，或者顶点V发送了消息但是所有的消息接收者都已经被匹配，那么，该顶点就不会再变为“活跃（active）”状态</span><br><span class="line"> （2）阶段1：对于右集合中的任意顶点U，如果它还没有被匹配，则会随机选择它接收到的消息中的其中一个，并向左集合中的消息发送者发送消息表示接受该匹配请求，然后给左集合中的其他请求者发送拒绝消息；然后，顶点U会调用VoteToHalt()进入“非活跃”状态</span><br><span class="line"> （3）阶段2：左集合中那些还未被匹配的顶点，会从它所收到的、右集合发送过来的接受请求中，选择其中一个给予确认，并发送一个确认消息。对于左集合中已经匹配的顶点而言，因为它们在阶段0不会向右集合发送任何匹配请求消息，因而也不会接收到任何来自右集合的匹配接受消息，因此，是不会执行阶段2的</span><br><span class="line"> （4）阶段3：右集合中还未被匹配的任意顶点U，会收到来自左集合的匹配确认消息，但是，每个未匹配的顶点U，最多会收到一个确认消息。然后，顶点U会调VoteToHalt()进入“非活跃”状态，完成它自身的匹配工作</span><br></pre></td></tr></table></figure><h2 id="Pregel和MapReduce实现PageRank算法的对比"><a href="#Pregel和MapReduce实现PageRank算法的对比" class="headerlink" title="Pregel和MapReduce实现PageRank算法的对比"></a>Pregel和MapReduce实现PageRank算法的对比</h2><h3 id="PageRank算法"><a href="#PageRank算法" class="headerlink" title="PageRank算法"></a>PageRank算法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• PageRank是一个函数，它为网络中每个网页赋一个权值。通过该权值来判断该网页的重要性</span><br><span class="line">• 该权值分配的方法并不是固定的，对PageRank算法的一些简单变形都会改变网页的相对PageRank值（PR值）</span><br><span class="line">• PageRank作为谷歌的网页链接排名算法，基本公式如下：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303082125420.png" alt="image-20230308212003706"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 对于任意一个网页链接，其PR值为链入到该链接的源链接的PR值对该链接的贡献和，其中，N表示该网络中所有网页的数量，Ni为第i个源链接的链出度，PRi表示第i个源链接的PR值</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 网络链接之间的关系可以用一个连通图来表示，下图就是四个网页（A,B,C,D）互相链入链出组成的连通图，从中可以看出，网页A中包含指向网页B、C和D的外链，网页B和D是网页A的源链接</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303082125422.png" alt="image-20230308212407285"></p><h3 id="PageRank算法在Pregel中的实现"><a href="#PageRank算法在Pregel中的实现" class="headerlink" title="PageRank算法在Pregel中的实现"></a>PageRank算法在Pregel中的实现</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">• 在Pregel计算模型中，图中的每个顶点会对应一个计算单元，每个计算单元包含三个成员变量：</span><br><span class="line">顶点值（Vertex value）：顶点对应的PR值</span><br><span class="line">出射边（Out edge）：只需要表示一条边，可以不取值</span><br><span class="line">消息（Message）：传递的消息，因为需要将本顶点对其它顶点的PR贡献值，传递给目标顶点</span><br><span class="line">• 每个计算单元包含一个成员函数Compute()，该函数定义了顶点上的运算，包括该顶点的PR值计算，以及从该顶点发送消息到其链出顶点</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class PageRankVertex: public Vertex&lt;double, void, double&gt; &#123;</span><br><span class="line">public:</span><br><span class="line">     virtual void Compute(MessageIterator* msgs) &#123;</span><br><span class="line">        if (superstep() &gt;&#x3D; 1) &#123;</span><br><span class="line">            double sum &#x3D; 0;</span><br><span class="line">            for (;!msgs-&gt;Done(); msgs-&gt;Next())</span><br><span class="line">            sum +&#x3D; msgs-&gt;Value();</span><br><span class="line">            *MutableValue() &#x3D;</span><br><span class="line">            0.15 &#x2F; NumVertices() + 0.85 * sum;</span><br><span class="line">        &#125;</span><br><span class="line">        if (superstep() &lt; 30) &#123;</span><br><span class="line">             const int64 n &#x3D; GetOutEdgeIterator().size();</span><br><span class="line">             SendMessageToAllNeighbors(GetValue()&#x2F; n);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">        VoteToHalt();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">• PageRankVertex继承自Vertex类，顶点值类型是double，用来保存PageRank中间值，消息类型也是double，用来传输PageRank值，边的value类型是void，因为不需要存储任何信息</span><br><span class="line">• 这里假设在第0个超步时，图中各顶点值被初始化为1&#x2F;NumVertices()，其中，NumVertices()表示顶点数目</span><br><span class="line">• 在前30个超步中，每个顶点都会沿着它的出射边，发送它的PageRank值除以出射边数目以后的结果值。从第1个超步开始，每个顶点会将到达的消息中的值加到sum值中，同时将它的PageRank值设为0.15&#x2F;NumVertices()+0.85*sum</span><br><span class="line">• 到了第30个超步后，就没有需要发送的消息了，同时所有的顶点停止计算，得到最终结果</span><br></pre></td></tr></table></figure><h3 id="PageRank算法在MapReduce中的实现"><a href="#PageRank算法在MapReduce中的实现" class="headerlink" title="PageRank算法在MapReduce中的实现"></a>PageRank算法在MapReduce中的实现</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">• MapReduce也是谷歌公司提出的一种计算模型，它是为全量计算而设计</span><br><span class="line">• 采用MapReduce实现PageRank的计算过程包括三个阶段：</span><br><span class="line">     第一阶段：解析网页</span><br><span class="line">     第二阶段：PageRank分配</span><br><span class="line">     第三阶段：收敛阶段</span><br></pre></td></tr></table></figure><h4 id="阶段1：解析网页"><a href="#阶段1：解析网页" class="headerlink" title="阶段1：解析网页"></a>阶段1：解析网页</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 该阶段的任务就是分析一个页面的链接数并赋初值。</span><br><span class="line">• 一个网页可以表示为由网址和内容构成的键值对&lt; URL，page content&gt;，作为Map任务的输入。阶段1的Map任务把&lt;URL，page content&gt;映射为&lt;URL，&lt;PRinit，url_list&gt;&gt;后进行输出，其中，PRinit是该URL页面对应的PageRank初始值，url_list包含了该URL页面中的外链所指向的所有URL。Reduce任务只是恒等函数，输入和输出相同。</span><br><span class="line">• 对右图，每个网页的初始PageRank值为1&#x2F;4。它在该阶段中:</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303082138941.png" alt="image-20230308213807879" style="zoom:67%;"><h4 id="阶段2：PageRank分配"><a href="#阶段2：PageRank分配" class="headerlink" title="阶段2：PageRank分配"></a>阶段2：PageRank分配</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">• 该阶段的任务就是多次迭代计算页面的PageRank值。</span><br><span class="line">• 在该阶段中，Map任务的输入是&lt;URL，&lt;cur_rank，url_list&gt;&gt;，其中，cur_rank是该</span><br><span class="line">URL页面对应的PageRank当前值，url_list包含了该URL页面中的外链所指向的所有</span><br><span class="line">URL。</span><br><span class="line">• 对于url_list中的每个元素u，Map任务输出&lt;u，&lt;URL, cur_rank&#x2F;|url_list|&gt;&gt;（其中，|url_list|表示外链的个数），并输出链接关系&lt;URL，url_list&gt;。</span><br><span class="line">• 每个页面的PageRank当前值被平均分配给了它们的每个外链。Map任务的输出会作为下面Reduce任务的输入。对下图第一次迭代Map任务的输入输出如下：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303082143692.png" alt="image-20230308214310636"></p><h4 id="阶段3：收敛阶段"><a href="#阶段3：收敛阶段" class="headerlink" title="阶段3：收敛阶段"></a>阶段3：收敛阶段</h4><h3 id="PageRank算法在Pregel和MapReduce中实现的比较"><a href="#PageRank算法在Pregel和MapReduce中实现的比较" class="headerlink" title="PageRank算法在Pregel和MapReduce中实现的比较"></a>PageRank算法在Pregel和MapReduce中实现的比较</h3><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第8章 流计算</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC8%E7%AB%A0-%E6%B5%81%E8%AE%A1%E7%AE%97.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC8%E7%AB%A0-%E6%B5%81%E8%AE%A1%E7%AE%97.html</id>
    <published>2023-02-28T14:32:22.000Z</published>
    <updated>2023-03-08T12:14:57.695Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第8章-流计算"><a href="#第8章-流计算" class="headerlink" title="第8章 流计算"></a>第8章 流计算</h1><h2 id="流计算概述"><a href="#流计算概述" class="headerlink" title="流计算概述"></a>流计算概述</h2><h3 id="静态数据和流数据"><a href="#静态数据和流数据" class="headerlink" title="静态数据和流数据"></a>静态数据和流数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">很多企业为了支持决策分析而构建的数据仓库系统，其中存放的大量历史数据就是静态数据。技术人员可以利用数据挖掘和OLAP（OnLine Analytical Processing）分析工具从静态数据中找到对企业有价值的信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081458289.png" alt="image-20230308145838198"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">• 近年来，在Web应用、网络监控、传感监测等领域，兴起了一种新的数据密集型应用——流数据，即数据以大量、快速、时变的流形式持续到达</span><br><span class="line">• 流数据具有如下特征：</span><br><span class="line">– 数据快速持续到达，潜在大小也许是无穷无尽的</span><br><span class="line">– 数据来源众多，格式复杂</span><br><span class="line">– 数据量大，但是不十分关注存储，一旦经过处理，要么被丢弃，要么被归档存储</span><br><span class="line">– 注重数据的整体价值，不过分关注个别数据</span><br><span class="line">– 数据顺序颠倒，或者不完整，系统无法控制将要处理的新到达的数据元素的顺序</span><br></pre></td></tr></table></figure><h3 id="批量计算和实时计算"><a href="#批量计算和实时计算" class="headerlink" title="批量计算和实时计算"></a>批量计算和实时计算</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 对静态数据和流数据的处理，对应着两种截然不同的计算模式：批量计算和实时计算</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081503278.png" alt="image-20230308150351231"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">• 批量计算以“静态数据”为对象，可在充裕的时间内对海量数据进行批量处理，计算得到有价值的信息。Hadoop是典型的批处理模型，由HDFS和HBase存放大量的静态数据，由MapReduce负责对海量数据执行批量计算</span><br><span class="line">• 流数据须采用实时计算。实时计算最重要的一个需求是能够实时得到计算结果，一般要求响应时间为秒级。当只需要处理少量数据时，实时计算并不是问题；但是，在大数据时代，数据格式复杂、来源众多、数据量巨大，对实时计算提出了很大的挑战。因此，针对流数据的实时计算——流计算，应运而生</span><br></pre></td></tr></table></figure><h3 id="流计算概念"><a href="#流计算概念" class="headerlink" title="流计算概念"></a>流计算概念</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 流计算：实时获取来自不同数据源的海量数据，经过实时分析处理，获得有价值的信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081512277.png" alt="image-20230308151223180"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">• 流计算秉承一个基本理念，即数据的价值随着时间的流逝而降低。因此，当事件出现时就应该立即进行处理，而不是缓存起来进行批量处理。为了及时处理流数据，就需要一个低延迟、可扩展、高可靠的处理引擎</span><br><span class="line">• 对于一个流计算系统来说，它应达到如下需求：</span><br><span class="line">– 高性能：处理大数据的基本要求，如每秒处理几十万条数据</span><br><span class="line">– 海量式：支持TB级甚至是PB级的数据规模</span><br><span class="line">– 实时性：保证较低的延迟时间，达到秒级别，甚至是毫秒级别</span><br><span class="line">– 分布式：支持大数据的基本架构，必须能够平滑扩展</span><br><span class="line">– 易用性：能够快速进行开发和部署</span><br><span class="line">– 可靠性：能可靠地处理流数据</span><br></pre></td></tr></table></figure><h3 id="流计算与Hadoop"><a href="#流计算与Hadoop" class="headerlink" title="流计算与Hadoop"></a>流计算与Hadoop</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• Hadoop设计的初衷是面向大规模数据的批量处理，每台机器并行运行MapReduce任务，最后对结果进行汇总输出</span><br><span class="line">• MapReduce是专门面向静态数据的批量处理的，内部各种实现机制都为批处理做了高度优化，不适合用于处理持续到达的动态数据</span><br><span class="line">• 我们可能会想到一种“变通”的方案来降低批处理的时间延迟——将基于MapReduce的批量处理转为小批量处理，将输入数据切成小的片段，每隔一个周期就启动一次MapReduce作业。但这种方式也无法有效处理流数据</span><br></pre></td></tr></table></figure><h3 id="流计算框架"><a href="#流计算框架" class="headerlink" title="流计算框架"></a>流计算框架</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">• 当前业界诞生了许多专门的流数据实时计算系统来满足各自需求</span><br><span class="line">• 目前有三类常见的流计算框架和平台：商业级的流计算平台、开源流计算框架、公司为支持自身业务开发的流计算框架</span><br><span class="line">• 较为常见的是开源流计算框架，代表如下：</span><br><span class="line">– Twitter Storm：免费、开源的分布式实时计算系统，可简单、高效、可靠地处理大量的流数据</span><br><span class="line">– Yahoo! S4（Simple Scalable Streaming System）：开源流计算平台，是通用的、分布式的、可扩展的、分区容错的、可插拔的流式系统</span><br></pre></td></tr></table></figure><h2 id="流计算处理流程"><a href="#流计算处理流程" class="headerlink" title="流计算处理流程"></a>流计算处理流程</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 传统的数据处理流程，需要先采集数据并存储在关系数据库等数据管理系统中，之后由用户通过查询操作和数据管理系统进行交互</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081522466.png" alt="image-20230308152245421"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 传统的数据处理流程隐含了两个前提：</span><br><span class="line">– 存储的数据是旧的。存储的静态数据是过去某一时刻的快照，这些数据在查询时可能已不具备时效性了</span><br><span class="line">– 需要用户主动发出查询来获取结果</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 流计算的处理流程一般包含三个阶段：数据实时采集、数据实时计算、实时查询服务</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081524733.png" alt="image-20230308152430685"></p><h3 id="数据实时采集"><a href="#数据实时采集" class="headerlink" title="数据实时采集"></a>数据实时采集</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">• 数据实时采集阶段通常采集多个数据源的海量数据，需要保证实时性、低延迟与稳定可靠</span><br><span class="line">• 以日志数据为例，由于分布式集群的广泛应用，数据分散存储在不同的机器上，因此需要实时汇总来自不同机器上的日志数据</span><br><span class="line">• 目前有许多互联网公司发布的开源分布式日志采集系统均可满足每秒数百MB的数据采集和传输需求，如：</span><br><span class="line">– Facebook的Scribe</span><br><span class="line">– LinkedIn的Kafka</span><br><span class="line">– 淘宝的Time Tunnel</span><br><span class="line">– 基于Hadoop的Chukwa和Flume</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">• 数据采集系统的基本架构一般有以下三个部分：</span><br><span class="line">– Agent：主动采集数据，并把数据推送到Collector部分</span><br><span class="line">– Collector：接收多个Agent的数据，并实现有序、可靠、高性能</span><br><span class="line">的转发</span><br><span class="line">– Store：存储Collector转发过来的数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081541896.png" alt="image-20230308154137847"></p><h3 id="数据实时计算"><a href="#数据实时计算" class="headerlink" title="数据实时计算"></a>数据实时计算</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">• 数据实时计算阶段对采集的数据进行实时的分析和计算，并反馈实时结果</span><br><span class="line">• 经流处理系统处理后的数据，可视情况进行存储，以便之后再进行分析计算。在时效性要求较高的场景中，处理之后的数据也可以直接丢弃</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081549997.png" alt="image-20230308154930940" style="zoom:80%;"><h3 id="实时查询服务"><a href="#实时查询服务" class="headerlink" title="实时查询服务"></a>实时查询服务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 实时查询服务：经由流计算框架得出的结果可供用户进行实时查询、展示或储存</span><br><span class="line">• 传统的数据处理流程，用户需要主动发出查询才能获得想要的结果。而在流处理流程中，实时查询服务可以不断更新结果，并将用户所需的结果实时推送给用户</span><br><span class="line">• 虽然通过对传统的数据处理系统进行定时查询，也可以实现不断地更新结果和结果推送，但通过这样的方式获取的结果，仍然是根据过去某一时刻的数据得到的结果，与实时结果有着本质的区别</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 实时查询服务：经由流计算框架得出的结果可供用户进行实时查询、展示或储存</span><br><span class="line">• 传统的数据处理流程，用户需要主动发出查询才能获得想要的结果。而在流处理流程中，实时查询服务可以不断更新结果，并将用户所需的结果实时推送给用户</span><br><span class="line">• 虽然通过对传统的数据处理系统进行定时查询，也可以实现不断地更新结果和结果推送，但通过这样的方式获取的结果，仍然是根据过去某一时刻的数据得到的结果，与实时结果有着本质的区别</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">• 可见，流处理系统与传统的数据处理系统有如下不同：</span><br><span class="line">– 流处理系统处理的是实时的数据，而传统的数据处理系统处理的是预先存储好的静态数据</span><br><span class="line">– 用户通过流处理系统获取的是实时结果，而通过传统的数据处理系统，获取的是过去某一时刻的结果</span><br><span class="line">– 流处理系统无需用户主动发出查询，实时查询服务可以主动将实时结果推送给用户</span><br></pre></td></tr></table></figure><h2 id="流计算的应用"><a href="#流计算的应用" class="headerlink" title="流计算的应用"></a>流计算的应用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 流计算是针对流数据的实时计算，可以应用在多种场景中，如Web服务、机器翻译、广告投放、自然语言处理、气候模拟预测等</span><br><span class="line">• 如百度、淘宝等大型网站中，每天都会产生大量流数据，包括用户的搜索内容、用户的浏览记录等数据。采用流计算进行实时数据分析，可以了解每个时刻的流量变化情况，甚至可以分析用户的实时浏览轨迹，从而进行实时个性化内容推荐</span><br><span class="line">• 但是，并不是每个应用场景都需要用到流计算的。流计算适合于需要处理持续到达的流数据、对数据处理有较高实时性要求的场景</span><br></pre></td></tr></table></figure><h3 id="应用场景1-实时分析"><a href="#应用场景1-实时分析" class="headerlink" title="应用场景1: 实时分析"></a>应用场景1: 实时分析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 传统的业务分析一般采用分布式离线计算的方式，即将数据全部保存起来，然后每隔一定的时间进行离线分析来得到结果。但这样会导致一定的延时，难以保证结果的实时性</span><br><span class="line">• 如淘宝网“双十一”、“双十二”的促销活动，商家需要根据广告效果来即使调整广告，这就需要对广告的受访情况进行分析。但以往采用分布式离线分析，需要几小时甚至一天的延时才能得到分析结果。而促销活动只持续一天，因此，隔天才能得到的分析结果便失去了价值</span><br><span class="line">• 虽然分布式离线分析带来的小时级的分析延时可以满足大部分商家的需求，但随着实时性要求越来越高，如何实现秒级别的实时分析响应成为业务分析的一大挑战</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 针对流数据，“量子恒道”开发了海量数据实时流计算框架Super Mario。通过该框架，量子恒道可处理每天TB级的实时流数据，并且从用户发出请求到数据展示，整个延时控制在2-3秒内，达到了实时性的要求</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081604482.png" alt="image-20230308160445437" style="zoom:67%;"><h3 id="应用场景2-实时交通"><a href="#应用场景2-实时交通" class="headerlink" title="应用场景2: 实时交通"></a>应用场景2: 实时交通</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 流计算不仅为互联网带来改变，也能改变我们的生活</span><br><span class="line">• 如提供导航路线，一般的导航路线并没有考虑实时的交通状况，即便在计算路线时有考虑交通状况，往往也只是使用了以往的交通状况数据。要达到根据实时交通状态进行导航的效果，就需要获取海量的实时交通数据并进行实时分析</span><br><span class="line">• 借助于流计算的实时特性，不仅可以根据交通情况制定路线，而且在行驶过程中，也可以根据交通情况的变化实时更新路线，始终为用户提供最佳的行驶路线</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">• IBM的流计算平台InfoSphere Streams，广泛应用于制造、零售、交通运输、金融证券以及监管各行各业的解决方案之中，使得实时快速做出决策的理念得以实现</span><br><span class="line">• 以上述的实时交通为例，InfoSphere Streams应用于斯德哥尔摩的交通信息管理，通过结合来自不同源的实时数据，可以生成动态的、多方位的观察交通流量的方式，为城市规划者和乘客提供实时交通状况查询</span><br></pre></td></tr></table></figure><h2 id="流计算开源框架-–-Storm"><a href="#流计算开源框架-–-Storm" class="headerlink" title="流计算开源框架 – Storm"></a>流计算开源框架 – Storm</h2><h3 id="Storm简介"><a href="#Storm简介" class="headerlink" title="Storm简介"></a>Storm简介</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• Twitter Storm是一个免费、开源的分布式实时计算系统，Storm对于实时计算的意义类似于Hadoop对于批处理的意义，Storm可以简单、高效、可靠地处理流数据，并支持多种编程语言</span><br><span class="line">• Storm框架可以方便地与数据库系统进行整合，从而开发出强大的实时计算系统</span><br><span class="line">• Twitter是全球访问量最大的社交网站之一，Twitter开发Storm流处理框架也是为了应对其不断增长的流数据实时处理需求</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">• Twitter采用了由实时系统和批处理系统组成的分层数据处理架构，一方面由Hadoop和ElephantDB组成批处理系统，另一方面由Storm和Cassandra组成实时系统</span><br><span class="line">• 在计算查询时，该系统会同时查询批处理视图和实时视图，并把它们合并起来以得到最终的结果</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081613772.png" alt="image-20230308161306725" style="zoom:67%;"><h3 id="Storm的特点"><a href="#Storm的特点" class="headerlink" title="Storm的特点"></a>Storm的特点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">• Storm可用于许多领域中，如实时分析、在线机器学习、持续计算、远程RPC、数据提取加载转换等</span><br><span class="line">• Storm具有以下主要特点：</span><br><span class="line">– 整合性：Storm可方便地与队列系统和数据库系统进行整合</span><br><span class="line">– 简易的API：Storm的API在使用上即简单又方便</span><br><span class="line">– 可扩展性：Storm的并行特性使其可以运行在分布式集群中</span><br><span class="line">– 容错性：Storm可自动进行故障节点的重启、任务的重新分配</span><br><span class="line">– 可靠的消息处理：Storm保证每个消息都能完整处理</span><br><span class="line">– 支持各种编程语言：Storm支持使用各种编程语言来定义任务</span><br><span class="line">– 快速部署：Storm可以快速进行部署和使用</span><br><span class="line">– 免费、开源：Storm是一款开源框架，可以免费使用</span><br></pre></td></tr></table></figure><h3 id="Storm设计思想"><a href="#Storm设计思想" class="headerlink" title="Storm设计思想"></a>Storm设计思想</h3><h4 id="Streams"><a href="#Streams" class="headerlink" title="Streams"></a>Streams</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">• 要了解Storm，首先需要了解Storm的设计思想。Storm对一些设计思想进行了抽象化，其主要术语包括Streams、Spouts、Bolts、Topology和Stream Groupings</span><br><span class="line">• Streams：Storm将流数据Stream描述成一个无限的Tuple序列，这些Tuple序列会以分布式的方式并行地创建和处理</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081637226.png" alt="image-20230308163744184" style="zoom:67%;"><h4 id="Spouts"><a href="#Spouts" class="headerlink" title="Spouts"></a>Spouts</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• Spouts：Storm认为每个Stream都有一个源头，并把这个源头抽象为Spouts。Spouts会从外部读取流数据并持续发出Tuple</span><br></pre></td></tr></table></figure><h4 id="Bolts"><a href="#Bolts" class="headerlink" title="Bolts"></a>Bolts</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• Bolts：Storm将Streams的状态转换过程抽象为Bolts。Bolts即可以处理Tuple，也可以将处理后的Tuple作为新的Streams发送给其他Bolts。对Tuple的处理逻辑都被封装在Bolts中，可执行过滤、聚合、查询等操作</span><br></pre></td></tr></table></figure><h4 id="Topology"><a href="#Topology" class="headerlink" title="Topology"></a>Topology</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• Topology：Storm将Spouts和Bolts组成的网络抽象成Topology，它可以被提交到Storm集群执行。Topology可视为流转换图，图中节点是一个Spout或Bolt，边则表示Bolt订阅了哪个Stream。当Spout或者Bolt发送元组时，它会把元组发送到每个订阅了该Stream的Bolt上进行处理</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081643689.png" alt="image-20230308164303642" style="zoom:67%;"><h4 id="Stream-Groupings"><a href="#Stream-Groupings" class="headerlink" title="Stream Groupings"></a>Stream Groupings</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• Stream Groupings：Storm中的Stream Groupings用于告知Topology如何在两个组件间（如Spout和Bolt之间，或者不同的Bolt之间）进行Tuple的传送。每一个Spout和Bolt都可以有多个分布式任务，一个任务在什么时候、以什么方式发送Tuple就是由Stream Groupings来决定的</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081645742.png" alt="image-20230308164524685"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">• 目前，Storm中的Stream Groupings有如下几种方式：</span><br><span class="line">– Shuffle Grouping：随机分组，随机分发Tuple</span><br><span class="line">– Fields Grouping：按字段分组，具有相同值的Tuple会被分发到对应的Bolt</span><br><span class="line">– All Grouping：广播分发，每个Tuple都会被分发到所有Bolt中</span><br><span class="line">– Global Grouping：全局分组，Tuple只会分发给一个Bolt</span><br><span class="line">– Non Grouping：不分组，与随机分组效果类似</span><br><span class="line">– Direct Grouping：直接分组，由Tuple的生产者来定义接收者</span><br></pre></td></tr></table></figure><h3 id="Storm框架设计"><a href="#Storm框架设计" class="headerlink" title="Storm框架设计"></a>Storm框架设计</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">• Storm运行任务的方式与Hadoop类似：Hadoop运行的是MapReduce作业，而Storm运行的是“Topology”</span><br><span class="line">• 但两者的任务大不相同，主要的不同是：MapReduce作业最终会完成计算并结束运行，而Topology将持续处理消息（直到人为终止）</span><br><span class="line">• Storm集群采用“Master—Worker”的节点方式：</span><br><span class="line">– Master节点运行名为“Nimbus”的后台程序（类似Hadoop中的“JobTracker”），负责在集群范围内分发代码、为Worker分配任务和监测故障</span><br><span class="line">– Worker节点运行名为“Supervisor”的后台程序，负责监听分配给它所在机器的工作，即根据Nimbus分配的任务来决定启动或停止Worker进程</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• Storm使用Zookeeper来作为分布式协调组件，负责Nimbus和多个Supervisor之间的所有协调工作。借助于Zookeeper，若Nimbus进程或Supervisor进程意外终止，重启时也能读取、恢复之前的状态并继续工作，使得Storm极其稳定</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081656820.png" alt="image-20230308165644767" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 基于这样的架构设计，Storm的工作流程如下图所示：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081657604.png" alt="image-20230308165745550"></p><h3 id="Storm实例"><a href="#Storm实例" class="headerlink" title="Storm实例"></a>Storm实例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 我们以单词统计的实例来加深对Storm的认识</span><br><span class="line">• Storm的编程模型非常简单，如下代码即定义了整个单词统计</span><br><span class="line">Topology的整体逻辑</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081658383.png" alt="image-20230308165849326"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• Topology中仅定义了整体的计算逻辑，还需要定义具体的处理函数</span><br><span class="line">。具体的处理函数可以使用任一编程语言来定义，甚至也可以结合多种编程语言来实现</span><br><span class="line">• 如SplitSentence()方法虽然是通过Java语言定义的，但具体的操作可通过Python脚本来完成</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081700391.png" alt="image-20230308170042334"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• Python脚本splitsentence.py定义了一个简单的单词分割方法，即通过空格来分割单词。分割后的单词通过emit()方法以Tuple的形式发送给订阅了该Stream的Bolt进行接收和处理</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081701390.png" alt="image-20230308170113339"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 单词统计的具体逻辑：首先判断单词是否统计过，若未统计过，需先将count值置为0。若单词已统计过，则每出现一次该单词，count值就加1</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081701542.png" alt="image-20230308170135478"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">• 基于Storm的单词统计在形式上与基于MapReduce的单词统计是类似的，MapReduce使用的是Map和Reduce的抽象，而Storm使用的是Soput和Bolt的抽象</span><br><span class="line">• 总结一下Storm进行单词统计的整个流程：</span><br><span class="line">– 从Spout中发送Stream（每个英文句子为一个Tuple）</span><br><span class="line">– 用于分割单词的Bolt将接收的句子分解为独立的单词，将单词作为Tuple的字段名发送出去</span><br><span class="line">– 用于计数的Bolt接收表示单词的Tuple，并对其进行统计</span><br><span class="line">– 输出每个单词以及单词出现过的次数</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081702343.png" alt="image-20230308170211283" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 上述虽然是一个简单的单词统计，但对其进行扩展，便可应用到许多场景中，如微博中的实时热门话题。Twitter也正是使用了Storm框架实现了实时热门话题</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081702971.png" alt="image-20230308170245909"></p><h3 id="哪些公司在使用Storm"><a href="#哪些公司在使用Storm" class="headerlink" title="哪些公司在使用Storm"></a>哪些公司在使用Storm</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• Storm自2011年发布以来，凭借其优良的框架设计及开源特性，在流计算领域获得了广泛认可，已应用到许多大型互联网公司的实际项目中</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081710751.png" alt="image-20230308171025682"></p><h2 id="Storm安装和运行实例"><a href="#Storm安装和运行实例" class="headerlink" title="Storm安装和运行实例"></a>Storm安装和运行实例</h2><p><a href="http://dblab.xmu.edu.cn/blog/install-storm/" target="_blank" rel="external nofollow noopener noreferrer">Storm安装教程_CentOS6.4/Storm0.9.6</a></p><h3 id="安装Storm的基本过程"><a href="#安装Storm的基本过程" class="headerlink" title="安装Storm的基本过程"></a>安装Storm的基本过程</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">本实例中Storm具体运行环境如下：</span><br><span class="line">•CentOS 6.4</span><br><span class="line">•Storm 0.9.6</span><br><span class="line">•Java JDK 1.7</span><br><span class="line">•ZooKeeper 3.4.6</span><br><span class="line">•Python 2.6</span><br><span class="line">备注：CentOS中已默认安装了Python 2.6，我们还需要安装 JDK 环境以及分布式应用程序协调服务 Zookeeper</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">安装Storm的基本过程如下：</span><br><span class="line">•第一步：安装Java环境</span><br><span class="line">•第二步：安装 Zookeeper</span><br><span class="line">•第三步：安装Storm（单机）</span><br><span class="line">•第四步：关闭Storm</span><br></pre></td></tr></table></figure><h4 id="第一步：安装Java环境"><a href="#第一步：安装Java环境" class="headerlink" title="第一步：安装Java环境"></a>第一步：安装Java环境</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">•Storm 运行需要 Java 环境，可选择 Oracle 的 JDK，或是 OpenJDK</span><br><span class="line">，现在一般 Linux 系统默认安装的基本是 OpenJDK，如 CentOS 6.4 </span><br><span class="line">就默认安装了 OpenJDK 1.7。但需要注意的是，CentOS 6.4 中默认安</span><br><span class="line">装的只是 Java JRE，而不是 JDK，为了开发方便，我们还是需要通过</span><br><span class="line">yum 进行安装 JDK</span><br><span class="line"></span><br><span class="line">$ sudo yum install java-1.7.0-openjdk java-1.7.0-openjdk-devel</span><br><span class="line"></span><br><span class="line">•接着需要配置一下 JAVA_HOME 环境变量，为方便，可以在 ~&#x2F;.bashrc</span><br><span class="line">中进行设置</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081716201.png" alt="image-20230308171605152"></p><h4 id="第二步：安装Zookeeper"><a href="#第二步：安装Zookeeper" class="headerlink" title="第二步：安装Zookeeper"></a>第二步：安装Zookeeper</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">到官网下载Zookeeper，比如下载 ―zookeeper-3.4.6.tar.gz‖</span><br><span class="line">下载后执行如下命令进行安装 zookeeper（将命令中 3.4.6 改为你下载的版本）：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081716354.png" alt="image-20230308171657313"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chown命令让hadoop用户拥有zookeeper目录下的所有文件的权限</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接着执行如下命令进行zookeeper配置：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081719215.png" alt="image-20230308171903171"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">将当中的 dataDir&#x3D;&#x2F;tmp&#x2F;zookeeper 更改为</span><br><span class="line">dataDir&#x3D;&#x2F;usr&#x2F;local&#x2F;zookeeper&#x2F;tmp 。接着执行：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081719069.png" alt="image-20230308171920028"></p><h4 id="第三步：安装Storm（单机）"><a href="#第三步：安装Storm（单机）" class="headerlink" title="第三步：安装Storm（单机）"></a>第三步：安装Storm（单机）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">到官网下载Storm，比如Storm0.9.6</span><br><span class="line">下载后执行如下命令进行安装Storm：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081720726.png" alt="image-20230308172015682"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接着执行如下命令进行Storm配置:</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081721931.png" alt="image-20230308172103894"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">修改其中的 storm.zookeeper.servers 和 nimbus.host 两个配置项，即取消掉</span><br><span class="line">注释且都修改值为 127.0.0.1（我们只需要在单机上运行），如下图所示。</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081721615.png" alt="image-20230308172131564" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">然后就可以启动 Storm 了。执行如下命令启动 nimbus 后台进程：</span><br><span class="line">$ .&#x2F;bin&#x2F;storm nimbus</span><br><span class="line"></span><br><span class="line">启动 nimbus 后，终端被该进程占用了，不能再继续执行其他命令了。因此</span><br><span class="line">我们需要另外开启一个终端，然后执行如下命令启动 supervisor 后台进程：</span><br><span class="line">$ # 需要另外开启一个终端</span><br><span class="line">$ &#x2F;usr&#x2F;local&#x2F;storm&#x2F;bin&#x2F;storm supervisor</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">同样的，启动 supervisor 后，我们还需要开启另外的终端才能执行其他命令</span><br><span class="line">。另外，我们可以使用 jps 命令 检查是否成功启动，若成功启动会显示</span><br><span class="line">nimbus、supervisor、QuorumPeeMain （QuorumPeeMain 是 zookeeper </span><br><span class="line">的后台进程，若显示 config_value 表明 nimbus 或 supervisor 还在启动中）</span><br><span class="line">，如下图所示。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081723654.png" alt="image-20230308172323611"></p><h4 id="第四步：关闭Storm"><a href="#第四步：关闭Storm" class="headerlink" title="第四步：关闭Storm"></a>第四步：关闭Storm</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">之前启动的 nimbus 和 supervisor 占用了两个终端窗口，切换到这两个终</span><br><span class="line">端窗口，按键盘的 Ctrl+C 可以终止进程，终止后，也就相当于关闭了Storm。</span><br></pre></td></tr></table></figure><h3 id="运行Storm实例"><a href="#运行Storm实例" class="headerlink" title="运行Storm实例"></a>运行Storm实例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Storm中自带了一些例子，我们可以执行一下 WordCount 例子来感受一</span><br><span class="line">下 Storm 的执行流程。执行如下命令：</span><br><span class="line">$ &#x2F;usr&#x2F;local&#x2F;storm&#x2F;bin&#x2F;storm jar &#x2F;usr&#x2F;local&#x2F;storm&#x2F;examples&#x2F;stormstarter&#x2F;storm-starter-topologies-0.9.6.jar </span><br><span class="line">storm.starter.WordCountTopology</span><br><span class="line"></span><br><span class="line">该程序是不断地取如下四句英文句子中的一句作为数据源，然后发送给</span><br><span class="line">bolt 来统计单词出现的次数。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081724098.png" alt="image-20230308172457055"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第7章 mapreduce</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC7%E7%AB%A0-mapreduce.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC7%E7%AB%A0-mapreduce.html</id>
    <published>2023-02-28T14:31:52.000Z</published>
    <updated>2023-03-14T02:01:17.369Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第7章-mapreduce"><a href="#第7章-mapreduce" class="headerlink" title="第7章 mapreduce"></a>第7章 mapreduce</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="分布式并行编程"><a href="#分布式并行编程" class="headerlink" title="分布式并行编程"></a>分布式并行编程</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•“摩尔定律”， CPU性能大约每隔18个月翻一番</span><br><span class="line">•从2005年开始摩尔定律逐渐失效(大数据摩尔定律：每年按50%增长) ，需要处理的数据量快速增加，人们开始借助于分布式并行编程来提高程序性能</span><br><span class="line">•分布式程序运行在大规模计算机集群上，可以并行执行大规模数据处理任务，从而获得海量的计算能力</span><br><span class="line">•谷歌公司最先提出了分布式并行编程模型MapReduce，Hadoop MapReduce是它的开源实现，后者比前者使用门槛低很多</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">问题：在MapReduce出现之前，已经有像MPI这样非常成熟的并行计算框架了，那么为什么Google还需要MapReduce？MapReduce相较于传统的并行计算框架有什么优势？</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031429645.png" alt="image-20230303142923582" style="zoom:67%;"><h3 id="MapReduce模型简介"><a href="#MapReduce模型简介" class="headerlink" title="MapReduce模型简介"></a>MapReduce模型简介</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">•MapReduce将复杂的、运行于大规模集群上的并行计算过程高度地抽象到了两个函数：Map和Reduce</span><br><span class="line">•编程容易，不需要掌握分布式并行编程细节，也可以很容易把自己的程序运行在分布式系统上，完成海量数据的计算</span><br><span class="line">•MapReduce采用“分而治之”策略，一个存储在分布式文件系统中的大规模数据集，会被切分成许多独立的分片（split），这些分片可以被多个Map任务并行处理</span><br><span class="line">•MapReduce设计的一个理念就是“计算向数据靠拢”，而不是“数据向计算靠拢”，因为，移动数据需要大量的网络传输开销</span><br><span class="line">•MapReduce框架采用了Master&#x2F;Slave架构，包括一个Master和若干个Slave。Master上运行JobTracker，Slave上运行TaskTracker</span><br><span class="line">•Hadoop框架是用Java实现的，但是，MapReduce应用程序则不一定要用Java来写</span><br></pre></td></tr></table></figure><h3 id="Map和Reduce函数"><a href="#Map和Reduce函数" class="headerlink" title="Map和Reduce函数"></a>Map和Reduce函数</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031447655.png" alt="image-20230303144723603"></p><h2 id="MapReduce体系结构"><a href="#MapReduce体系结构" class="headerlink" title="MapReduce体系结构"></a>MapReduce体系结构</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MapReduce体系结构主要由四个部分组成，分别是：Client、JobTracker、TaskTracker以及Task</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031450101.png" alt="image-20230303145010046" style="zoom:67%;"><h3 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">•用户编写的MapReduce程序通过Client提交到JobTracker端</span><br><span class="line">•用户可通过Client提供的一些接口查看作业运行状态</span><br></pre></td></tr></table></figure><h3 id="JobTracker"><a href="#JobTracker" class="headerlink" title="JobTracker"></a>JobTracker</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">•JobTracker负责资源监控和作业调度</span><br><span class="line">•JobTracker监控所有TaskTracker与Job的健康状况，一旦发现失败，就将相应的任务转移到其他节点</span><br><span class="line">•JobTracker会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器（Task Scheduler），而调度器会在资源出现空闲时，选择合适的任务去使用这些资源</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031503481.png" alt="image-20230303150311444" style="zoom:67%;"><h3 id="TaskTracker"><a href="#TaskTracker" class="headerlink" title="TaskTracker"></a>TaskTracker</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">•TaskTracker 会周期性地通过“心跳”将本节点上资源的使用情况和任务的运行进度汇报给JobTracker，同时接收JobTracker 发送过来的命令并执行相应的操作（如启动新任务、杀死任务等）</span><br><span class="line">•TaskTracker使用“slot”等量划分本节点上的资源量（CPU、内存等）。一个Task 获取到一个slot 后才有机会运行，而Hadoop调度器的作用就是将各个TaskTracker上的空闲slot分配给Task使用。slot分为Map slot 和Reduce slot两种，分别供MapTask和Reduce Task使用</span><br></pre></td></tr></table></figure><h3 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Task分为Map Task 和Reduce Task 两种，均由TaskTracker 启动</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031503074.png" alt="image-20230303150354031" style="zoom:67%;"><h2 id="MapReduce工作流程"><a href="#MapReduce工作流程" class="headerlink" title="MapReduce工作流程"></a>MapReduce工作流程</h2><h3 id="工作流程概述"><a href="#工作流程概述" class="headerlink" title="工作流程概述"></a>工作流程概述</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031531923.png" alt="image-20230303153107874"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•不同的Map任务之间不会进行通信</span><br><span class="line">•不同的Reduce任务之间也不会发生任何信息交换</span><br><span class="line">•用户不能显式地从一台机器向另一台机器发送消息</span><br><span class="line">•所有的数据交换都是通过MapReduce框架自身去实现的</span><br></pre></td></tr></table></figure><h3 id="MapReduce各个执行阶段"><a href="#MapReduce各个执行阶段" class="headerlink" title="MapReduce各个执行阶段"></a>MapReduce各个执行阶段</h3><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031531102.png" alt="image-20230303153150037" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">split：是逻辑分割，如起点，长度</span><br><span class="line">RR（Record Reader):按照split切分的起点和长度读取</span><br><span class="line">shuffle：分区、排序、合并、归并过程</span><br></pre></td></tr></table></figure><h4 id="关于Split（分片）"><a href="#关于Split（分片）" class="headerlink" title="关于Split（分片）"></a>关于Split（分片）</h4><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031532479.png" alt="image-20230303153252421" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HDFS以固定大小的block为基本单位存储数据，而对于MapReduce 而言，其处理单位是split。split 是一个逻辑概念，它只包含一些元数据信息，比如数据起始位置、数据长度、数据所在节点等。它的划分方法完全由用户自己决定。</span><br></pre></td></tr></table></figure><h4 id="Map任务的数量"><a href="#Map任务的数量" class="headerlink" title="Map任务的数量"></a>Map任务的数量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">•Hadoop为每个split创建一个Map任务，split 的多少决定了Map任务的数目。大多数情况下，理想的分片大小是一个HDFS块</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031533401.png" alt="image-20230303153342357"></p><h4 id="Reduce任务的数量"><a href="#Reduce任务的数量" class="headerlink" title="Reduce任务的数量"></a>Reduce任务的数量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">•最优的Reduce任务个数取决于集群中可用的reduce任务槽(slot)的数目</span><br><span class="line">•通常设置比reduce任务槽数目稍微小一些的Reduce任务个数（这样可以预留一些系统资源处理可能发生的错误）</span><br></pre></td></tr></table></figure><h3 id="Shuffle过程详解"><a href="#Shuffle过程详解" class="headerlink" title="Shuffle过程详解"></a>Shuffle过程详解</h3><h4 id="Shuffle过程简介"><a href="#Shuffle过程简介" class="headerlink" title="Shuffle过程简介"></a>Shuffle过程简介</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031619938.png" alt="image-20230303161924888"></p><h4 id="Map端的Shuffle过程"><a href="#Map端的Shuffle过程" class="headerlink" title="Map端的Shuffle过程"></a>Map端的Shuffle过程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">•每个Map任务分配一个缓存</span><br><span class="line">•MapReduce默认100MB缓存</span><br><span class="line">•设置溢写比例0.8</span><br><span class="line">•分区默认采用哈希函数</span><br><span class="line">•排序是默认的操作</span><br><span class="line">•排序后可以合并（Combine）(用户定义了就执行，不是必须的)</span><br><span class="line">•合并不能改变最终结果(注意事项) 如求和，最大值</span><br><span class="line">•在Map任务全部结束之前进行归并(对之前多次溢写生成的磁盘文件，合并成大文件，文件里的数据是分区，排序了的)</span><br><span class="line">•归并得到一个大的文件，放在本地磁盘</span><br><span class="line">•文件归并时，如果溢写文件数量大于预定值（默认是3）则可以再次启动Combiner，少于3不需要</span><br><span class="line">•JobTracker会一直监测Map任务的执行，并通知Reduce任务来领取数据</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031620847.png" alt="image-20230303162007805" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">合并（Combine）和归并（Merge）的区别：</span><br><span class="line">两个键值对&lt;“a”,1&gt;和&lt;“a”,1&gt;，如果合并，会得到&lt;“a”,2&gt;，如果归并，会得到&lt;“a”,&lt;1,1&gt;&gt;</span><br></pre></td></tr></table></figure><h4 id="Reduce端的Shuffle过程"><a href="#Reduce端的Shuffle过程" class="headerlink" title="Reduce端的Shuffle过程"></a>Reduce端的Shuffle过程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•Reduce任务通过RPC向JobTracker询问Map任务是否已经完成，若完成，则领取数据</span><br><span class="line">•Reduce领取数据先放入缓存，来自不同Map机器，先归并，再合并，写入磁盘</span><br><span class="line">•多个溢写文件归并成一个或多个大文件，文件中的键值对是排序的</span><br><span class="line">•当数据很少时，不需要溢写到磁盘，直接在缓存中归并，然后输出给Reduce</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031620059.png" alt="image-20230303162053011" style="zoom:67%;"><h3 id="MapReduce应用程序执行过程"><a href="#MapReduce应用程序执行过程" class="headerlink" title="MapReduce应用程序执行过程"></a>MapReduce应用程序执行过程</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031622836.png" alt="image-20230303162200768"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">中间的输出是输出到磁盘不是hdfs！！！</span><br></pre></td></tr></table></figure><h2 id="实例分析：WordCount"><a href="#实例分析：WordCount" class="headerlink" title="实例分析：WordCount"></a>实例分析：WordCount</h2><h3 id="WordCount程序任务"><a href="#WordCount程序任务" class="headerlink" title="WordCount程序任务"></a>WordCount程序任务</h3><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303032219049.png" alt="image-20230303221931969" style="zoom:67%;"><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303032219887.png" alt="image-20230303221943836" style="zoom:67%;"><h3 id="WordCount设计思路"><a href="#WordCount设计思路" class="headerlink" title="WordCount设计思路"></a>WordCount设计思路</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 首先，需要检查WordCount程序任务是否可以采用MapReduce来实现</span><br><span class="line">• 其次，确定MapReduce程序的设计思路</span><br><span class="line">• 最后，确定MapReduce程序的执行过程</span><br></pre></td></tr></table></figure><h3 id="一个WordCount执行过程的实例"><a href="#一个WordCount执行过程的实例" class="headerlink" title="一个WordCount执行过程的实例"></a>一个WordCount执行过程的实例</h3><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC7%E7%AB%A0-mapreduce.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230303222506161.png" alt="image-20230303222506161" style="zoom:67%;"><h4 id="用户没有定义Combiner时"><a href="#用户没有定义Combiner时" class="headerlink" title="用户没有定义Combiner时"></a>用户没有定义Combiner时</h4><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC7%E7%AB%A0-mapreduce.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230303222631369.png" alt="image-20230303222631369" style="zoom:67%;"><h4 id="用户有定义Combiner时"><a href="#用户有定义Combiner时" class="headerlink" title="用户有定义Combiner时"></a>用户有定义Combiner时</h4><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303032227607.png" alt="image-20230303222701557" style="zoom:67%;"><h2 id="MapReduce的具体应用"><a href="#MapReduce的具体应用" class="headerlink" title="MapReduce的具体应用"></a>MapReduce的具体应用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">MapReduce可以很好地应用于各种计算问题</span><br><span class="line">• 关系代数运算（选择、投影、并、交、差、连接）</span><br><span class="line">• 分组与聚合运算</span><br><span class="line">• 矩阵-向量乘法</span><br><span class="line">• 矩阵乘法</span><br></pre></td></tr></table></figure><h3 id="用MapReduce实现关系的自然连接"><a href="#用MapReduce实现关系的自然连接" class="headerlink" title="用MapReduce实现关系的自然连接"></a>用MapReduce实现关系的自然连接</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303032243239.png" alt="image-20230303224301191"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">• 假设有关系R(A，B)和S(B,C)，对二者进行自然连接操作</span><br><span class="line">• 使用Map过程，把来自R的每个元组&lt;a,b&gt;转换成一个键值对&lt;b, &lt;R,a&gt;&gt;，其中的键就是属性B的值。把关系R包含到值中，这样做使得我们可以在Reduce阶段，只把那些来自R的元组和来自S的元组进行匹配。类似地，使用Map过程，把来自S的每个元组&lt;b,c&gt;，转换成一个键值对&lt;b,&lt;S,c&gt;&gt;</span><br><span class="line">• 所有具有相同B值的元组被发送到同一个Reduce进程中，Reduce进程的任务是，把来自关系R和S的、具有相同属性B值的元组进行合并</span><br><span class="line">• Reduce进程的输出则是连接后的元组&lt;a,b,c&gt;，输出被写到一个单独的输出文件中</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303032243094.png" alt="image-20230303224345042" style="zoom:67%;"><h2 id="MapReduce编程实践"><a href="#MapReduce编程实践" class="headerlink" title="MapReduce编程实践"></a>MapReduce编程实践</h2><h3 id="任务要求"><a href="#任务要求" class="headerlink" title="任务要求"></a>任务要求</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">文件A的内容如下：</span><br><span class="line">China is my motherland</span><br><span class="line">I love China</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">文件B的内容如下：</span><br><span class="line">I am from China</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">期望结果如右侧所示：</span><br><span class="line">I 2</span><br><span class="line">is 1</span><br><span class="line">China 3</span><br><span class="line">my 1</span><br><span class="line">love 1</span><br><span class="line">am 1</span><br><span class="line">from 1</span><br><span class="line">motherland 1</span><br></pre></td></tr></table></figure><h3 id="编写Map处理逻辑"><a href="#编写Map处理逻辑" class="headerlink" title="编写Map处理逻辑"></a>编写Map处理逻辑</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•Map输入类型为&lt;key,value&gt;</span><br><span class="line">•期望的Map输出类型为&lt;单词，出现次数&gt;</span><br><span class="line">•Map输入类型最终确定为&lt;Object,Text&gt;</span><br><span class="line">•Map输出类型最终确定为&lt;Text,IntWritable&gt;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Object</span>,<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> IntWritable one = <span class="keyword">new</span> IntWritable(<span class="number">1</span>); </span><br><span class="line">    <span class="keyword">private</span> Text word = <span class="keyword">new</span> Text(); </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> </span></span><br><span class="line"><span class="function">IOException,InterruptedException</span>&#123; </span><br><span class="line">    StringTokenizer itr = <span class="keyword">new</span> StringTokenizer(value.toString()); </span><br><span class="line">    <span class="keyword">while</span> (itr.hasMoreTokens())</span><br><span class="line">    &#123; </span><br><span class="line">        word.set(itr.nextToken()); </span><br><span class="line">        context.write(word,one); </span><br><span class="line">    &#125; </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="编写Reduce处理逻辑"><a href="#编写Reduce处理逻辑" class="headerlink" title="编写Reduce处理逻辑"></a>编写Reduce处理逻辑</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">•在Reduce处理数据之前，Map的结果首先通过Shuffle阶段进行整理</span><br><span class="line">•Reduce阶段的任务：对输入数字序列进行求和</span><br><span class="line">•Reduce的输入数据为&lt;key,Iterable容器&gt;</span><br><span class="line">Reduce任务的输入数据：</span><br><span class="line">&lt;”I”,&lt;1,1&gt;&gt;</span><br><span class="line">&lt;”is”,1&gt;</span><br><span class="line">……</span><br><span class="line">&lt;”from”,1&gt;</span><br><span class="line">&lt;”China”,&lt;1,1,1&gt;&gt;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span> <span class="keyword">extends</span> </span></span><br><span class="line"><span class="class"><span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">IntWritable</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt;</span>&#123; </span><br><span class="line">    <span class="keyword">private</span> IntWritable result = <span class="keyword">new</span> IntWritable(); </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; </span></span></span><br><span class="line"><span class="function"><span class="params">values, Context context)</span> <span class="keyword">throws</span> IOException,InterruptedException</span>&#123; </span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>; </span><br><span class="line">        <span class="keyword">for</span> (IntWritable val : values) </span><br><span class="line">        &#123;</span><br><span class="line">        sum += val.get(); </span><br><span class="line">        &#125;</span><br><span class="line">        result.set(sum); </span><br><span class="line">        context.write(key,result); </span><br><span class="line">&#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="编写main方法"><a href="#编写main方法" class="headerlink" title="编写main方法"></a>编写main方法</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration(); <span class="comment">//程序运行时参数</span></span><br><span class="line">String[] otherArgs = <span class="keyword">new</span> GenericOptionsParser(conf,args).getRemainingArgs();</span><br><span class="line"><span class="keyword">if</span> (otherArgs.length != <span class="number">2</span>)</span><br><span class="line">&#123; System.err.println(<span class="string">"Usage: wordcount &lt;in&gt; &lt;out&gt;"</span>);</span><br><span class="line">System.exit(<span class="number">2</span>);</span><br><span class="line">&#125;</span><br><span class="line">Job job = <span class="keyword">new</span> Job(conf,<span class="string">"word count"</span>); <span class="comment">//设置环境参数</span></span><br><span class="line">job.setJarByClass(WordCount<span class="class">.<span class="keyword">class</span>)</span>; <span class="comment">//设置整个程序的类名</span></span><br><span class="line">job.setMapperClass(MyMapper<span class="class">.<span class="keyword">class</span>)</span>; <span class="comment">//添加MyMapper类</span></span><br><span class="line">job.setReducerClass(MyReducer<span class="class">.<span class="keyword">class</span>)</span>; <span class="comment">//添加MyReducer类</span></span><br><span class="line">job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>; <span class="comment">//设置输出类型</span></span><br><span class="line">job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>; <span class="comment">//设置输出类型</span></span><br><span class="line">FileInputFormat.addInputPath(job,<span class="keyword">new</span> Path(otherArgs[<span class="number">0</span>])); <span class="comment">//设置输入文件</span></span><br><span class="line">FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(otherArgs[<span class="number">1</span>])); <span class="comment">//设置输出文件</span></span><br><span class="line">System.exit(job.waitForCompletion(<span class="keyword">true</span>)?<span class="number">0</span>:<span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException; </span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.GenericOptionsParser;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span></span>&#123; </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Object</span>,<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt;</span>&#123; </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> IntWritable one = <span class="keyword">new</span> IntWritable(<span class="number">1</span>); </span><br><span class="line">    <span class="keyword">private</span> Text word = <span class="keyword">new</span> Text(); </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException,InterruptedException</span>&#123; </span><br><span class="line">    StringTokenizer itr = <span class="keyword">new</span> StringTokenizer(value.toString()); </span><br><span class="line">    <span class="keyword">while</span> (itr.hasMoreTokens())&#123; </span><br><span class="line">word.set(itr.nextToken()); </span><br><span class="line">context.write(word,one); </span><br><span class="line">&#125; </span><br><span class="line">&#125; </span><br><span class="line">&#125; </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">IntWritable</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt;</span>&#123; </span><br><span class="line"><span class="keyword">private</span> IntWritable result = <span class="keyword">new</span> IntWritable(); </span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException,InterruptedException</span>&#123; </span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>; </span><br><span class="line">        <span class="keyword">for</span> (IntWritable val : values) </span><br><span class="line">        &#123; </span><br><span class="line">        sum += val.get(); </span><br><span class="line">        &#125; </span><br><span class="line">        result.set(sum); </span><br><span class="line">        context.write(key,result); </span><br><span class="line">        &#125; </span><br><span class="line">&#125; </span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123; </span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration(); </span><br><span class="line">String[] otherArgs = <span class="keyword">new</span> GenericOptionsParser(conf,args).getRemainingArgs(); </span><br><span class="line"><span class="keyword">if</span> (otherArgs.length != <span class="number">2</span>) </span><br><span class="line">&#123; </span><br><span class="line">    System.err.println(<span class="string">"Usage: wordcount &lt;in&gt; &lt;out&gt;"</span>); </span><br><span class="line">    System.exit(<span class="number">2</span>); </span><br><span class="line">&#125; </span><br><span class="line">    Job job = <span class="keyword">new</span> Job(conf,<span class="string">"word count"</span>); </span><br><span class="line">    job.setJarByClass(WordCount<span class="class">.<span class="keyword">class</span>)</span>; </span><br><span class="line">    job.setMapperClass(MyMapper<span class="class">.<span class="keyword">class</span>)</span>; </span><br><span class="line">    job.setReducerClass(MyReducer<span class="class">.<span class="keyword">class</span>)</span>; </span><br><span class="line">    job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>; </span><br><span class="line">    job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>; </span><br><span class="line">    FileInputFormat.addInputPath(job,<span class="keyword">new</span> Path(otherArgs[<span class="number">0</span>])); </span><br><span class="line">    FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(otherArgs[<span class="number">1</span>])); </span><br><span class="line">System.exit(job.waitForCompletion(<span class="keyword">true</span>)?<span class="number">0</span>:<span class="number">1</span>); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="编译打包代码以及运行程序"><a href="#编译打包代码以及运行程序" class="headerlink" title="编译打包代码以及运行程序"></a>编译打包代码以及运行程序</h3><ul><li><a href="https://dblab.xmu.edu.cn/blog/hadoop-build-project-by-shell/" target="_blank" rel="external nofollow noopener noreferrer">使用命令行编译打包运行自己的MapReduce程序</a></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">实验步骤：</span><br><span class="line">•使用java编译程序，生成.class文件</span><br><span class="line">•将.class文件打包为jar包</span><br><span class="line">•运行jar包（需要启动Hadoop）</span><br><span class="line">•查看结果</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Hadoop 2.x 版本中的依赖 jar</span><br><span class="line">Hadoop 2.x 版本中 jar 不再集中在一个 hadoop-core*.jar 中，而是分成多个 jar，如使用 Hadoop 2.6.0 运行 WordCount 实例至少需要如下三个</span><br><span class="line">jar:</span><br><span class="line">•$HADOOP_HOME&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;hadoop-common-2.6.0.jar</span><br><span class="line">•$HADOOP_HOME&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduceclient-core-2.6.0.jar</span><br><span class="line">•$HADOOP_HOME&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;lib&#x2F;commons-cli-1.2.jar</span><br><span class="line"></span><br><span class="line">通过命令 hadoop classpath 可以得到运行 Hadoop 程序所需的全部classpath信息</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">将 Hadoop 的 classhpath 信息添加到 CLASSPATH 变量中，在 ~&#x2F;.bashrc </span><br><span class="line">中增加如下几行：</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;hadoop export </span><br><span class="line">CLASSPATH&#x3D;$($HADOOP_HOME&#x2F;bin&#x2F;hadoop classpath):$CLASSPATH</span><br><span class="line">执行 source ~&#x2F;.bashrc 使变量生效，接着就可以通过 javac 命令编译</span><br><span class="line">WordCount.java</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303040040026.png" alt="image-20230304004002960" style="zoom:67%;"><h4 id="如何使用Eclipse编译运行MapReduce程序？"><a href="#如何使用Eclipse编译运行MapReduce程序？" class="headerlink" title="如何使用Eclipse编译运行MapReduce程序？"></a>如何使用Eclipse编译运行MapReduce程序？</h4><ul><li><a href="https://dblab.xmu.edu.cn/blog/hadoop-build-project-using-eclipse/" target="_blank" rel="external nofollow noopener noreferrer">使用Eclipse编译运行MapReduce程序</a></li></ul><h3 id="Hadoop中执行MapReduce任务的几种方式"><a href="#Hadoop中执行MapReduce任务的几种方式" class="headerlink" title="Hadoop中执行MapReduce任务的几种方式"></a>Hadoop中执行MapReduce任务的几种方式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">•Hadoop jar</span><br><span class="line">•Pig</span><br><span class="line">•Hive</span><br><span class="line">•Python</span><br><span class="line">•Shell脚本</span><br><span class="line">在解决问题的过程中，开发效率、执行效率都是要考虑的因素，不要太局限于某一种方法</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第6章 云数据库</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC6%E7%AB%A0-%E4%BA%91%E6%95%B0%E6%8D%AE%E5%BA%93.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC6%E7%AB%A0-%E4%BA%91%E6%95%B0%E6%8D%AE%E5%BA%93.html</id>
    <published>2023-02-28T14:31:29.000Z</published>
    <updated>2023-03-03T06:11:18.498Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第6章-云数据库"><a href="#第6章-云数据库" class="headerlink" title="第6章 云数据库"></a>第6章 云数据库</h1><h2 id="云数据库概述"><a href="#云数据库概述" class="headerlink" title="云数据库概述"></a>云数据库概述</h2><h3 id="云计算是云数据库兴起的基础"><a href="#云计算是云数据库兴起的基础" class="headerlink" title="云计算是云数据库兴起的基础"></a>云计算是云数据库兴起的基础</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303022043143.png" alt="image-20230302204336773"></p><h3 id="云数据库概念"><a href="#云数据库概念" class="headerlink" title="云数据库概念"></a>云数据库概念</h3><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303022045849.png" alt="image-20230302204509707" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">云数据库是部署和虚拟化在云计算环境中的数据库。云数据库是在云计算的大背景下发展起来的一种新兴的共享基础架构的方法，它极大地增强了数据库的存储能力，消除了人员、硬件、软件的重复配置，让软、硬件升级变得更加容易。云数据库具有高可扩展性、高可用性、采用多租形式和支持资源有效分发等特点。</span><br></pre></td></tr></table></figure><h3 id="云数据库的特性"><a href="#云数据库的特性" class="headerlink" title="云数据库的特性"></a>云数据库的特性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">（1）动态可扩展</span><br><span class="line">（2）高可用性</span><br><span class="line">（3）较低的使用代价</span><br><span class="line">（4）易用性</span><br><span class="line">（5）高性能</span><br><span class="line">（6）免维护</span><br><span class="line">（7）安全</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303022048023.png" alt="image-20230302204851887" style="zoom:67%;"><h3 id="云数据库是个性化数据存储需求的理想选择"><a href="#云数据库是个性化数据存储需求的理想选择" class="headerlink" title="云数据库是个性化数据存储需求的理想选择"></a>云数据库是个性化数据存储需求的理想选择</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">企业类型不同，对于存储的需求也千差万别，而云数据库可以很好地满足不同企业的个性化存储需求：</span><br><span class="line">•首先，云数据库可以满足大企业的海量数据存储需求</span><br><span class="line">•其次，云数据库可以满足中小企业的低成本数据存储需求</span><br><span class="line">•另外，云数据库可以满足企业动态变化的数据存储需求</span><br><span class="line"></span><br><span class="line">到底选择自建数据库还是选择云数据库，取决于企业自身的具体需求</span><br><span class="line">•对于一些大型企业，目前通常采用自建数据库</span><br><span class="line">•对于一些财力有限的中小企业而言，IT预算比较有限，云数据库这种前期零投入、后期免维护的数据库服务，可以很好满足它们的需求</span><br></pre></td></tr></table></figure><h3 id="云数据库与其他数据库的关系"><a href="#云数据库与其他数据库的关系" class="headerlink" title="云数据库与其他数据库的关系"></a>云数据库与其他数据库的关系</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•从数据模型的角度来说，云数据库并非一种全新的数据库技术，而只是以服务的方式提供数据库功能</span><br><span class="line">•云数据库并没有专属于自己的数据模型，云数据库所采用的数据模型可以是关系数据库所使用的关系模型（微软的SQL Azure云数据库、阿里云RDS都采用了关系模型），也可以是NoSQL数据库所使用的非关系模型（Amazon Dynamo云数据库采用的是“键&#x2F;值”存储）</span><br><span class="line">•同一个公司也可能提供采用不同数据模型的多种云数据库服务</span><br><span class="line">•许多公司在开发云数据库时，后端数据库都是直接使用现有的各种关系数据库或NoSQL数据库产品</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303022054765.png" alt="image-20230302205402630" style="zoom:67%;"><h2 id="云数据库产品"><a href="#云数据库产品" class="headerlink" title="云数据库产品"></a>云数据库产品</h2><h3 id="云数据库厂商概述"><a href="#云数据库厂商概述" class="headerlink" title="云数据库厂商概述"></a>云数据库厂商概述</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303022056265.png" alt="image-20230302205607114"></p><h3 id="Amazon的云数据库产品"><a href="#Amazon的云数据库产品" class="headerlink" title="Amazon的云数据库产品"></a>Amazon的云数据库产品</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Amazon是云数据库市场的先行者。Amazon除了提供著名的S3</span><br><span class="line">存储服务和EC2计算服务以外，还提供基于云的数据库服务：</span><br><span class="line">•Amazon RDS：云中的关系数据库</span><br><span class="line">•Amazon SimpleDB：云中的键值数据库</span><br><span class="line">•Amazon DynamoDB：云中的NoSQL数据库</span><br><span class="line">•Amazon Redshift：云中的数据仓库</span><br><span class="line">•Amazon ElastiCache：云中的分布式内存缓存</span><br></pre></td></tr></table></figure><h3 id="Google的云数据库产品"><a href="#Google的云数据库产品" class="headerlink" title="Google的云数据库产品"></a>Google的云数据库产品</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">•Google Cloud SQL是谷歌公司推出的基于MySQL的云数据库</span><br><span class="line">•使用Cloud SQL，所有的事务都在云中，并由谷歌管理，用户不需要配置或者排查错误</span><br><span class="line">•谷歌还提供导入或导出服务，方便用户将数据库带进或带出云</span><br><span class="line">•谷歌使用用户非常熟悉的MySQL，带有JDBC支持（适用于基于Java的App Engine应用）和DB-API支持（适用于基于Python的App Engine应用）的传统MySQL数据库环境，因此，多数应用程序不需过多调试即可运行，数据格式对于大多数开发者和管理员来说也是非常熟悉的</span><br><span class="line">•Google Cloud SQL还有一个好处就是与Google App Engine集成</span><br></pre></td></tr></table></figure><h3 id="Microsoft的云数据库产品"><a href="#Microsoft的云数据库产品" class="headerlink" title="Microsoft的云数据库产品"></a>Microsoft的云数据库产品</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SQL Azure具有以下特性：</span><br><span class="line">•属于关系型数据库：支持使用TSQL（Transact Structured Query Language）来管理、创建和操作云数据库</span><br><span class="line">•支持存储过程：它的数据类型、存储过程和传统的SQL Server具有很大的相似性，因此，应用可以在本地进行开发，然后部署到云平台上</span><br><span class="line">•支持大量数据类型：包含了几乎所有典型的SQL Server 2008的数据类型</span><br><span class="line">•支持云中的事务：支持局部事务，但是不支持分布式事务</span><br></pre></td></tr></table></figure><h3 id="其他云数据库产品"><a href="#其他云数据库产品" class="headerlink" title="其他云数据库产品"></a>其他云数据库产品</h3><h2 id="云数据库系统架构"><a href="#云数据库系统架构" class="headerlink" title="云数据库系统架构"></a>云数据库系统架构</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UMP（Unified MySQL Platform）是由阿里集团核心系统数据库团队设计与实现的，提供低成本和高性能的MySQL云数据服务。</span><br></pre></td></tr></table></figure><h3 id="UMP系统概述"><a href="#UMP系统概述" class="headerlink" title="UMP系统概述"></a>UMP系统概述</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">•UMP系统是低成本和高性能的MySQL云数据库方案</span><br><span class="line">总的来说，UMP系统架构设计遵循了以下原则：</span><br><span class="line">•保持单一的系统对外入口，并且为系统内部维护单一的资源池</span><br><span class="line">•消除单点故障，保证服务的高可用性</span><br><span class="line">•保证系统具有良好的可伸缩，能够动态地增加、删减计算与存储节点</span><br><span class="line">•保证分配给用户的资源也是弹性可伸缩的，资源之间相互隔离，确保应用和数据安全</span><br></pre></td></tr></table></figure><h3 id="UMP系统架构"><a href="#UMP系统架构" class="headerlink" title="UMP系统架构"></a>UMP系统架构</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">UMP系统中的角 LVS 色包括：</span><br><span class="line">•Controller服务器</span><br><span class="line">•Proxy服务器</span><br><span class="line">•Agent服务器</span><br><span class="line">•Web控制台</span><br><span class="line">•日志分析服务器</span><br><span class="line">•信息统计服务器</span><br><span class="line">•愚公系统</span><br><span class="line"></span><br><span class="line">依赖的开源组件包括：</span><br><span class="line">•Mnesia</span><br><span class="line">•LVS</span><br><span class="line">•RabbitMQ</span><br><span class="line">•ZooKeeper</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303022103037.png" alt="image-20230302210343710" style="zoom:67%;"><h4 id="Mnesia"><a href="#Mnesia" class="headerlink" title="Mnesia"></a>Mnesia</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•Mnesia是一个分布式数据库管理系统</span><br><span class="line">•Mnesia支持事务，支持透明的数据分片，利用两阶段锁实现分布式事务，可以线性扩展到至少50个节点</span><br><span class="line">•Mnesia的数据库模式(schema)可在运行时动态重配置，表能被迁移或复制到多个节点来改进容错性</span><br><span class="line">•Mnesia的这些特性，使其在开发云数据库时被用来提供分布式数据库服务</span><br></pre></td></tr></table></figure><h4 id="RabbitMQ"><a href="#RabbitMQ" class="headerlink" title="RabbitMQ"></a>RabbitMQ</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">•RabbitMQ是一个工业级的消息队列产品（功能类似于IBM公司的消息队列产品IBM Websphere MQ），作为消息传输中间件来使用，可以实现可靠的消息传送</span><br><span class="line">•UMP集群中各个节点之间的通信，不需要建立专门的连接，都是通过读写队列消息来实现的</span><br></pre></td></tr></table></figure><h4 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Zookeeper是高效和可靠的协同工作系统，提供分布式锁之类的基本服务（比如统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等），用于构建分布式应用，减轻分布式应用程序所承担的协调任务</span><br><span class="line"></span><br><span class="line">在UMP系统中，Zookeeper主要发挥三个作用：</span><br><span class="line">•作为全局的配置服务器</span><br><span class="line">•提供分布式锁（选出一个集群的“总管”）</span><br><span class="line">•监控所有MySQL实例</span><br></pre></td></tr></table></figure><h4 id="LVS"><a href="#LVS" class="headerlink" title="LVS"></a>LVS</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">•LVS(Linux Virtual Server)即Linux虚拟服务器，是一个虚拟的服务器集群系统</span><br><span class="line">•UMP系统借助于LVS来实现集群内部的负载均衡</span><br><span class="line">•LVS集群采用IP负载均衡技术和基于内容请求分发技术</span><br><span class="line">•调度器是LVS集群系统的唯一入口点，调度器具有很好的吞吐率，将请求均衡地转移到不同的服务器上执行，且调度器自动屏蔽掉服务器的故障，从而将一组服务器构成一个高性能的、高可用的虚拟服务器</span><br><span class="line">•整个服务器集群的结构对客户是透明的，而且无需修改客户端和服务器端的程序</span><br></pre></td></tr></table></figure><h4 id="Controller服务器"><a href="#Controller服务器" class="headerlink" title="Controller服务器"></a>Controller服务器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•Controller服务器向UMP集群提供各种管理服务，实现集群成员管理、元数据存储、MySQL实例管理、故障恢复、备份、迁移、扩容等功能</span><br><span class="line">•Controller服务器上运行了一组Mnesia分布式数据库服务，其中存储了各种系统元数据，主要包括集群成员、用户的配置和状态信息，以及用户名到后端MySQL实例地址的映射关系（或称为“路由表”）等</span><br><span class="line">•当其它服务器组件需要获取用户数据时，可以向Controller服务器发送请求获取数据</span><br><span class="line">•为了避免单点故障，保证系统的高可用性，UMP系统中部署了多台Controller服务器，然后，由Zookeeper的分布式锁功能来帮助选出一个“总管”，负责各种系统任务的调度和监控</span><br></pre></td></tr></table></figure><h4 id="Web控制台"><a href="#Web控制台" class="headerlink" title="Web控制台"></a>Web控制台</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Web控制台向用户提供系统管理界面</span><br></pre></td></tr></table></figure><h4 id="Proxy服务器"><a href="#Proxy服务器" class="headerlink" title="Proxy服务器"></a>Proxy服务器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Proxy服务器向用户提供访问MySQL数据库的服务，它完全实现了MySQL协议，用户可以使用已有的MySQL客户端连接到Proxy服务器，Proxy服务器通过用户名获取到用户的认证信息、资源配额的限制(例如QPS、IOPS（I&#x2F;O Per Second）、最大连接数等)，以及后台MySQL实例的地址，然后，用户的SQL查询请求会被转发到相应的MySQL实例上。</span><br><span class="line">除了数据路由的基本功能外，Proxy服务器中还实现了很多重要的功能，主要包括屏蔽MySQL实例故障、读写分离、分库分表、资源隔离、记录用户访问日志等</span><br></pre></td></tr></table></figure><h4 id="Agent服务器"><a href="#Agent服务器" class="headerlink" title="Agent服务器"></a>Agent服务器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Agent服务器部署在运行MySQL进程的机器上，用来管理每台物理机上的MySQL实例，执行主从切换、创建、删除、备份、迁移等操作，同时，还负责收集和分析MySQL进程的统计信息、慢查询日志（Slow Query Log）和bin-log</span><br></pre></td></tr></table></figure><h4 id="日志分析服务器"><a href="#日志分析服务器" class="headerlink" title="日志分析服务器"></a>日志分析服务器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">日志分析服务器存储和分析Proxy服务器传入的用户访问日志，并支持实时查询一段时间内的慢日志和统计报表</span><br></pre></td></tr></table></figure><h4 id="信息统计服务器"><a href="#信息统计服务器" class="headerlink" title="信息统计服务器"></a>信息统计服务器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">信息统计服务器定期将采集到的用户的连接数、QPS数值以及MySQL实例的进程状态用RRDtool进行统计，可以在 Web界面上可视化展示统计结果，也可以把统计结果作为今后实现弹性的资源分配和自动化的MySQL实例迁移的依据</span><br></pre></td></tr></table></figure><h4 id="愚公系统"><a href="#愚公系统" class="headerlink" title="愚公系统"></a>愚公系统</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">愚公系统是一个全量复制结合bin-log分析进行增量复制的工具，可以实现在不停机的情况下动态扩容、缩容和迁移</span><br></pre></td></tr></table></figure><h3 id="UMP系统功能"><a href="#UMP系统功能" class="headerlink" title="UMP系统功能"></a>UMP系统功能</h3><h2 id="Amazon-AWS和云数据库"><a href="#Amazon-AWS和云数据库" class="headerlink" title="Amazon AWS和云数据库"></a>Amazon AWS和云数据库</h2><h2 id="微软云数据库SQL-Azure"><a href="#微软云数据库SQL-Azure" class="headerlink" title="微软云数据库SQL Azure"></a>微软云数据库SQL Azure</h2><h2 id="云数据库实践"><a href="#云数据库实践" class="headerlink" title="云数据库实践"></a>云数据库实践</h2><h3 id="阿里云RDS简介"><a href="#阿里云RDS简介" class="headerlink" title="阿里云RDS简介"></a>阿里云RDS简介</h3><h3 id="RDS中的概念"><a href="#RDS中的概念" class="headerlink" title="RDS中的概念"></a>RDS中的概念</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RDS实例，是用户购买RDS服务的基本单位。在实例中：</span><br><span class="line">• 可以创建多个数据库</span><br><span class="line">• 可以使用常见的数据库客户端连接、管理及使用数据</span><br><span class="line">• 可以通过RDS管理控制台或OPEN API来创建、修改和删除数据库</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> RDS数据库，是用户在一个实例下创建的逻辑单元</span><br><span class="line">• 一个实例可以创建多个数据库，在实例内数据库命名唯一，所有数据库都会共享该实例下的资源，如CPU、内存、磁盘容量等</span><br><span class="line">• RDS不支持使用标准的SQL语句或客户端工具创建数据库，必须使用OPEN API或RDS管理控制台进行操作</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> 地域指的是用户所购买的RDS实例的服务器所处的地理位置RDS目前支持杭州、青岛、北京、深圳和香港五个地域，服务品质完全相同。用户可以在购买RDS实例时指定地域，购买实例后暂不支持更改</span><br><span class="line"> RDS可用区是指在同一地域下，电力、网络隔离的物理区域，可用区之间内网互通，可用区内网络延时更小，不同可用区之间故障隔离</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RDS可用区又分为单可用区和多可用区</span><br><span class="line">• 单可用区是指RDS实例的主备节点位于相同的可用区，它可以有效控制云产品间的网络延迟</span><br><span class="line">• 多可用区是指RDS实例的主备节点位于不同的可用区，当主节点所在可用区出现故障（如机房断电等），RDS进行主备切换后，会切换到备节点所在的可用区继续提供服务。多可用区的RDS轻松实现了同城容灾</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> 磁盘容量是用户购买RDS实例时，所选择购买的磁盘大小实例所占用的磁盘容量，除了存储表格数据外，还有实例正常运行所需要的空间，如系统数据库、数据库回滚日志、重做日志、索引等</span><br><span class="line"> RDS连接数，是应用程序可以同时连接到RDS实例的连接数量</span><br><span class="line">• 任意连接到RDS实例的连接均计算在内，与应用程序或者网站能够支持的最大用户数无关</span><br><span class="line">• 用户在购买RDS实例时所选择的内存大小决定了该实例的最大连接数</span><br></pre></td></tr></table></figure><h3 id="购买和使用RDS数据库"><a href="#购买和使用RDS数据库" class="headerlink" title="购买和使用RDS数据库"></a>购买和使用RDS数据库</h3><h4 id="购买RDS实例"><a href="#购买RDS实例" class="headerlink" title="购买RDS实例"></a>购买RDS实例</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">进入RDS页面后，点击“立即购买”，即</span><br><span class="line">可跳到下图的购买页面</span><br><span class="line">如果已经购买阿里云服务器ECS（Elastic </span><br><span class="line">Compute Service），若选择和ECS所在地</span><br><span class="line">域相同，ECS和RDS之间可以以内网方式</span><br><span class="line">访问</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031348841.png" alt="image-20230303134824745"></p><h4 id="管理RDS"><a href="#管理RDS" class="headerlink" title="管理RDS"></a>管理RDS</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">购买RDS实例成功后，可以通过管理控制台，查看已开通的产品与服务。点击云数据库RDS 进入管理界面如下图。我们可以创建新实例、对已购买实例进行管理、续费和升级操作</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031350779.png" alt="image-20230303135035722"></p><h4 id="管理RDS实例"><a href="#管理RDS实例" class="headerlink" title="管理RDS实例"></a>管理RDS实例</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•点击已购买RDS实例的管理操作，可以查看该实例</span><br><span class="line">的基本信息如下图</span><br><span class="line">• 一个实例可以创建多个数据库，在实例内数据库</span><br><span class="line">命名唯一，所有数据库都会共享该实例下的资源</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031352019.png" alt="image-20230303135228962" style="zoom:67%;"><h4 id="新建RDS账号"><a href="#新建RDS账号" class="headerlink" title="新建RDS账号"></a>新建RDS账号</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 点击“创建新账号”按钮后，可创建新的RDS账号，并选定需绑定的数据库，以及输入账号密码和账号类型（读写权限）等信息</span><br><span class="line">• MySQL实例支持最多创建50个账号，SQL Server实例支持最多创建20个账号</span><br><span class="line">• 创建完RDS账号后，还可以对RDS账号进行重置密码和修改操作</span><br></pre></td></tr></table></figure><h4 id="新建RDS数据库"><a href="#新建RDS数据库" class="headerlink" title="新建RDS数据库"></a>新建RDS数据库</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 点击“数据库管理”按钮后，可查看数据库基本信息，并可对数据库进行创建、删除（需至少有1个数据库）的操作</span><br><span class="line">• 点击“增加数据库”后，在弹出的界面中填写数据库相关信息，提交后即可生效</span><br><span class="line">• 此外，RDS数据库还可以是自建数据库迁移来的或是从其他RDS实例中迁入的</span><br></pre></td></tr></table></figure><h4 id="连接RDS数据库"><a href="#连接RDS数据库" class="headerlink" title="连接RDS数据库"></a>连接RDS数据库</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 如果是在阿里云服务器ECS上连接RDS数据库，就选择内网模式；</span><br><span class="line">• 如果是在其他服务器上连接RDS使用，就选择外网模式，在控制台的右上角有切换方式</span><br><span class="line">• 从本地对云端的RDS数据库进行远程访问时使用外网模式，需要在“安全控制-&gt;白名单设置”位置填入本地机器的外网IP，从而让RDS数据库允许我们的本地机器访问</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> 使用客户端MySQL-Front 访问</span><br><span class="line"> 使用数据库管理工具Navicat_MySQL</span><br><span class="line"> 使用MySQL命令登录</span><br><span class="line">命令格式如下：</span><br><span class="line">mysql -u user_name -h yuqianli.mysql.rds.aliyuncs.com -P3306 -pxxxx</span><br><span class="line"> 使用阿里云控制台iDB Cloud访问</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031401865.png" alt="image-20230303140131813" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RDS连接地址以及端口不需要再输入，只需在“用户名”中输入数据库的账号，在“密码”栏中输入数据库账号的密码，便可以登录RDS。</span><br></pre></td></tr></table></figure><h4 id="操作RDS数据库"><a href="#操作RDS数据库" class="headerlink" title="操作RDS数据库"></a>操作RDS数据库</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">• 连接RDS数据库后，对数据库的操作与直接对本机MySQL数据库操作无异。iDB Cloud登录数据库后的界面如下图所示：</span><br><span class="line">• 在“iDB Cloud登录数据库界面”（如右图）</span><br><span class="line">的顶端可以看到iDB Cloud提供以下三种创建</span><br><span class="line">表的方法：</span><br><span class="line">可视化界面</span><br><span class="line">SQL窗口</span><br><span class="line">命令窗口</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031403043.png" alt="image-20230303140347992"></p><h3 id="将本地数据库迁移到云端RDS数据库"><a href="#将本地数据库迁移到云端RDS数据库" class="headerlink" title="将本地数据库迁移到云端RDS数据库"></a>将本地数据库迁移到云端RDS数据库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">• 假设我们有一个本地应用程序，它使用本地的MySQL数据库存取和管理数据。现在，我们打算把本地MySQL数据库中的数据全部迁移到远程的阿里云RDS数据库中，本地应用程序不迁移（依然运行在本地），但是，我们希望本地应用程序使用云端的RDS数据库服务进行数据存取和管</span><br><span class="line">理。为此，需要执行以下两步操作：</span><br><span class="line">第一步：把本地数据库迁移到云端的RDS数据库</span><br><span class="line">第二步：修改本地应用程序配置，使用RDS数据库服务</span><br></pre></td></tr></table></figure><h4 id="如何把本地数据库迁移到云端的RDS数据库？"><a href="#如何把本地数据库迁移到云端的RDS数据库？" class="headerlink" title="如何把本地数据库迁移到云端的RDS数据库？"></a>如何把本地数据库迁移到云端的RDS数据库？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">第1步：在本地数据库中创建一个迁移账号</span><br><span class="line">第2步：设置迁移账号权限</span><br><span class="line">第3步：确认本地数据库中的配置文件是否正确，需要确认本地数据</span><br><span class="line">库中的MySQL配置文件my.cnf</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031405291.png" alt="image-20230303140559230"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">第4步：登录本地数据库，通过命令查看是否为“ROW”模式</span><br><span class="line">第5步：在RDS管理控制台对应的实例页面，点击“将数据迁移至RDS”按钮，在弹出的页面中，填写待迁移的本地数据库连接地址、数据库连接端口、数据库账号、数据库密码，即可完成从本地迁移到云端</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第5章 NoSQL数据库</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC5%E7%AB%A0-NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC5%E7%AB%A0-NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93.html</id>
    <published>2023-02-28T14:31:10.000Z</published>
    <updated>2023-03-02T12:37:26.982Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第5章-NoSQL数据库"><a href="#第5章-NoSQL数据库" class="headerlink" title="第5章 NoSQL数据库"></a>第5章 NoSQL数据库</h1><h2 id="NoSQL简介"><a href="#NoSQL简介" class="headerlink" title="NoSQL简介"></a>NoSQL简介</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303010035462.png" alt="image-20230301003541415"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">通常，NoSQL数据库具有以下几个特点：</span><br><span class="line">（1）灵活的可扩展性</span><br><span class="line">（2）灵活的数据模型</span><br><span class="line">（3）与云计算紧密融合</span><br></pre></td></tr></table></figure><h2 id="NoSQL兴起的原因"><a href="#NoSQL兴起的原因" class="headerlink" title="NoSQL兴起的原因"></a>NoSQL兴起的原因</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">关系数据库已经无法满足Web2.0的需求。主要表现在以下几个方面：</span><br><span class="line">（1）无法满足海量数据的管理需求</span><br><span class="line">（2）无法满足数据高并发的需求</span><br><span class="line">（3）无法满足高可扩展性和高可用性的需求</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">（主从机制实现读写负载分离，同步或异步传输）--到---&gt;分库分表</span><br><span class="line">MySQL集群是否可以完全解决问题？</span><br><span class="line">•复杂性：部署、管理、配置很复杂</span><br><span class="line">•数据库复制：MySQL主备之间采用复制方式，只能是异步复制，当主库压力较大时可能产生较大延迟，主备切换可能会丢失最后一部分更新事务，这时往往需要人工介入，备份和恢复不方便</span><br><span class="line">•扩容问题：如果系统压力过大需要增加新的机器，这个过程涉及数据重新划分，整个过程比较复杂，且容易出错</span><br><span class="line">•动态数据迁移问题：如果某个数据库组压力过大，需要将其中部分数据迁移出去，迁移过程需要总控节点整体协调，以及数据库节点的配合。这个过程很难做到自动化</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303010045076.png" alt="image-20230301004517003"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">“One size fits all”模式很难适用于截然不同的业务场景</span><br><span class="line">•关系模型作为统一的数据模型既被用于数据分析，也被用于在线业务。但这两者一个强调高吞吐，一个强调低延时，已经演化出完全不同的架构。用同一套模型来抽象显然是不合适的</span><br><span class="line">•Hadoop就是针对数据分析</span><br><span class="line">•MongoDB、Redis等是针对在线业务，两者都抛弃了关系模型</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">关系数据库的关键特性包括完善的事务机制和高效的查询机制。但是，关系数据库引以为傲的两个关键特性，到了Web2.0时代却成了鸡肋，主要表现在以下几个方面：</span><br><span class="line">（1）Web2.0网站系统通常不要求严格的数据库事务(银行交易)</span><br><span class="line">（2）Web2.0并不要求严格的读写实时性(微博发布后是否快速可见)</span><br><span class="line">（3）Web2.0通常不包含大量复杂的SQL查询（去结构化，存储空间换取更好的查询性能）</span><br></pre></td></tr></table></figure><h2 id="NoSQL与关系数据库的比较"><a href="#NoSQL与关系数据库的比较" class="headerlink" title="NoSQL与关系数据库的比较"></a>NoSQL与关系数据库的比较</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ACID，是指数据库管理系统（DBMS）在写入或更新资料的过程中，为保证事务（transaction）是正确可靠的，所必须具备的四个特性：原子性（atomicity，或称不可分割性）、一致性（consistency）、隔离性（isolation，又称独立性）、持久性（durability）。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RDBMS:关系型数据库管理系统</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC5%E7%AB%A0-NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230228233539924.png" alt="image-20230228233539924" style="zoom:80%;"><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302282336998.png" alt="image-20230228233645936"></p><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC5%E7%AB%A0-NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230228233700240.png" alt="image-20230228233700240" style="zoom:80%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">总结</span><br><span class="line">（1）关系数据库</span><br><span class="line">优势：以完善的关系代数理论作为基础，有严格的标准，支持事务ACID四性，借助索引机制可以实现高效的查询，技术成熟，有专业公司的技术支持</span><br><span class="line">劣势：可扩展性较差，无法较好支持海量数据存储，数据模型过于死板、无法较好支持Web2.0应用，事务机制影响了系统的整体性能等</span><br><span class="line">（2）NoSQL数据库</span><br><span class="line">优势：可以支持超大规模数据存储，灵活的数据模型可以很好地支持Web2.0应用，具有强大的横向扩展能力等</span><br><span class="line">劣势：缺乏数学理论基础，复杂查询性能不高，大都不能实现事务强一致性，很难实现数据完整性，技术尚不成熟，缺乏专业团队的技术支持，维护较困难等</span><br><span class="line"></span><br><span class="line">关系数据库和NoSQL数据库各有优缺点，彼此无法取代</span><br><span class="line">•关系数据库应用场景：电信、银行等领域的关键业务系统，需要保证强事务一致性</span><br><span class="line">•NoSQL数据库应用场景：互联网企业、传统企业的非关键业务（比如数据分析）</span><br><span class="line"></span><br><span class="line">采用混合架构</span><br><span class="line">•案例：亚马逊公司就使用不同类型的数据库来支撑它的电子商务应用</span><br><span class="line">•对于“购物篮”这种临时性数据，采用键值存储会更加高效</span><br><span class="line">•当前的产品和订单信息则适合存放在关系数据库中</span><br><span class="line">•大量的历史订单信息则适合保存在类似MongoDB的文档数据库中</span><br></pre></td></tr></table></figure><h2 id="NoSQL的四大类型"><a href="#NoSQL的四大类型" class="headerlink" title="NoSQL的四大类型"></a>NoSQL的四大类型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NoSQL数据库虽然数量众多，但是，归结起来，典型的NoSQL数据库通常包括键值数据库、列族数据库、文档数据库和图形数据库</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC5%E7%AB%A0-NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230228234144829.png" alt="image-20230228234144829" style="zoom:80%;"><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC5%E7%AB%A0-NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230228234215303.png" alt="image-20230228234215303" style="zoom:80%;"><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC5%E7%AB%A0-NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230228234306458.png" alt="image-20230228234306458" style="zoom:80%;"><h3 id="键值数据库"><a href="#键值数据库" class="headerlink" title="键值数据库"></a>键值数据库</h3><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302282344758.png" alt="image-20230228234416688" style="zoom:80%;"><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302282345799.png" alt="image-20230228234538745"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">关系型数据库管理系统（Relational Database Management System）RDBMS</span><br></pre></td></tr></table></figure><h3 id="列族数据库"><a href="#列族数据库" class="headerlink" title="列族数据库"></a>列族数据库</h3><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC5%E7%AB%A0-NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230228234918950.png" alt="image-20230228234918950" style="zoom:80%;"><h3 id="文档数据库"><a href="#文档数据库" class="headerlink" title="文档数据库"></a>文档数据库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">“文档”其实是一个数据记录，这个记录能够对包含的数据类型和内容进行“自我描述”。XML文档、HTML文档和JSON 文档就属于这一类。SequoiaDB就是使用JSON格式的文档数据库，它的存储的数据是这样的</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302282351530.png" alt="image-20230228235133485"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302282352815.png" alt="image-20230228235234768"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">关系数据库：</span><br><span class="line">必须有schema信息才能理解数据的含义</span><br><span class="line">学生（学号，姓名，性别，年龄，系，年级）</span><br><span class="line">（1001，张三，男，20，计算机，2002）</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">•数据是不规则的，每一条记录包含了所有的有关“SequoiaDB”的信息而没有任何外部的引用，这条记录就是“自包含”的</span><br><span class="line">•这使得记录很容易完全移动到其他服务器，因为这条记录的所有信息都包含在里面了，不需要考虑还有信息在别的表没有一起迁移走</span><br><span class="line">•同时，因为在移动过程中，只有被移动的那一条记录（文档）需要操作，而不像关系型中每个有关联的表都需要锁住来保证一致性，这样一来ACID的保证就会变得更快速，读写的速度也会有很大的提升</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302282357394.png" alt="image-20230228235722334"></p><h3 id="图形数据库"><a href="#图形数据库" class="headerlink" title="图形数据库"></a>图形数据库</h3><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC5%E7%AB%A0-NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230228235744083.png" alt="image-20230228235744083" style="zoom:80%;"><h2 id="NoSQL的三大基石"><a href="#NoSQL的三大基石" class="headerlink" title="NoSQL的三大基石"></a>NoSQL的三大基石</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302282359330.png" alt="image-20230228235943292"></p><h3 id="CAP"><a href="#CAP" class="headerlink" title="CAP"></a>CAP</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">C（Consistency）：一致性，是指任何一个读操作总是能够读到之前完成的写操作的结果，也就是在分布式环境中，多点的数据是一致的，或者说，所有节点在同一时间具有相同的数据</span><br><span class="line">A:（Availability）：可用性，是指快速获取数据，可以在确定的时间内返回操作结果，保证每个请求不管成功或者失败都有响应；</span><br><span class="line">P（Tolerance of Network Partition）：分区容忍性，是指当出现网络分区的情况时（即系统中的一部分节点无法和其他节点进行通信），分离的系统也能够正常运行，也就是说，系统中任意信息的丢失或失败不会影响系统的继续运作。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CAP理论告诉我们，一个分布式系统不可能同时满足一致性、可用性和分区容忍性这三个需求，最多只能同时满足其中两个，正所谓“鱼和熊掌不可兼得”。</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303010003922.png" alt="image-20230301000322877" style="zoom:67%;"><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303010123156.png" alt="image-20230301012357087" style="zoom: 50%;"><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303010124239.png" alt="image-20230301012418187" style="zoom:50%;"><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303010124061.png" alt="image-20230301012434006" style="zoom:50%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">当处理CAP的问题时，可以有几个明显的选择：</span><br><span class="line">1.CA：也就是强调一致性（C）和可用性（A），放弃分区容忍性（P），最</span><br><span class="line">简单的做法是把所有与事务相关的内容都放到同一台机器上。很显然，这种</span><br><span class="line">做法会严重影响系统的可扩展性。传统的关系数据库（MySQL、SQL Server</span><br><span class="line">和PostgreSQL），都采用了这种设计原则，因此，扩展性都比较差</span><br><span class="line">2.CP：也就是强调一致性（C）和分区容忍性（P），放弃可用性（A），当</span><br><span class="line">出现网络分区的情况时，受影响的服务需要等待数据一致，因此在等待期间</span><br><span class="line">就无法对外提供服务</span><br><span class="line">3.AP：也就是强调可用性（A）和分区容忍性（P），放弃一致性（C），允</span><br><span class="line">许系统返回不一致的数据</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303010127593.png" alt="image-20230301012745550" style="zoom:50%;"><h3 id="BASE"><a href="#BASE" class="headerlink" title="BASE"></a>BASE</h3> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">说起BASE（Basically Availble, Soft-state, Eventual consistency），</span><br><span class="line">不得不谈到ACID。</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303010131759.png" alt="image-20230301013119713" style="zoom:80%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">一个数据库事务具有ACID四性：</span><br><span class="line">A（Atomicity）：原子性，是指事务必须是原子工作单元，对于其数</span><br><span class="line">据修改，要么全都执行，要么全都不执行</span><br><span class="line">C（Consistency）：一致性，是指事务在完成时，必须使所有的数据</span><br><span class="line">都保持一致状态</span><br><span class="line">I（Isolation）：隔离性，是指由并发事务所做的修改必须与任何其它</span><br><span class="line">并发事务所做的修改隔离</span><br><span class="line">D（Durability）：持久性，是指事务完成之后，它对于系统的影响是</span><br><span class="line">永久性的，该修改即使出现致命的系统故障也将一直保持</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">BASE的基本含义是基本可用（Basically Availble）、软状态（Soft state）和最终一致性（Eventual consistency）：</span><br><span class="line">  基本可用</span><br><span class="line">基本可用，是指一个分布式系统的一部分发生问题变得不可用时，其他部分仍然可以正常使用，也就是允许分区失败的情形出现</span><br><span class="line">  软状态</span><br><span class="line">“软状态（soft-state）”是与“硬状态（hard-state）”相对应的一种提法。数据库保存的数据是“硬状态”时，可以保证数据一致性，即保证数据一直是正确的。“软状态”是指状态可以有一段时间不同步，具有一定的滞后性</span><br><span class="line">  最终一致性</span><br><span class="line">一致性的类型包括强一致性和弱一致性，二者的主要区别在于高并发的数据访问操作下，后续操作是否能够获取最新的数据。对于强一致性而言，当执行完一次更新操作后，后续的其他读操作就可以保证读到更新后的最新数据；反之，如果不能保证后续访问读到的都是更新后的最新数据，那么就是弱一致性。而最终一致性只不过是弱一致性的一种特例，允许后续的访问操作可以暂时读不到更新后的数据，但是经过一段时间之后，必须最终读到更新后的数据。</span><br><span class="line">最常见的实现最终一致性的系统是DNS（域名系统）。一个域名更新操作根据配置的形式被分发出去，并结合有过期机制的缓存；最终所有的客户端可以看到最新的值。</span><br></pre></td></tr></table></figure><h3 id="最终一致性"><a href="#最终一致性" class="headerlink" title="最终一致性"></a>最终一致性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">最终一致性根据更新数据后各进程访问到数据的时间和方式的不同，</span><br><span class="line">  又可以区分为：</span><br><span class="line">  因果一致性：如果进程A通知进程B它已更新了一个数据项，那么进程B</span><br><span class="line">的后续访问将获得A写入的最新值。而与进程A无因果关系的进程C的访问</span><br><span class="line">，仍然遵守一般的最终一致性规则</span><br><span class="line">  “读己之所写”一致性：可以视为因果一致性的一个特例。当进程A自</span><br><span class="line">己执行一个更新操作之后，它自己总是可以访问到更新过的值，绝不会看</span><br><span class="line">到旧值</span><br><span class="line">  单调读一致性：如果进程已经看到过数据对象的某个值，那么任何后续</span><br><span class="line">访问都不会返回在那个值之前的值</span><br><span class="line">  会话一致性：它把访问存储系统的进程放到会话（session）的上下文中</span><br><span class="line">，只要会话还存在，系统就保证“读己之所写”一致性。如果由于某些失</span><br><span class="line">败情形令会话终止，就要建立新的会话，而且系统保证不会延续到新的会</span><br><span class="line">话</span><br><span class="line">  单调写一致性：系统保证来自同一个进程的写操作顺序执行。系统必须</span><br><span class="line">保证这种程度的一致性，否则就非常难以编程了</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">如何实现各种类型的一致性？</span><br><span class="line">对于分布式数据系统：</span><br><span class="line">•N — 数据复制的份数</span><br><span class="line">•W — 更新数据是需要保证写完成的节点数</span><br><span class="line">•R — 读取数据的时候需要读取的节点数</span><br><span class="line">如果W+R&gt;N，写的节点和读的节点重叠，则是强一致性。例如对于典型的一主</span><br><span class="line">一备同步复制的关系型数据库，N&#x3D;2,W&#x3D;2,R&#x3D;1，则不管读的是主库还是备库的</span><br><span class="line">数据，都是一致的。一般设定是R＋W &#x3D; N+1，这是保证强一致性的最小设定如果W+R&lt;&#x3D;N，则是弱一致性。例如对于一主一备异步复制的关系型数据库，N&#x3D;2,W&#x3D;1,R&#x3D;1，则如果读的是备库，就可能无法读取主库已经更新过的数据，所以是弱一致性</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">对于分布式系统，为了保证高可用性，一般设置N&gt;&#x3D;3。不同的N,W,R组合，是</span><br><span class="line">在可用性和一致性之间取一个平衡，以适应不同的应用场景。</span><br><span class="line">•如果N&#x3D;W,R&#x3D;1，任何一个写节点失效，都会导致写失败，因此可用性会降低，</span><br><span class="line">但是由于数据分布的N个节点是同步写入的，因此可 以保证强一致性。</span><br><span class="line">实例：HBase是借助其底层的HDFS来实现其数据冗余备份的。HDFS采用的就</span><br><span class="line">是强一致性保证。在数据没有完全同步到N个节点前，写操作是不会返回成功的</span><br><span class="line">。也就是说它的W＝N，而读操作只需要读到一个值即可，也就是说它R＝1。</span><br><span class="line">•像Voldemort，Cassandra和Riak这些类Dynamo的系统，通常都允许用户按需要设置N，R，W三个值，即使是设置成W＋R&lt;&#x3D; N也是可以的。也就是说他允</span><br><span class="line">许用户在强一致性和最终一致性之间自由选择。而在用户选择了最终一致性，或</span><br><span class="line">者是W&lt;N的强一致性时，则总会出现一段“各个节点数据不同步导致系统处理</span><br><span class="line">不一致的时间”。为了提供最终一致性的支持，这些系统会提供一些工具来使数</span><br><span class="line">据更新被最终同步到所有相关节点。</span><br></pre></td></tr></table></figure><h2 id="从NoSQL到NewSQL数据库"><a href="#从NoSQL到NewSQL数据库" class="headerlink" title="从NoSQL到NewSQL数据库"></a>从NoSQL到NewSQL数据库</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">关系型数据库：适用事务型</span><br><span class="line">非关系型数据库：适用互联网</span><br><span class="line">newsql：适用数据分析</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303021517991.png" alt="image-20230302151742886" style="zoom:80%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">newsql数据库同时具备关系型数据库和非关系型数据库的优点(事务和水平拓展)</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303021518715.png" alt="image-20230302151829648" style="zoom:80%;"><h2 id="文档数据库MongoDB"><a href="#文档数据库MongoDB" class="headerlink" title="文档数据库MongoDB"></a>文档数据库MongoDB</h2><a href="/%E5%B4%94%E5%BA%86%E6%89%8Dpython3%E7%88%AC%E8%99%AB-pymongo.html" title="文章标题（可选）">文章标题（可选）</a><h3 id="MongoDB简介"><a href="#MongoDB简介" class="headerlink" title="MongoDB简介"></a>MongoDB简介</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">•MongoDB 是由C++语言编写的，是一个基于分布式文件存储的开源数据</span><br><span class="line">库系统。</span><br><span class="line">•在高负载的情况下，添加更多的节点，可以保证服务器性能。</span><br><span class="line">•MongoDB 旨在为WEB应用提供可扩展的高性能数据存储解决方案。</span><br><span class="line">•MongoDB 将数据存储为一个文档，数据结构由键值(key&#x3D;&gt;value)对组成。MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303021536435.png" alt="image-20230302153655381" style="zoom:67%;"><h4 id="主要特点"><a href="#主要特点" class="headerlink" title="主要特点"></a>主要特点</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">•提供了一个面向文档存储，操作起来比较简单和容易</span><br><span class="line">•可以设置任何属性的索引来实现更快的排序</span><br><span class="line">•具有较好的水平可扩展性</span><br><span class="line">•支持丰富的查询表达式，可轻易查询文档中内嵌的对象及数组</span><br><span class="line">•可以实现替换完成的文档（数据）或者一些指定的数据字段</span><br><span class="line">•MongoDB中的Map&#x2F;Reduce主要是用来对数据进行批量处理和聚合操作</span><br><span class="line">•支持各种编程语言:RUBY，PYTHON，JAVA，C++，PHP，C#等语言</span><br><span class="line">•MongoDB安装简单</span><br></pre></td></tr></table></figure><h3 id="MongoDB概念解析"><a href="#MongoDB概念解析" class="headerlink" title="MongoDB概念解析"></a>MongoDB概念解析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在mongodb中基本的概念是文档、集合、数据库</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303021538144.png" alt="image-20230302153842093" style="zoom:67%;"><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303021540254.png" alt="image-20230302154010180" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">举例2：在一个关系型数据库中，一篇博客（包含文章内容、评论、评论的投票</span><br><span class="line">）会被打散在多张数据表中。在文档数据库MongoDB中，能用一个文档来表示</span><br><span class="line">一篇博客， 评论与投票作为文档数组，放在正文主文档中。这样数据更易于管</span><br><span class="line">理，消除了传统关系型数据库中影响性能和水平扩展性的“JOIN”操作。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303021542579.png" alt="image-20230302154214533"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">关系数据库中的其中一条记录，在文档数据库MongoDB中的存储方式类似如下：</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303021542657.png" alt="image-20230302154247610" style="zoom:67%;"><h4 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">•一个mongodb中可以建立多个数据库。</span><br><span class="line">•MongoDB的默认数据库为&quot;db&quot;，该数据库存储在data目录中。</span><br><span class="line">•MongoDB的单个实例可以容纳多个独立的数据库，每一个都有自己的集合和权限，不同的数据库也放置在不同的文件中。</span><br></pre></td></tr></table></figure><h4 id="文档"><a href="#文档" class="headerlink" title="文档"></a>文档</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">文档是一个键值(key-value)对(即BSON)。MongoDB 的文档不需要设置相</span><br><span class="line">同的字段，并且相同的字段不需要相同的数据类型，这与关系型数据库有很大的区别，也是 MongoDB 非常突出的特点。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303021546083.png" alt="image-20230302154657033"></p><h4 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">•集合就是 MongoDB 文档组，类似于 RDBMS （关系数据库管理系统：</span><br><span class="line">Relational Database Management System)中的表格。</span><br><span class="line">•集合存在于数据库中，集合没有固定的结构，这意味着你在对集合可以插</span><br><span class="line">入不同格式和类型的数据，但通常情况下我们插入集合的数据都会有一定的关联性。</span><br><span class="line">比如，我们可以将以下不同数据结构的文档插入到集合中：</span><br></pre></td></tr></table></figure><h4 id="MongoDB-数据类型"><a href="#MongoDB-数据类型" class="headerlink" title="MongoDB 数据类型"></a>MongoDB 数据类型</h4><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC5%E7%AB%A0-NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230302155125678.png" alt="image-20230302155125678" style="zoom:67%;"><h3 id="安装MongoDB"><a href="#安装MongoDB" class="headerlink" title="安装MongoDB"></a>安装MongoDB</h3><h4 id="Window平台安装-MongoDB"><a href="#Window平台安装-MongoDB" class="headerlink" title="Window平台安装 MongoDB"></a>Window平台安装 MongoDB</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MongoDB提供了可用于32位和64位系统的预编译二进制包，你可</span><br><span class="line">以从MongoDB官网下载安装，MongoDB预编译二进制包下载地址</span><br><span class="line">：http:&#x2F;&#x2F;www.mongodb.org&#x2F;downloads</span><br><span class="line">注意：在 MongoDB2.2 版本后已经不再支持 Windows XP 系统。</span><br></pre></td></tr></table></figure><h4 id="Linux平台安装MongoDB"><a href="#Linux平台安装MongoDB" class="headerlink" title="Linux平台安装MongoDB"></a>Linux平台安装MongoDB</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MongoDB提供了linux平台上32位和64位的安装包，你可以在官网下载安装包。</span><br><span class="line">下载地址：http:&#x2F;&#x2F;www.mongodb.org&#x2F;downloads</span><br><span class="line">启动 MongoDB服务</span><br><span class="line">只需要在MongoDB安装目录的bin目录下执行&#39;mongod&#39;即可</span><br></pre></td></tr></table></figure><h3 id="访问MongoDB"><a href="#访问MongoDB" class="headerlink" title="访问MongoDB"></a>访问MongoDB</h3><h4 id="使用-MongoDB-shell访问MongoDB"><a href="#使用-MongoDB-shell访问MongoDB" class="headerlink" title="使用 MongoDB shell访问MongoDB"></a>使用 MongoDB shell访问MongoDB</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mongodb:&#x2F;&#x2F;localhost</span><br><span class="line">•使用 MongoDB shell 来连接 MongoDB 服务器</span><br><span class="line">•使用用户名和密码连接登陆到指定数据库：</span><br><span class="line">mongodb:&#x2F;&#x2F;admin:123456@localhost&#x2F;test</span><br></pre></td></tr></table></figure><h5 id="MongoDB-创建数据库"><a href="#MongoDB-创建数据库" class="headerlink" title="MongoDB 创建数据库"></a>MongoDB 创建数据库</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MongoDB 创建数据库的语法格式如下：</span><br><span class="line">use DATABASE_NAME</span><br><span class="line">如果数据库不存在，则创建数据库，否则切换到指定数据库。</span><br><span class="line">如果你想查看所有数据库，可以使用 show dbs 命令</span><br></pre></td></tr></table></figure><h5 id="创建集合"><a href="#创建集合" class="headerlink" title="创建集合"></a>创建集合</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MongoDB没有单独创建集合名的shell命令，在插入数据的时候，MongoDB会自动创建对应的集合。</span><br></pre></td></tr></table></figure><h5 id="MongoDB-插入文档"><a href="#MongoDB-插入文档" class="headerlink" title="MongoDB 插入文档"></a>MongoDB 插入文档</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">文档的数据结构和JSON基本一样。</span><br><span class="line">所有存储在集合中的数据都是BSON格式。</span><br><span class="line">BSON是一种类JSON的一种二进制形式的存储格式,简称Binary JSON。</span><br><span class="line">MongoDB 使用 insert() 或 save() 方法向集合中插入文档，语法如下：</span><br><span class="line">db.COLLECTION_NAME.insert(document)</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303021557873.png" alt="image-20230302155758827" style="zoom:50%;"><h4 id="使用Java程序访问-MongoDB"><a href="#使用Java程序访问-MongoDB" class="headerlink" title="使用Java程序访问 MongoDB"></a>使用Java程序访问 MongoDB</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">环境配置</span><br><span class="line">•在Java程序中如果要使用MongoDB，需要确保已经安装了Java环境及MongoDB JDBC 驱动。</span><br><span class="line">•首先必须下载mongo jar包，下载地址：</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;mongodb&#x2F;mongo-java-driver&#x2F;downloads, 请确保下载最新版本。</span><br><span class="line">•需要将mongo.jar包含在你的 classpath 中</span><br></pre></td></tr></table></figure><h5 id="（1）连接数据库"><a href="#（1）连接数据库" class="headerlink" title="（1）连接数据库"></a>（1）连接数据库</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.mongodb.MongoClient;</span><br><span class="line">……<span class="comment">//这里省略其他需要导入的包</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MongoDBJDBC</span></span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">( String args[] )</span></span>&#123;</span><br><span class="line"><span class="keyword">try</span>&#123; </span><br><span class="line">    <span class="comment">// 连接到 mongodb 服务</span></span><br><span class="line">    MongoClient mongoClient = <span class="keyword">new</span> MongoClient( <span class="string">"localhost"</span> , <span class="number">27017</span> );</span><br><span class="line">    <span class="comment">// 连接到数据库</span></span><br><span class="line">    DB db = mongoClient.getDB( <span class="string">"test"</span> );</span><br><span class="line">    System.out.println(<span class="string">"Connect to database successfully"</span>);</span><br><span class="line">    <span class="keyword">boolean</span> auth = db.authenticate(myUserName, myPassword);</span><br><span class="line">    System.out.println(<span class="string">"Authentication: "</span>+auth);</span><br><span class="line">&#125;<span class="keyword">catch</span>(Exception e)&#123;</span><br><span class="line">    System.err.println( e.getClass().getName() + <span class="string">": "</span> + e.getMessage() );</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="（2）创建集合"><a href="#（2）创建集合" class="headerlink" title="（2）创建集合"></a>（2）创建集合</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">可以使用com.mongodb.DB类中的createCollection()来创建集合</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MongoDBJDBC</span></span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">( String args[] )</span></span>&#123;</span><br><span class="line"><span class="keyword">try</span>&#123; </span><br><span class="line">    <span class="comment">// 连接到 mongodb 服务</span></span><br><span class="line">    MongoClient mongoClient = <span class="keyword">new</span> MongoClient( <span class="string">"localhost"</span> , <span class="number">27017</span> );</span><br><span class="line">    <span class="comment">// 连接到数据库</span></span><br><span class="line">    DB db = mongoClient.getDB( <span class="string">"test"</span> );</span><br><span class="line">    System.out.println(<span class="string">"Connect to database successfully"</span>);</span><br><span class="line">    <span class="keyword">boolean</span> auth = db.authenticate(myUserName, myPassword);</span><br><span class="line">    System.out.println(<span class="string">"Authentication: "</span>+auth);</span><br><span class="line">    DBCollection coll = db.createCollection(<span class="string">"mycol"</span>);</span><br><span class="line">    System.out.println(<span class="string">"Collection created successfully"</span>);</span><br><span class="line">&#125;<span class="keyword">catch</span>(Exception e)&#123;</span><br><span class="line">    System.err.println( e.getClass().getName() + <span class="string">": "</span> + e.getMessage() );</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="（3）插入文档"><a href="#（3）插入文档" class="headerlink" title="（3）插入文档"></a>（3）插入文档</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">可以使用com.mongodb.DBCollection类的 insert() 方法来插入一个文档</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MongoDBJDBC</span></span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">( String args[] )</span></span>&#123;</span><br><span class="line"><span class="keyword">try</span>&#123; </span><br><span class="line">    <span class="comment">// 连接到 mongodb 服务</span></span><br><span class="line">    MongoClient mongoClient = <span class="keyword">new</span> MongoClient( <span class="string">"localhost"</span> , <span class="number">27017</span> );</span><br><span class="line">    <span class="comment">// 连接到数据库</span></span><br><span class="line">    DB db = mongoClient.getDB( <span class="string">"test"</span> );</span><br><span class="line">    System.out.println(<span class="string">"Connect to database successfully"</span>);</span><br><span class="line">    <span class="keyword">boolean</span> auth = db.authenticate(myUserName, myPassword);</span><br><span class="line">    System.out.println(<span class="string">"Authentication: "</span>+auth); </span><br><span class="line">    DBCollection coll = db.getCollection(<span class="string">"mycol"</span>);</span><br><span class="line">    System.out.println(<span class="string">"Collection mycol selected successfully"</span>);</span><br><span class="line">    BasicDBObject doc = <span class="keyword">new</span> BasicDBObject(<span class="string">"title"</span>, <span class="string">"MongoDB"</span>).</span><br><span class="line">    append(<span class="string">"description"</span>, <span class="string">"database"</span>).</span><br><span class="line">    append(<span class="string">"likes"</span>, <span class="number">100</span>).</span><br><span class="line">    append(<span class="string">"url"</span>, <span class="string">"http://www.w3cschool.cc/mongodb/"</span>).</span><br><span class="line">    append(<span class="string">"by"</span>, <span class="string">"w3cschool.cc"</span>);</span><br><span class="line">    coll.insert(doc);</span><br><span class="line">    System.out.println(<span class="string">"Document inserted successfully"</span>);</span><br><span class="line">&#125;<span class="keyword">catch</span>(Exception e)&#123;</span><br><span class="line">System.err.println( e.getClass().getName() + <span class="string">": "</span> + e.getMessage() );</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第四章 分布式数据库HBase</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93HBase.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93HBase.html</id>
    <published>2023-02-28T05:13:43.000Z</published>
    <updated>2023-02-28T14:29:12.985Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第四章-分布式数据库HBase"><a href="#第四章-分布式数据库HBase" class="headerlink" title="第四章 分布式数据库HBase"></a>第四章 分布式数据库HBase</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="从BigTable说起"><a href="#从BigTable说起" class="headerlink" title="从BigTable说起"></a>从BigTable说起</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BigTable是一个分布式存储系统</span><br><span class="line">BigTable起初用于解决典型的互联网搜索问题</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">•建立互联网索引</span><br><span class="line">1 爬虫持续不断地抓取新页面，这些页面每页一行地存储到BigTable里</span><br><span class="line">2 MapReduce计算作业运行在整张表上，生成索引，为网络搜索应用做准备</span><br><span class="line">•搜索互联网</span><br><span class="line">3 用户发起网络搜索请求</span><br><span class="line">4 网络搜索应用查询建立好的索引，从BigTable得到网页</span><br><span class="line">5 搜索结果提交给用户</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302281327539.png" alt="image-20230228132330400"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">•BigTable是一个分布式存储系统</span><br><span class="line">•利用谷歌提出的MapReduce分布式并行计算模型来处理海量数据</span><br><span class="line">•使用谷歌分布式文件系统GFS作为底层数据存储</span><br><span class="line">•采用Chubby提供协同服务管理</span><br><span class="line">•可以扩展到PB级别的数据和上千台机器，具备广泛应用性、可扩展性、高性能和高可用性等特点</span><br><span class="line">•谷歌的许多项目都存储在BigTable中，包括搜索、地图、财经、打印、社交网站Orkut、视频共享网站YouTube和博客网站Blogger等</span><br></pre></td></tr></table></figure><h3 id="HBase简介"><a href="#HBase简介" class="headerlink" title="HBase简介"></a>HBase简介</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HBase是一个高可靠、高性能、面向列、可伸缩的分布式数据库，是谷歌BigTable的开源实现，主要用来存储非结构化和半结构化的松散数据。HBase的目标是处理非常庞大的表，可以通过水平扩展的方式，利用廉价计算机集群处理由超过10亿行数据和数百万列元素组成的数据表</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302281329752.png" alt="image-20230228132917704" style="zoom:80%;"><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302281332502.png" alt="image-20230228133233461" style="zoom:80%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">关系数据库已经流行很多年，并且Hadoop已经有了HDFS和MapReduce，为什么需要HBase?</span><br><span class="line">•Hadoop可以很好地解决大规模数据的离线批量处理问题，但是，受限于Hadoop MapReduce编程框架的高延迟数据处理机制，使得Hadoop无法满足大规模数据实时处理应用的需求</span><br><span class="line">•HDFS面向批量访问模式，不是随机访问模式</span><br><span class="line">•传统的通用关系型数据库无法应对在数据规模剧增时导致的系统扩展性和性能问题（分库分表也不能很好解决）</span><br><span class="line">•传统关系数据库在数据结构变化时一般需要停机维护；空列浪费存储空间</span><br><span class="line">•因此，业界出现了一类面向半结构化数据存储和处理的高可扩展、低写入&#x2F;查询延迟的系统，例如，键值数据库、文档数据库和列族数据库（如BigTable和HBase等）</span><br><span class="line">•HBase已经成功应用于互联网服务领域和传统行业的众多在线式数据分析处理系统中</span><br></pre></td></tr></table></figure><h3 id="HBase与传统关系数据库的对比分析"><a href="#HBase与传统关系数据库的对比分析" class="headerlink" title="HBase与传统关系数据库的对比分析"></a>HBase与传统关系数据库的对比分析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">• HBase与传统的关系数据库的区别主要体现在以下几个方面：</span><br><span class="line">• （1）数据类型：关系数据库采用关系模型，具有丰富的数据类型和存储方式，HBase则采用了更加简单的数据模型，它把数据存储为未经解释的字符串</span><br><span class="line">• （2）数据操作：关系数据库中包含了丰富的操作，其中会涉及复杂的多表连接。HBase操作则不存在复杂的表与表之间的关系，只有简单的插入、查询、删除、清空等，因为HBase在设计上就避免了复杂的表和表之间的关系</span><br><span class="line">• （3）存储模式：关系数据库是基于行模式存储的。HBase是基于列存储的，每个列族都由几个文件保存，不同列族的文件是分离的</span><br><span class="line">• （4）数据索引：关系数据库通常可以针对不同列构建复杂的多个索</span><br><span class="line">引，以提高数据访问性能。HBase只有一个索引——行键，通过巧妙</span><br><span class="line">的设计，HBase中的所有访问方法，或者通过行键访问，或者通过行</span><br><span class="line">键扫描，从而使得整个系统不会慢下来</span><br><span class="line">• （5）数据维护：在关系数据库中，更新操作会用最新的当前值去替</span><br><span class="line">换记录中原来的旧值，旧值被覆盖后就不会存在。而在HBase中执行</span><br><span class="line">更新操作时，并不会删除数据旧的版本，而是生成一个新的版本，旧</span><br><span class="line">有的版本仍然保留</span><br><span class="line">• （6）可伸缩性：关系数据库很难实现横向扩展，纵向扩展的空间也</span><br><span class="line">比较有限。相反，HBase和BigTable这些分布式数据库就是为了实现</span><br><span class="line">灵活的水平扩展而开发的，能够轻易地通过在集群中增加或者减少硬</span><br><span class="line">件数量来实现性能的伸缩</span><br></pre></td></tr></table></figure><h2 id="HBase访问接口"><a href="#HBase访问接口" class="headerlink" title="HBase访问接口"></a>HBase访问接口</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302281418222.png" alt="image-20230228141802166"></p><h2 id="HBase数据模型"><a href="#HBase数据模型" class="headerlink" title="HBase数据模型"></a>HBase数据模型</h2><h3 id="数据模型概述"><a href="#数据模型概述" class="headerlink" title="数据模型概述"></a>数据模型概述</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">• HBase是一个稀疏、多维度、排序的映射表，这张表的索引是行键、列族、列限定符和时间戳</span><br><span class="line">• 每个值是一个未经解释的字符串，没有数据类型</span><br><span class="line">• 用户在表中存储数据，每一行都有一个可排序的行键和任意多的列</span><br><span class="line">• 表在水平方向由一个或者多个列族组成，一个列族中可以包含任意多个列，同一个列族里面的数据存储在一起</span><br><span class="line">• 列族支持动态扩展，可以很轻松地添加一个列族或列，无需预先定义列的数量以及类型，所有列均以字符串形式存储，用户需要自行进行数据类型转换</span><br><span class="line">• HBase中执行更新操作时，并不会删除数据旧的版本，而是生成一个新的版本，旧有的版本仍然保留（这是和HDFS只允许追加不允许修改的特性相关的）</span><br></pre></td></tr></table></figure><h3 id="数据模型相关概念"><a href="#数据模型相关概念" class="headerlink" title="数据模型相关概念"></a>数据模型相关概念</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">• 表：HBase采用表来组织数据，表由行和列组成，列划分为若干个列族</span><br><span class="line">• 行：每个HBase表都由若干行组成，每个行由行键（row key）来标识。</span><br><span class="line">• 列族：一个HBase表被分组成许多“列族”（Column Family）的集合，它是</span><br><span class="line">基本的访问控制单元</span><br><span class="line">• 列限定符：列族里的数据通过列限定符（或列）来定位</span><br><span class="line">• 单元格：在HBase表中，通过行、列族和列限定符确定一个“单元格”（cell），单元格中存储的数据没有数据类型，总被视为字节数组byte[]</span><br><span class="line">• 时间戳：每个单元格都保存着同一份数据的多个版本，这些版本采用时间戳进行索引</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302281430826.png" alt="image-20230228143056780" style="zoom:80%;"><h3 id="数据坐标"><a href="#数据坐标" class="headerlink" title="数据坐标"></a>数据坐标</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">• HBase中需要根据行键、列族、列限定符和时间戳来确定一个单元格，因此</span><br><span class="line">，可以视为一个“四维坐标”，即[行键, 列族, 列限定符, 时间戳]</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302281433903.png" alt="image-20230228143358844" style="zoom:80%;"><h3 id="概念视图"><a href="#概念视图" class="headerlink" title="概念视图"></a>概念视图</h3><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302281434121.png" alt="image-20230228143426066" style="zoom:80%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这里能看出hbase是稀疏表</span><br><span class="line">但实际物理储存不是这样的</span><br></pre></td></tr></table></figure><h3 id="物理视图"><a href="#物理视图" class="headerlink" title="物理视图"></a>物理视图</h3><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302281436903.png" alt="image-20230228143617852" style="zoom:80%;"><h3 id="面向列的存储"><a href="#面向列的存储" class="headerlink" title="面向列的存储"></a>面向列的存储</h3><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93HBase.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230228145808169.png" alt="image-20230228145808169" style="zoom:80%;"><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302281500698.png" alt="image-20230228150021647"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">列式存储适用于分析，如只分析年龄、性别</span><br><span class="line">还有一个好处是同一列数据类型相同，好压缩</span><br></pre></td></tr></table></figure><h2 id="HBase的实现原理"><a href="#HBase的实现原理" class="headerlink" title="HBase的实现原理"></a>HBase的实现原理</h2><h3 id="HBase功能组件"><a href="#HBase功能组件" class="headerlink" title="HBase功能组件"></a>HBase功能组件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">HBase的实现包括三个主要的功能组件：</span><br><span class="line">– （1）库函数：链接到每个客户端</span><br><span class="line">– （2）一个Master主服务器</span><br><span class="line">– （3）许多个Region服务器</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">• 主服务器Master负责管理和维护HBase表的分区信息，维护Region服务器列表，分配Region，负载均衡</span><br><span class="line">• Region服务器负责存储和维护分配给自己的Region，处理来自客户端的读写请求</span><br><span class="line">• 客户端并不是直接从Master主服务器上读取数据，而是在获得Region的存储位置信息后，直接从Region服务器上读取数据</span><br><span class="line">• 客户端并不依赖Master，而是通过Zookeeper来获得Region位置信息，大多数客户端甚至从来不和Master通信，这种设计方式使得Master负载很小</span><br></pre></td></tr></table></figure><h3 id="表和Region"><a href="#表和Region" class="headerlink" title="表和Region"></a>表和Region</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">•开始只有一个Region，后来不断分裂</span><br><span class="line">•Region拆分操作非常快，接近瞬间，因为拆分之后的Region读取的仍然是原存储文件，直到“合并”过程把存储文件异步地写到独立的文件之后，才会读取新文件</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302281518866.png" alt="image-20230228151807823" style="zoom:80%;"><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302281518309.png" alt="image-20230228151836266" style="zoom:80%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">•每个Region默认大小是100MB到200MB（2006年以前的硬件配置）</span><br><span class="line">•每个Region的最佳大小取决于单台服务器的有效处理能力</span><br><span class="line">•目前每个Region最佳大小建议1GB-2GB（2013年以后的硬件配置）</span><br><span class="line">•同一个Region不会被分拆到多个Region服务器</span><br><span class="line">•每个Region服务器存储10-1000个Region</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302281522002.png" alt="image-20230228152226946"></p><h3 id="Region的定位"><a href="#Region的定位" class="headerlink" title="Region的定位"></a>Region的定位</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">•元数据表，又名.META.表，存储了Region和Region服务器的映射关系</span><br><span class="line">•当HBase表很大时， .META.表也会被分裂成多个Region</span><br><span class="line">•根数据表，又名-ROOT-表，记录所有元数据的具体位置</span><br><span class="line">•-ROOT-表只有唯一一个Region，名字是在程序中被写死的</span><br><span class="line">•Zookeeper文件记录了-ROOT-表的位置</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302281525790.png" alt="image-20230228152512735"></p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302281536901.png" alt="image-20230228153623839" style="zoom:80%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">•为了加快访问速度，.META.表的全部Region都会被保存在内存中</span><br><span class="line">•假设.META.表的每行（一个映射条目）在内存中大约占用1KB，并且每个Region限制为128MB，那么，上面的三层结构可以保存的用户数据表的Region数目的计算方法是：</span><br><span class="line">•（-ROOT-表能够寻址的.META.表的Region个数）×（每个.META.表的Region可以寻址的用户数据表的Region个数）</span><br><span class="line">•一个-ROOT-表最多只能有一个Region，也就是最多只能有128MB，按照每行（一个映射条目）占用1KB内存计算，128MB空间可以容纳128MB&#x2F;1KB&#x3D;217行，也就是说，一个-ROOT-表可以寻址217个.META.表的Region。</span><br><span class="line">•同理，每个.META.表的 Region可以寻址的用户数据表的Region个数是128MB&#x2F;1KB&#x3D;217。</span><br><span class="line">•最终，三层结构可以保存的Region数目是(128MB&#x2F;1KB) × (128MB&#x2F;1KB) &#x3D; 234个Region</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302281537564.png" alt="image-20230228153722526" style="zoom:80%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">客户端访问数据时的“三级寻址”</span><br><span class="line">•为了加速寻址，客户端会缓存位置信息，同时，需要解决缓存失效问题</span><br><span class="line">•寻址过程客户端只需要询问Zookeeper服务器，不需要连接Master服务器</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302281537467.png" alt="image-20230228153756421" style="zoom:80%;"><h2 id="HBase运行机制"><a href="#HBase运行机制" class="headerlink" title="HBase运行机制"></a>HBase运行机制</h2><h3 id="HBase系统架构"><a href="#HBase系统架构" class="headerlink" title="HBase系统架构"></a>HBase系统架构</h3><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302281557920.png" alt="image-20230228155722859" style="zoom:80%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. 客户端</span><br><span class="line">客户端包含访问HBase的接口，同时在缓存中维护着已经访问过的Region位置信息，用来加快后续数据访问过程</span><br><span class="line">2. Zookeeper服务器</span><br><span class="line">Zookeeper可以帮助选举出一个Master作为集群的总管，并保证在任何时刻总有唯一一个Master在运行，这就避免了Master的“单点失效”问题</span><br><span class="line"></span><br><span class="line">Zookeeper是一个很好的集群管理工具，被大量用于分布式计算，提供配置维护、域名服务、分布式同步、组服务等。</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302281558723.png" alt="image-20230228155838651" style="zoom:80%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">3. Master</span><br><span class="line">主服务器Master主要负责表和Region的管理工作：</span><br><span class="line">管理用户对表的增加、删除、修改、查询等操作</span><br><span class="line">实现不同Region服务器之间的负载均衡</span><br><span class="line">在Region分裂或合并后，负责重新调整Region的分布</span><br><span class="line">对发生故障失效的Region服务器上的Region进行迁移</span><br><span class="line">4. Region服务器</span><br><span class="line">Region服务器是HBase中最核心的模块，负责维护分配给自己的Region，并响应用户的读写请求</span><br></pre></td></tr></table></figure><h3 id="Region服务器工作原理"><a href="#Region服务器工作原理" class="headerlink" title="Region服务器工作原理"></a>Region服务器工作原理</h3><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302281607273.png" alt="image-20230228160741212" style="zoom:80%;"><h4 id="用户读写数据过程"><a href="#用户读写数据过程" class="headerlink" title="用户读写数据过程"></a>用户读写数据过程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•用户写入数据时，被分配到相应Region服务器去执行</span><br><span class="line">•用户数据首先被写入到MemStore和Hlog中</span><br><span class="line">•只有当操作写入Hlog之后，commit()调用才会将其返回给客户端</span><br><span class="line">•当用户读取数据时，Region服务器会首先访问MemStore缓存，如果找不到，再去磁盘上面的StoreFile中寻找</span><br></pre></td></tr></table></figure><h4 id="缓存的刷新"><a href="#缓存的刷新" class="headerlink" title="缓存的刷新"></a>缓存的刷新</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">•系统会周期性地把MemStore缓存里的内容刷写到磁盘的StoreFile文件中，清空缓存，并在Hlog里面写入一个标记</span><br><span class="line">•每次刷写都生成一个新的StoreFile文件，因此，每个Store包含多个StoreFile文件</span><br><span class="line">•每个Region服务器都有一个自己的HLog文件，每次启动都检查该文件，确认最近一次执行缓存刷新操作之后是否发生新的写入操作；如果发现更新，则先写入MemStore，再刷写到StoreFile，最后删除旧的Hlog文件，开始为用户提供服务</span><br></pre></td></tr></table></figure><h4 id="StoreFile-的合并"><a href="#StoreFile-的合并" class="headerlink" title="StoreFile  的合并"></a>StoreFile  的合并</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">•每次刷写都生成一个新的StoreFile，数量太多，影响查找速度</span><br><span class="line">•调用Store.compact()把多个合并成一个</span><br><span class="line">•合并操作比较耗费资源，只有数量达到一个阈值才启动合并</span><br></pre></td></tr></table></figure><h3 id="Store工作原理"><a href="#Store工作原理" class="headerlink" title="Store工作原理"></a>Store工作原理</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">•Store是Region服务器的核心</span><br><span class="line">•多个StoreFile合并成一个</span><br><span class="line">•单个StoreFile过大时，又触发分裂操作，1个父Region被分裂成两个子Region</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302281621195.png" alt="image-20230228162152144" style="zoom:80%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这就是上面region的分裂</span><br></pre></td></tr></table></figure><h3 id="HLog工作原理"><a href="#HLog工作原理" class="headerlink" title="HLog工作原理"></a>HLog工作原理</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">• 分布式环境必须要考虑系统出错。HBase采用HLog保证系统恢复</span><br><span class="line">• HBase系统为每个Region服务器配置了一个HLog文件，它是一种预写式日志（Write Ahead Log）</span><br><span class="line">• 用户更新数据必须首先写入日志后，才能写入MemStore缓存，并且，直到MemStore缓存内容对应的日志已经写入磁盘，该缓存内容才能被刷写到磁盘</span><br><span class="line">• Zookeeper会实时监测每个Region服务器的状态，当某个Region服务器发生故障时，Zookeeper会通知Master</span><br><span class="line">• Master首先会处理该故障Region服务器上面遗留的HLog文件，这个遗留的HLog文件中包含了来自多个Region对象的日志记录</span><br><span class="line">• 系统会根据每条日志记录所属的Region对象对HLog数据进行拆分，分别放到相应Region对象的目录下，然后，再将失效的Region重新分配到可用的Region服务器中，并把与该Region对象相关的HLog日志记录也发送给相应的Region服务器</span><br><span class="line">• Region服务器领取到分配给自己的Region对象以及与之相关的HLog日志记录以后，会重新做一遍日志记录中的各种操作，把日志记录中的数据写入到MemStore缓存中，然后，刷新到磁盘的StoreFile文件中，完成数据恢复</span><br><span class="line">• 共用日志优点：提高对表的写操作性能；缺点：恢复时需要分拆日志</span><br></pre></td></tr></table></figure><h2 id="HBase应用方案"><a href="#HBase应用方案" class="headerlink" title="HBase应用方案"></a>HBase应用方案</h2><h3 id="HBase实际应用中的性能优化方法"><a href="#HBase实际应用中的性能优化方法" class="headerlink" title="HBase实际应用中的性能优化方法"></a>HBase实际应用中的性能优化方法</h3><h4 id="行键"><a href="#行键" class="headerlink" title="行键"></a>行键</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">行键是按照字典序存储，因此，设计行键时，要充分利用这个排序特点，将经常一起读取的数据存储到一块，将最近可能会被访问的数据放在一块。</span><br><span class="line"></span><br><span class="line">举个例子：如果最近写入HBase表中的数据是最可能被访问的，可以考虑将时间戳作为行键的一部分，由于是字典序排序，所以可以使用Long.MAX_VALUE - timestamp作为行键，这样能保证新写入的数据在读取时可以被快速命中。</span><br></pre></td></tr></table></figure><h4 id="InMemory"><a href="#InMemory" class="headerlink" title="InMemory"></a>InMemory</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">创建表的时候，可以通过HColumnDescriptor.setInMemory(true)将表放到</span><br><span class="line">Region服务器的缓存中，保证在读取的时候被cache命中。</span><br></pre></td></tr></table></figure><h4 id="Max-Version"><a href="#Max-Version" class="headerlink" title="Max Version"></a>Max Version</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">创建表的时候，可以通过HColumnDescriptor.setMaxVersions(int maxVersions)设置表中数据的最大版本，如果只需要保存最新版本的数据，那么可以设置setMaxVersions(1)。</span><br></pre></td></tr></table></figure><h4 id="Time-To-Live"><a href="#Time-To-Live" class="headerlink" title="Time To Live"></a>Time To Live</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•Time To Live</span><br><span class="line">创建表的时候，可以通过HColumnDescriptor.setTimeToLive(int timeToLive)</span><br><span class="line">设置表中数据的存储生命期，过期数据将自动被删除，例如如果只需要存储</span><br><span class="line">最近两天的数据，那么可以设置setTimeToLive(2 * 24 * 60 * 60)。</span><br></pre></td></tr></table></figure><h3 id="HBase性能监视"><a href="#HBase性能监视" class="headerlink" title="HBase性能监视"></a>HBase性能监视</h3><h4 id="Master-status-自带"><a href="#Master-status-自带" class="headerlink" title="Master-status(自带)"></a>Master-status(自带)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">•HBase Master默认基于Web的UI</span><br><span class="line">服务端口为60010，HBase region</span><br><span class="line">服务器默认基于Web的UI服务端</span><br><span class="line">口为60030.如果master运行在名</span><br><span class="line">为master.foo.com的主机中，</span><br><span class="line">mater的主页地址就是</span><br><span class="line">http:&#x2F;&#x2F;master.foo.com:60010，用</span><br><span class="line">户可以通过Web浏览器输入这个</span><br><span class="line">地址查看该页面</span><br><span class="line">•可以查看HBase集群的当前状态</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302281820158.png" alt="image-20230228182008076" style="zoom:80%;"><h4 id="Ganglia"><a href="#Ganglia" class="headerlink" title="Ganglia"></a>Ganglia</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Ganglia是UC Berkeley发起的一个开源集群监视项目，用于监控系统性能</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302281820966.png" alt="image-20230228182042858" style="zoom:80%;"><h4 id="OpenTSDB"><a href="#OpenTSDB" class="headerlink" title="OpenTSDB"></a>OpenTSDB</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">OpenTSDB可以从大规模的集群（包括集群中的网络设备、操作系统、应用程序）</span><br><span class="line">中获取相应的metrics并进行存储、索引以及服务，从而使得这些数据更容易让人</span><br><span class="line">理解，如web化，图形化等</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302281821565.png" alt="image-20230228182110452" style="zoom:80%;"><h4 id="Ambari"><a href="#Ambari" class="headerlink" title="Ambari"></a>Ambari</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Ambari 的作用就是创建、管理、监视 Hadoop 的集群</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302281824040.png" alt="image-20230228182401952" style="zoom:80%;"><h3 id="在HBase之上构建SQL引擎"><a href="#在HBase之上构建SQL引擎" class="headerlink" title="在HBase之上构建SQL引擎"></a>在HBase之上构建SQL引擎</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">NoSQL区别于关系型数据库的一点就是NoSQL不使用SQL作为查询语言，</span><br><span class="line">至于为何在NoSQL数据存储HBase上提供SQL接口，有如下原因：</span><br><span class="line"> 1.易使用。使用诸如SQL这样易于理解的语言，使人们能够更加轻松地使用HBase。</span><br><span class="line"> 2.减少编码。使用诸如SQL这样更高层次的语言来编写，减少了编写的代码量。 </span><br><span class="line"></span><br><span class="line">方案：</span><br><span class="line">1.Hive整合HBase</span><br><span class="line">2.Phoenix</span><br></pre></td></tr></table></figure><h4 id="Hive整合HBase"><a href="#Hive整合HBase" class="headerlink" title="Hive整合HBase"></a>Hive整合HBase</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hive与HBase的整合功能从Hive0.6.0版本已经开始出现，利用两者对外的API接口互相通信，通信主要依靠hive_hbase-handler.jar工具包(Hive Storage Handlers)。由于HBase有一次比较大的版本变动，所以并不是每个版本的Hive都能和现有的HBase版本进行整合，所以在使用过程中特别注意的就是两者版本的一致性。</span><br></pre></td></tr></table></figure><h4 id="Phoenix"><a href="#Phoenix" class="headerlink" title="Phoenix"></a>Phoenix</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Phoenix由Salesforce.com开源，是构建在Apache HBase之上的一个SQL中间层，可以让开发者在HBase上执行SQL查询。</span><br></pre></td></tr></table></figure><h3 id="构建HBase二级索引"><a href="#构建HBase二级索引" class="headerlink" title="构建HBase二级索引"></a>构建HBase二级索引</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">HBase只有一个针对行健的索引</span><br><span class="line">访问HBase表中的行，只有三种方式：</span><br><span class="line">    •通过单个行健访问</span><br><span class="line">    •通过一个行健的区间来访问</span><br><span class="line">    •全表扫描</span><br><span class="line"></span><br><span class="line">使用其他产品为HBase行健提供索引功能：</span><br><span class="line">•Hindex二级索引</span><br><span class="line">•HBase+Redis</span><br><span class="line">•HBase+solr</span><br><span class="line"></span><br><span class="line">原理：采用HBase0.92版本之后引入的Coprocessor特性</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">•Coprocessor构建二级索引</span><br><span class="line">•Coprocessor提供了两个实现：endpoint和observer，endpoint相当于关系型数</span><br><span class="line">据库的存储过程，而observer则相当于触发器</span><br><span class="line">•observer允许我们在记录put前后做一些处理，因此，而我们可以在插入数据时</span><br><span class="line">同步写入索引表</span><br><span class="line"></span><br><span class="line">优点：</span><br><span class="line">非侵入性：引擎构建在HBase之上，既没有对HBase进行任何改动，也不需要上层应用做任何妥协</span><br><span class="line"></span><br><span class="line">•缺点：每插入一条数据需要向索引表插入数据，即耗时是双倍的，对HBase的集群的压力也是双倍的</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302282029835.png" alt="image-20230228202924759" style="zoom:80%;"><h4 id="Hindex二级索引"><a href="#Hindex二级索引" class="headerlink" title="Hindex二级索引"></a>Hindex二级索引</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Hindex 是华为公司开发的纯 Java 编写的HBase二级索引，兼容 Apache HBase 0.94.8。当前的特性如下：</span><br><span class="line">•多个表索引</span><br><span class="line">•多个列索引</span><br><span class="line">•基于部分列值的索引</span><br></pre></td></tr></table></figure><h4 id="HBase-Redis"><a href="#HBase-Redis" class="headerlink" title="HBase+Redis"></a>HBase+Redis</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•Redis+HBase方案</span><br><span class="line">•Coprocessor构建二级索引</span><br><span class="line">•Redis做客户端缓存</span><br><span class="line">•将索引实时更新到Redis等KV系统中，定时从KV更新索引到HBase的索引表中</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302282031644.png" alt="image-20230228203150589" style="zoom:80%;"><h4 id="Solr-HBase"><a href="#Solr-HBase" class="headerlink" title="Solr+HBase"></a>Solr+HBase</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Solr是一个高性能，采用Java5开发，基于Lucene的全文搜索服务器。同时对其</span><br><span class="line">进行了扩展，提供了比Lucene更为丰富的查询语言，同时实现了可配置、可扩展</span><br><span class="line">并对查询性能进行了优化，并且提供了一个完善的功能管理界面，是一款非常优</span><br><span class="line">秀的全文搜索引擎。</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302282032278.png" alt="image-20230228203218220" style="zoom:80%;"><h2 id="HBase编程实践"><a href="#HBase编程实践" class="headerlink" title="HBase编程实践"></a>HBase编程实践</h2><h3 id="HBase的安装与配置"><a href="#HBase的安装与配置" class="headerlink" title="HBase的安装与配置"></a>HBase的安装与配置</h3><h4 id="HBase安装"><a href="#HBase安装" class="headerlink" title="HBase安装"></a>HBase安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">•下载安装包hbase-1.1.2-bin.tar.gz</span><br><span class="line">•解压安装包hbase-1.1.2-bin.tar.gz至路径 &#x2F;usr&#x2F;local</span><br><span class="line">•配置系统环境,将hbase下的bin目录添加到系统的path中</span><br><span class="line"></span><br><span class="line">备注：第2章安装完Hadoop时，只包含HDFS和MapReduce等核心组件，</span><br><span class="line">并不包含HBase，因此，HBase需要单独安装</span><br></pre></td></tr></table></figure><h4 id="HBase配置"><a href="#HBase配置" class="headerlink" title="HBase配置"></a>HBase配置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">HBase有三种运行模式，单机模式、伪分布式模式、分布式模式。</span><br><span class="line">以下先决条件很重要，比如没有配置JAVA_HOME环境变量，就会报错。</span><br><span class="line">– JDK</span><br><span class="line">– Hadoop( 单机模式不需要，伪分布式模式和分布式模式需要)</span><br><span class="line">– SSH</span><br><span class="line"></span><br><span class="line">启动关闭Hadoop和HBase的顺序一定是：</span><br><span class="line">启动Hadoop—&gt;启动HBase—&gt;关闭HBase—&gt;关闭Hadoop</span><br><span class="line">HBASE_MANAGES_ZK&#x3D;true，则由HBase自己管理Zookeeper</span><br><span class="line">否则，启动独立的Zookeeper</span><br><span class="line">建议：单机版HBase，使用自带Zookeeper；集群安装HBase则采用单独Zookeeper集群</span><br></pre></td></tr></table></figure><h3 id="HBase常用Shell命令"><a href="#HBase常用Shell命令" class="headerlink" title="HBase常用Shell命令"></a>HBase常用Shell命令</h3><h4 id="create、list"><a href="#create、list" class="headerlink" title="create、list"></a>create、list</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">•create：创建表</span><br><span class="line">•list：列出HBase中所有的表信息</span><br><span class="line">例子1：创建一个表，该表名称为tempTable，包含3个列族f1，f2和f3</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302282057560.png" alt="image-20230228205731501" style="zoom:80%;"><h4 id="put、scan"><a href="#put、scan" class="headerlink" title="put、scan"></a>put、scan</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">put：向表、行、列指定的单元格添加数据</span><br><span class="line"> 一次只能为一个表的一行数据的一个列添加一个数据</span><br><span class="line"> 在添加数据时，HBase会自动为添加的数据添加一个时间戳，当然，也可以在添加数据时人工指定时间戳的值</span><br><span class="line">scan：浏览表的相关信息</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">例子2：继续向表tempTable中的第r1行、第“f1:c1”列，添加数据值为“hello,dblab”</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302282059055.png" alt="image-20230228205940994" style="zoom:80%;"><h4 id="get"><a href="#get" class="headerlink" title="get"></a>get</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get：通过表名、行、列、时间戳、时间范围和版本号来获得相应单元格的值</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">例子3：</span><br><span class="line">（1）从tempTable中，获取第r1行、第“f1:c1”列的值</span><br><span class="line">（2）从tempTable中，获取第r1行、第“f1:c3”列的值</span><br><span class="line">备注：f1是列族，c1和c3都是列</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302282113102.png" alt="image-20230228211300041" style="zoom:80%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">从运行结果可以看出： tempTable中第r1行、第“f1:c3”列的值当前不存在</span><br></pre></td></tr></table></figure><h4 id="enable-disable：使表有效或无效"><a href="#enable-disable：使表有效或无效" class="headerlink" title="enable/disable：使表有效或无效"></a>enable/disable：使表有效或无效</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">删除之前，要使之无效</span><br></pre></td></tr></table></figure><h4 id="drop：删除表"><a href="#drop：删除表" class="headerlink" title="drop：删除表"></a>drop：删除表</h4><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302282133190.png" alt="image-20230228213350133" style="zoom:80%;"><h3 id="HBase常用Java-API及应用实例"><a href="#HBase常用Java-API及应用实例" class="headerlink" title="HBase常用Java API及应用实例"></a>HBase常用Java API及应用实例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">HBase是Java编写的，它的原生的API也是Java开发的，不过，可以使用Java或其他语言调用API来访问HBase</span><br><span class="line"></span><br><span class="line">首先要在工程中导入一下jar包：</span><br><span class="line">这里只需要导入hbase安装目录中的lib文件中的所有jar包，此处不用再导入</span><br><span class="line">第三章Hadoop中的jar包，避免由于Hadoop和HBase的版本冲突引起错误。</span><br><span class="line"></span><br><span class="line">备注：使用的HBase版本号为1.1.2</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">任务要求：创建表、插入数据、浏览数据</span><br><span class="line">创建一个学生信息表，用来存储学生姓名（姓名作为行键，并且假设姓名不会重复）以及考试成绩，其中，考试成绩是一个列族，分别存储了各个科目的考试成绩。逻辑视图如表4-18所示。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302282139347.png" alt="image-20230228213955290"></p><h4 id="每次都建立和关闭连接"><a href="#每次都建立和关闭连接" class="headerlink" title="每次都建立和关闭连接"></a>每次都建立和关闭连接</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Chapter4</span></span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Configuration configuration;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Connection connection;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Admin admin;</span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span><span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">         createTable(“student”,<span class="keyword">new</span> String[]&#123;“score”&#125;);</span><br><span class="line">         insertData(“student”,“zhangsan”,“score”,“English”,“<span class="number">69</span>”);</span><br><span class="line">         insertData(“student”,“zhangsan”,“score”,“Math”,“<span class="number">86</span>”);</span><br><span class="line">        insertData(“student”,“zhangsan”,“score”,“Computer”,“<span class="number">77</span>”);</span><br><span class="line">         getData(“student”, “zhangsan”, “score”, “English”);</span><br><span class="line"> &#125;</span><br><span class="line">……</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span></span>&#123;……&#125;<span class="comment">//建立连接</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>&#123;……&#125;<span class="comment">//关闭连接</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">createTable</span><span class="params">()</span></span>&#123;……&#125;<span class="comment">//创建表</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">insertData</span><span class="params">()</span> </span>&#123;……&#125;<span class="comment">//插入数据</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> getData&#123;……&#125;<span class="comment">//浏览数据</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="所有操作只建立和关闭一次连接"><a href="#所有操作只建立和关闭一次连接" class="headerlink" title="所有操作只建立和关闭一次连接"></a>所有操作只建立和关闭一次连接</h4><h5 id="建立连接"><a href="#建立连接" class="headerlink" title="建立连接"></a>建立连接</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//建立连接</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span></span>&#123;</span><br><span class="line"> configuration = HBaseConfiguration.create();      configuration.set(<span class="string">"hbase.rootdir"</span>,<span class="string">"hdfs://localhost:9000/hbase"</span>);</span><br><span class="line"> <span class="keyword">try</span>&#123;</span><br><span class="line">     connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">     admin = connection.getAdmin();</span><br><span class="line"> &#125;<span class="keyword">catch</span> (IOException e)&#123;</span><br><span class="line"> e.printStackTrace();</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302282200531.png" alt="image-20230228220028478" style="zoom:80%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">拷贝这个文件到工程目录也行，就不用写配置路径信息</span><br></pre></td></tr></table></figure><h5 id="关闭连接"><a href="#关闭连接" class="headerlink" title="关闭连接"></a>关闭连接</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//关闭连接</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>&#123;</span><br><span class="line"> <span class="keyword">try</span>&#123;</span><br><span class="line"> <span class="keyword">if</span>(admin != <span class="keyword">null</span>)&#123;</span><br><span class="line"> admin.close();</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">if</span>(<span class="keyword">null</span> != connection)&#123;</span><br><span class="line"> connection.close();</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;<span class="keyword">catch</span> (IOException e)&#123;</span><br><span class="line"> e.printStackTrace();</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h5 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">①创建表</span><br><span class="line">创建一个学生信息表，用来存储学生姓名（姓名作为行键，并且</span><br><span class="line">假设姓名不会重复）以及考试成绩，其中，考试成绩是一个列族，</span><br><span class="line">分别存储了各个科目的考试成绩。</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*创建表*/</span></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> myTableName 表名</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> colFamily列族数组</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">createTable</span><span class="params">(String myTableName,String[] colFamily)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">     TableName tableName = TableName.valueOf(myTableName);</span><br><span class="line">     <span class="keyword">if</span>(admin.tableExists(tableName))&#123;</span><br><span class="line"> System.out.println(<span class="string">"table exists!"</span>);</span><br><span class="line"> &#125;<span class="keyword">else</span> &#123;</span><br><span class="line"> HTableDescriptor hTableDescriptor = <span class="keyword">new</span> HTableDescriptor(tableName);</span><br><span class="line"> <span class="keyword">for</span>(String str: colFamily)&#123;</span><br><span class="line"> HColumnDescriptor hColumnDescriptor = <span class="keyword">new</span> HColumnDescriptor(str);</span><br><span class="line"> hTableDescriptor.addFamily(hColumnDescriptor);</span><br><span class="line"> &#125;</span><br><span class="line"> admin.createTable(hTableDescriptor);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302282216054.png" alt="image-20230228221650996" style="zoom:67%;"><h5 id="添加数据"><a href="#添加数据" class="headerlink" title="添加数据"></a>添加数据</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">②添加数据</span><br><span class="line">现在向表student中添加数据。</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*添加数据*/</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">* <span class="doctag">@param</span> rowKey 行键</span></span><br><span class="line"><span class="comment">* <span class="doctag">@param</span> colFamily 列族</span></span><br><span class="line"><span class="comment">* <span class="doctag">@param</span> col 列限定符</span></span><br><span class="line"><span class="comment">* <span class="doctag">@param</span> val 数据</span></span><br><span class="line"><span class="comment">* <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">insertData</span><span class="params">(String tableName, String rowKey, String colFamily, String col, String val)</span> <span class="keyword">throws</span> IOException </span>&#123; </span><br><span class="line"> Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line"> Put put = <span class="keyword">new</span> Put(Bytes.toBytes(rowkey));</span><br><span class="line"> put.addColumn(Bytes.toBytes(colFamily), Bytes.toBytes(col), Bytes.toBytes(val));</span><br><span class="line">     table.put(put);</span><br><span class="line">     table.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">添加数据时，需要分别设置参数myTableName、rowkey、colFamily、col、val的</span><br><span class="line">值，然后运行上述代码</span><br><span class="line">为insertData()方法指定相应参数，并运行如下3</span><br><span class="line">行代码：</span><br><span class="line">insertData(&quot;student&quot;,&quot;zhangsan&quot;,&quot;score&quot;,&quot;English&quot;,&quot;69&quot;);</span><br><span class="line">insertData(&quot;student&quot;,&quot;zhangsan&quot;,&quot;score&quot;,&quot;Math&quot;,&quot;86&quot;);</span><br><span class="line">insertData(&quot;student&quot;,&quot;zhangsan&quot;,&quot;score&quot;,&quot;Computer&quot;,&quot;77&quot;);</span><br><span class="line">上述代码与如下HBase Shell命令等效：</span><br><span class="line">put &#39;student&#39;,’zhangsan’,’score:English’,’69’；</span><br><span class="line">put &#39;student&#39;,’zhangsan’,’score:Math’,’86’；</span><br><span class="line">put &#39;student&#39;,’zhangsan’,’score:Computer’,’77’；</span><br></pre></td></tr></table></figure><h5 id="浏览数据"><a href="#浏览数据" class="headerlink" title="浏览数据"></a>浏览数据</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*获取某单元格数据*/</span></span><br><span class="line"><span class="comment">/** * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey 行键</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> colFamily 列族</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> col 列限定符</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException */</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getData</span><span class="params">(String tableName,String rowKey,String </span></span></span><br><span class="line"><span class="function"><span class="params">colFamily,String col)</span><span class="keyword">throws</span> IOException</span>&#123; </span><br><span class="line">     Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">     Get get = <span class="keyword">new</span> Get(Bytes.toBytes(rowkey));</span><br><span class="line">    get.addColumn(Bytes.toBytes(colFamily),Bytes.toBytes(col));</span><br><span class="line">     <span class="comment">//获取的result数据是结果集，还需要格式化输出想要的数据才行</span></span><br><span class="line">     Result result = table.get(get);</span><br><span class="line">     System.out.println(<span class="keyword">new</span> </span><br><span class="line">String(result.getValue(colFamily.getBytes(),col==<span class="keyword">null</span>?<span class="keyword">null</span>:col.getBytes())));</span><br><span class="line"> table.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">比如，现在要获取姓名为“zhangsan”在“English”上的数据，就可以在运行上述代码时，指定参数tableName为“student”、rowKey为“zhangsan”、colFamily为“score”、col为“English”。</span><br><span class="line"></span><br><span class="line">getData(&quot;student&quot;, &quot;zhangsan&quot;, &quot;score&quot;, &quot;English&quot;);</span><br><span class="line">上述代码与如下HBase Shell命令等效：</span><br><span class="line">get ‗student&#39;,&#39;zhangsan&#39;,&#123;COLUMN&#x3D;&gt;&#39;score:English&#39;&#125;‖</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第三章 分布式文件系统HDFS</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC%E4%B8%89%E7%AB%A0-%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9FHDFS.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC%E4%B8%89%E7%AB%A0-%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9FHDFS.html</id>
    <published>2023-02-25T08:58:58.000Z</published>
    <updated>2023-02-25T16:28:17.541Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第三章-分布式文件系统HDFS"><a href="#第三章-分布式文件系统HDFS" class="headerlink" title="第三章 分布式文件系统HDFS"></a>第三章 分布式文件系统HDFS</h1><h2 id="分布式文件系统"><a href="#分布式文件系统" class="headerlink" title="分布式文件系统"></a>分布式文件系统</h2><h3 id="计算机集群结构"><a href="#计算机集群结构" class="headerlink" title="计算机集群结构"></a>计算机集群结构</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">•分布式文件系统把文件分布存储到多个计算机节点上，成千上万的计算机节点构成计算机集群</span><br><span class="line">•与之前使用多个处理器和专用高级硬件的并行化处理装置不同的是，目前的分布式文件系统所采用的计算机集群，都是由普通硬件构成的，这就大大降低了硬件上的开销</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302251702664.png" alt="image-20230225170222620"></p><h3 id="分布式文件系统的结构"><a href="#分布式文件系统的结构" class="headerlink" title="分布式文件系统的结构"></a>分布式文件系统的结构</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">分布式文件系统在物理结构上是由计算机集群中的多个节点构成的，这些节点分为两类，一类叫“主节点”(Master Node)或者也被称为“名称结点”(NameNode)，另一类叫“从节点”（Slave Node）或者也被称为“数据节点”(DataNode)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302251703978.png" alt="image-20230225170351933"></p><h2 id="HDFS简介"><a href="#HDFS简介" class="headerlink" title="HDFS简介"></a>HDFS简介</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">总体而言，HDFS要实现以下目标：</span><br><span class="line">    ●兼容廉价的硬件设备</span><br><span class="line">    ●流数据读写</span><br><span class="line">    ●大数据集</span><br><span class="line">    ●简单的文件模型</span><br><span class="line">    ●强大的跨平台兼容性</span><br><span class="line">HDFS特殊的设计，在实现上述优良特性的同时，也使得自身具有一些应用</span><br><span class="line">局限性，主要包括以下几个方面：</span><br><span class="line">    ●不适合低延迟数据访问</span><br><span class="line">    ●无法高效存储大量小文件</span><br><span class="line">    ●不支持多用户写入及任意修改文件</span><br></pre></td></tr></table></figure><h2 id="HDFS相关概念"><a href="#HDFS相关概念" class="headerlink" title="HDFS相关概念"></a>HDFS相关概念</h2><h3 id="块"><a href="#块" class="headerlink" title="块"></a>块</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">HDFS默认一个块64MB，一个文件被分成多个块，以块作为存储单位块的大小远远大于普通文件系统，可以最小化寻址开销</span><br><span class="line">HDFS采用抽象的块概念可以带来以下几个明显的好处：</span><br><span class="line">● 支持大规模文件存储：文件以块为单位进行存储，一个大规模文件可以被分拆成若干个文件块，不同的文件块可以被分发到不同的节点上，因此，一个文件的大小不会受到单个节点的存储容量的限制，可以远远大于网络中任意节点的存储容量</span><br><span class="line">● 简化系统设计：首先，大大简化了存储管理，因为文件块大小是固定的，这样就可以很容易计算出一个节点可以存储多少文件块；其次，方便了元数据的管理，元数据不需要和文件块一起存储，可以由其他系统负责管理元数据</span><br><span class="line">● 适合数据备份：每个文件块都可以冗余存储到多个节点上，大大提高了系统的容错性和可用性</span><br></pre></td></tr></table></figure><h3 id="名称节点和数据节点"><a href="#名称节点和数据节点" class="headerlink" title="名称节点和数据节点"></a>名称节点和数据节点</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302251725266.png" alt="image-20230225172556184"></p><h4 id="名称节点的数据结构"><a href="#名称节点的数据结构" class="headerlink" title="名称节点的数据结构"></a>名称节点的数据结构</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•在HDFS中，名称节点（NameNode）负责管理分布式文件系统的命名空间（Namespace），保存了两个核心的数据结构，即FsImage和EditLog</span><br><span class="line">    •FsImage用于维护文件系统树以及文件树中所有的文件和文件夹的元数据</span><br><span class="line">    •操作日志文件EditLog中记录了所有针对文件的创建、删除、重命名等操作</span><br><span class="line">•名称节点记录了每个文件中各个块所在的数据节点的位置信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302251730749.png" alt="image-20230225173043700"></p><h4 id="FsImage文件"><a href="#FsImage文件" class="headerlink" title="FsImage文件"></a>FsImage文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">•FsImage文件包含文件系统中所有目录和文件inode的序列化形式。每个inode是一个文件或目录的元数据的内部表示，并包含此类信息：文件的复制等级、修改和访问时间、访问权限、块大小以及组成文件的块。对于目录，则存储修改时间、权限和配额元数据</span><br><span class="line">•FsImage文件没有记录块存储在哪个数据节点。而是由名称节点把这些映射保留在内存中，当数据节点加入HDFS集群时，数据节点会把自己所包含的块列表告知给名称节点，此后会定期执行这种告知操作，以确保名称节点的块映射是最新的。</span><br></pre></td></tr></table></figure><h4 id="名称节点的启动"><a href="#名称节点的启动" class="headerlink" title="名称节点的启动"></a>名称节点的启动</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">•在名称节点启动的时候，它会将FsImage文件中的内容加载到内存中，之后再执行EditLog文件中的各项操作，使得内存中的元数据和实际的同步，存在内存中的元数据支持客户端的读操作。</span><br><span class="line">•一旦在内存中成功建立文件系统元数据的映射，则创建一个新的FsImage文件和一个空的EditLog文件</span><br><span class="line">•名称节点起来之后，HDFS中的更新操作会重新写到EditLog文件中，因为</span><br><span class="line">FsImage文件一般都很大（GB级别的很常见），如果所有的更新操作都往</span><br><span class="line">FsImage文件中添加，这样会导致系统运行的十分缓慢，但是，如果往EditLog</span><br><span class="line">文件里面写就不会这样，因为EditLog 要小很多。每次执行写操作之后，且在向客户端发送成功代码之前，edits文件都需要同步更新</span><br></pre></td></tr></table></figure><h4 id="名称节点运行期间EditLog不断变大的问题"><a href="#名称节点运行期间EditLog不断变大的问题" class="headerlink" title="名称节点运行期间EditLog不断变大的问题"></a>名称节点运行期间EditLog不断变大的问题</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•在名称节点运行期间，HDFS的所有更新操作都是直接写到EditLog中，久而久之， EditLog文件将会变得很大</span><br><span class="line">•虽然这对名称节点运行时候是没有什么明显影响的，但是，当名称节点重启的时候，名称节点需要先将FsImage里面的所有内容映像到内存中，然后再一条一条地执行EditLog中的记录，当EditLog文件非常大的时候，会导致名称节点启动操作非常慢，而在这段时间内HDFS系统处于安全模式，一直无法对外提供写操作，影响了用户的使用</span><br><span class="line"></span><br><span class="line">名称节点运行期间EditLog不断变大的问题如何解决？答案是：    SecondaryNameNode第二名称节点</span><br></pre></td></tr></table></figure><h4 id="第二名称节点SecondaryNameNode"><a href="#第二名称节点SecondaryNameNode" class="headerlink" title="第二名称节点SecondaryNameNode"></a>第二名称节点SecondaryNameNode</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第二名称节点是HDFS架构中的一个组成部分，它是用来保存名称节点中对HDFS </span><br><span class="line">元数据信息的备份，并减少名称节点重启的时间。SecondaryNameNode一般是</span><br><span class="line">单独运行在一台机器上</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302251748961.png" alt="image-20230225174840912" style="zoom:80%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">SecondaryNameNode的工作情况：</span><br><span class="line">（1）SecondaryNameNode会定期和NameNode通信，请求其停止使用EditLog</span><br><span class="line">文件，暂时将新的写操作写到一个新的文件edit.new上来，这个操作是瞬间完成，上层写日志的函数完全感觉不到差别；</span><br><span class="line">（2）SecondaryNameNode通过HTTP GET方式从NameNode上获取到FsImage和EditLog文件，并下载到本地的相应目录下；</span><br><span class="line">（3）SecondaryNameNode将下载下来的FsImage载入到内存，然后一条一条地</span><br><span class="line">执行EditLog文件中的各项更新操作，使得内存中的FsImage保持最新；这个过程就是EditLog和FsImage文件合并；</span><br><span class="line">（4）SecondaryNameNode执行完（3）操作之后，会通过post方式将新的</span><br><span class="line">FsImage文件发送到NameNode节点上</span><br><span class="line">（5）NameNode将从SecondaryNameNode接收到的新的FsImage替换旧的FsImage文件，同时将edit.new替换EditLog文件，通过这个过程</span><br><span class="line">EditLog就变小了</span><br></pre></td></tr></table></figure><h4 id="数据节点DataNode"><a href="#数据节点DataNode" class="headerlink" title="数据节点DataNode"></a>数据节点DataNode</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">•数据节点是分布式文件系统HDFS的工作节点，负责数据的存储和读取，会根据客户端或者是名称节点的调度来进行数据的存储和检索，并且向名称节点定期发送自己所存储的块的列表</span><br><span class="line">•每个数据节点中的数据会被保存在各自节点的本地Linux文件系统中</span><br></pre></td></tr></table></figure><h2 id="HDFS体系结构"><a href="#HDFS体系结构" class="headerlink" title="HDFS体系结构"></a>HDFS体系结构</h2><h3 id="HDFS体系结构概述"><a href="#HDFS体系结构概述" class="headerlink" title="HDFS体系结构概述"></a>HDFS体系结构概述</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">HDFS采用了主从（Master&#x2F;Slave）结构模型，一个HDFS集群包括一个名称节点（NameNode）和若干个数据节点（DataNode）（如图3-4所示）。名称节点作为中心服务器，负责管理文件系统的命名空间及客户端对文件的访问。</span><br><span class="line"></span><br><span class="line">集群中的数据节点一般是一个节点运行一个数据节点进程，负责处理文件系统客户端的读&#x2F;写请求，在名称节点的统一调度下进行数据块的创建、删除和复制等操作。每个数据节点的数据实际上是保存在本地Linux文件系统中的</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC%E4%B8%89%E7%AB%A0-%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9FHDFS.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230225201234868.png" alt="image-20230225201234868" style="zoom:80%;"><h3 id="HDFS命名空间管理"><a href="#HDFS命名空间管理" class="headerlink" title="HDFS命名空间管理"></a>HDFS命名空间管理</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• HDFS的命名空间包含目录、文件和块</span><br><span class="line">• 在HDFS1.0体系结构中，在整个HDFS集群中只有一个命名空间，并且只有唯一一个名称节点，该节点负责对这个命名空间进行管理</span><br><span class="line">• HDFS使用的是传统的分级文件体系，因此，用户可以像使用普通文件系统一样，创建、删除目录和文件，在目录间转移文件，重命名文件等</span><br></pre></td></tr></table></figure><h3 id="通信协议"><a href="#通信协议" class="headerlink" title="通信协议"></a>通信协议</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">• HDFS是一个部署在集群上的分布式文件系统，因此，很多数据需要</span><br><span class="line">通过网络进行传输</span><br><span class="line">• 所有的HDFS通信协议都是构建在TCP&#x2F;IP协议基础之上的</span><br><span class="line">• 客户端通过一个可配置的端口向名称节点主动发起TCP连接，并使用客户端协议与名称节点进行交互</span><br><span class="line">• 名称节点和数据节点之间则使用数据节点协议进行交互</span><br><span class="line">• 客户端与数据节点的交互是通过RPC（Remote Procedure Call）来实现的。在设计上，名称节点不会主动发起RPC，而是响应来自客户端和数据节点的RPC请求</span><br></pre></td></tr></table></figure><h3 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">• 客户端是用户操作HDFS最常用的方式，HDFS在部署时都提供了客户端</span><br><span class="line">• HDFS客户端是一个库，暴露了HDFS文件系统接口，这些接口隐藏了HDFS实现中的大部分复杂性</span><br><span class="line">• 严格来说，客户端并不算是HDFS的一部分</span><br><span class="line">• 客户端可以支持打开、读取、写入等常见的操作，并且提供了类似Shell的命令行方式来访问HDFS中的数据</span><br><span class="line">• 此外，HDFS也提供了Java API，作为应用程序访问文件系统的客户端编程接口</span><br></pre></td></tr></table></figure><h3 id="HDFS体系结构的局限性"><a href="#HDFS体系结构的局限性" class="headerlink" title="HDFS体系结构的局限性"></a>HDFS体系结构的局限性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">HDFS只设置唯一一个名称节点，这样做虽然大大简化了系统设计，但也带来了一些明显的局限性，具体如下：</span><br><span class="line">    （1）命名空间的限制：名称节点是保存在内存中的，因此，名称节点能够容纳的对象（文件、块）的个数会受到内存空间大小的限制。</span><br><span class="line">    （2）性能的瓶颈：整个分布式文件系统的吞吐量，受限于单个名称节点的吞吐量。</span><br><span class="line">    （3）隔离问题：由于集群中只有一个名称节点，只有一个命名空间，因此，无法对不同应用程序进行隔离。</span><br><span class="line">    （4）集群的可用性：一旦这个唯一的名称节点发生故障，会导致整个集群变得不可用。</span><br></pre></td></tr></table></figure><h2 id="HDFS存储原理"><a href="#HDFS存储原理" class="headerlink" title="HDFS存储原理"></a>HDFS存储原理</h2><h3 id="冗余数据保存"><a href="#冗余数据保存" class="headerlink" title="冗余数据保存"></a>冗余数据保存</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">作为一个分布式文件系统，为了保证系统的容错性和可用性，HDFS采用了多副本方式对数据进行冗余存储，通常一个数据块的多个副本会被分布到不同的数据节点上，如图3-5所示，数据块1被分别存放到数据节点A和C上，数据块2被存放在数据节点A和B上。这种多副本方式具有以下几个优点：</span><br><span class="line">（1）加快数据传输速度</span><br><span class="line">（2）容易检查数据错误</span><br><span class="line">（3）保证数据可靠性</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC%E4%B8%89%E7%AB%A0-%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9FHDFS.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230225203213489.png" alt="image-20230225203213489" style="zoom:80%;"><h3 id="数据存取策略"><a href="#数据存取策略" class="headerlink" title="数据存取策略"></a>数据存取策略</h3><h4 id="数据存放"><a href="#数据存放" class="headerlink" title="数据存放"></a>数据存放</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•第一个副本：放置在上传文件的数据节点；如果是集群外提交，则随机挑选一台磁盘不太满、CPU不太忙的节点</span><br><span class="line">•第二个副本：放置在与第一个副本不同的机架的节点上</span><br><span class="line">•第三个副本：与第一个副本相同机架的其他节点上</span><br><span class="line">•更多副本：随机节点</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC%E4%B8%89%E7%AB%A0-%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9FHDFS.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230225203929514.png" alt="image-20230225203929514" style="zoom:80%;"><h4 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">•HDFS提供了一个API可以确定一个数据节点所属的机架ID，客户端也可以调用API获取自己所属的机架ID</span><br><span class="line">•当客户端读取数据时，从名称节点获得数据块不同副本的存放位置列表，列表中包含了副本所在的数据节点，可以调用API来确定客户端和这些数据节点所属的机架ID，当发现某个数据块副本对应的机架ID和客户端对应的机架ID相同时，就优先选择该副本读取数据，如果没有发现，就随机选择一个副本读取数据</span><br></pre></td></tr></table></figure><h3 id="数据错误与恢复"><a href="#数据错误与恢复" class="headerlink" title="数据错误与恢复"></a>数据错误与恢复</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HDFS具有较高的容错性，可以兼容廉价的硬件，它把硬件出错看作一种常态，而不是异常，并设计了相应的机制检测数据错误和进</span><br><span class="line">行自动恢复，主要包括以下几种情形：名称节点出错、数据节点出错和数据出错。</span><br></pre></td></tr></table></figure><h5 id="名称节点出错"><a href="#名称节点出错" class="headerlink" title="名称节点出错"></a>名称节点出错</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">名称节点保存了所有的元数据信息，其中，最核心的两大数据</span><br><span class="line">结构是FsImage和Editlog，如果这两个文件发生损坏，那么整个HDFS实例将失效。因此，HDFS设置了备份机制，把这些核心文件同步复制到备份服务器SecondaryNameNode上。当名称节点出错时，就可以根据备份服务器SecondaryNameNode中的FsImage和Editlog数据进行恢复。</span><br></pre></td></tr></table></figure><h5 id="数据节点出错"><a href="#数据节点出错" class="headerlink" title="数据节点出错"></a>数据节点出错</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">•每个数据节点会定期向名称节点发送“心跳”信息，向名称节点报告自己的状态</span><br><span class="line">•当数据节点发生故障，或者网络发生断网时，名称节点就无法收到来自一些数据节点的心跳信息，这时，这些数据节点就会被标记为“宕机”，节点上面的所有数据都会被标记为“不可读”，名称节点不会再给它们发送任何I&#x2F;O请求</span><br><span class="line">•这时，有可能出现一种情形，即由于一些数据节点的不可用，会导致一些数据块的副本数量小于冗余因子</span><br><span class="line">•名称节点会定期检查这种情况，一旦发现某个数据块的副本数量小于冗余因子，就会启动数据冗余复制，为它生成新的副本</span><br><span class="line">•HDFS和其它分布式文件系统的最大区别就是可以调整冗余数据的位置</span><br></pre></td></tr></table></figure><h5 id="数据出错"><a href="#数据出错" class="headerlink" title="数据出错"></a>数据出错</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•网络传输和磁盘错误等因素，都会造成数据错误</span><br><span class="line">•客户端在读取到数据后，会采用md5和sha1对数据块进行校验，以确定读取到正确的数据</span><br><span class="line">•在文件被创建时，客户端就会对每一个文件块进行信息摘录，并把这些信息写入到同一个路径的隐藏文件里面</span><br><span class="line">•当客户端读取文件的时候，会先读取该信息文件，然后，利用该信息文件对每个读取的数据块进行校验，如果校验出错，客户端就会请求到另外一个数据节点读取该文件块，并且向名称节点报告这个文件块有错误，名称节点会定期检查并且重新复制这个块</span><br></pre></td></tr></table></figure><h2 id="HDFS数据读写过程"><a href="#HDFS数据读写过程" class="headerlink" title="HDFS数据读写过程"></a>HDFS数据读写过程</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">是hdfs中比较核心的东西</span><br></pre></td></tr></table></figure><p><a href="https://www.iqiyi.com/w_19rt07086t.html" target="_blank" rel="external nofollow noopener noreferrer">课程视频</a></p><h3 id="读取文件"><a href="#读取文件" class="headerlink" title="读取文件"></a>读取文件</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302252223474.png" alt="image-20230225210019007"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Chapter3</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">    FileSystem fs = FileSystem.get(conf);</span><br><span class="line">    Path filename = <span class="keyword">new</span> Path(“hdfs:<span class="comment">//localhost:9000/user/hadoop/test.t</span></span><br><span class="line">xt<span class="string">");</span></span><br><span class="line"><span class="string">    FSDataInputStream is = fs.open(filename);</span></span><br><span class="line"><span class="string">    BufferedReader d = new BufferedReader(new InputStreamReader(is));</span></span><br><span class="line"><span class="string">    String content = d.readLine(); //读取文件一行</span></span><br><span class="line"><span class="string">    System.out.println(content);</span></span><br><span class="line"><span class="string">    d.close(); //关闭文件</span></span><br><span class="line"><span class="string">    fs.close(); //关闭hdfs</span></span><br><span class="line"><span class="string">    &#125; catch (Exception e) &#123;</span></span><br><span class="line"><span class="string">e.printStackTrace();</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><h3 id="写入文件"><a href="#写入文件" class="headerlink" title="写入文件"></a>写入文件</h3><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302252223211.png" alt="image-20230225210046285" style="zoom:80%;"><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Chapter3</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">            Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">            FileSystem fs = FileSystem.get(conf);</span><br><span class="line">            <span class="keyword">byte</span>[] buff = <span class="string">"Hello world"</span>.getBytes(); <span class="comment">// 要写入的内容</span></span><br><span class="line">            String filename = <span class="string">" hdfs://localhost:9000/user/hadoop/test.txt "</span>; <span class="comment">//要写入的文件名</span></span><br><span class="line">            FSDataOutputStream os = fs.create(<span class="keyword">new</span> Path(filename));</span><br><span class="line">            os.write(buff,<span class="number">0</span>,buff.length);</span><br><span class="line">            System.out.println(<span class="string">"Create:"</span>+ filename);</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•FileSystem是一个通用文件系统的抽象基类，可以被分布式文件系统继承，所有可能使用Hadoop文件系统的代码，都要使用这个类</span><br><span class="line">•Hadoop为FileSystem这个抽象类提供了多种具体实现</span><br><span class="line">•DistributedFileSystem就是FileSystem在HDFS文件系统中的具体实现</span><br><span class="line">•FileSystem的open()方法返回的是一个输入流FSDataInputStream对象，在HDFS文件系统中，具体的输入流就是DFSInputStream；FileSystem中的create()方法返回的是一个输出流FSDataOutputStream对象，在HDFS文件系统中，具体的输出流就是DFSOutputStream。</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(conf);</span><br><span class="line">FSDataInputStream in = fs.open(<span class="keyword">new</span> Path(uri));</span><br><span class="line">FSDataOutputStream out = fs.create(<span class="keyword">new</span> Path(uri));</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">备注：创建一个Configuration对象时，其构造方法会默认加载工程项目下两个配置文件，分别是hdfs-site.xml以及core-site.xml，这两个文件中会有访问HDFS所需的参数值，主要是fs.defaultFS，指定了HDFS的地址（比如hdfs:&#x2F;&#x2F;localhost:9000），有了这个地址客户端就可以通过这个地址访问HDFS了</span><br></pre></td></tr></table></figure><h2 id="HDFS编程实践"><a href="#HDFS编程实践" class="headerlink" title="HDFS编程实践"></a>HDFS编程实践</h2><p>[分布式文件系统HDFS 学习指南](<a href="https://dblab.xmu.edu.cn/blog/290/" target="_blank" rel="external nofollow noopener noreferrer">大数据技术原理与应用 第三章 分布式文件系统HDFS 学习指南_厦大数据库实验室博客 (xmu.edu.cn)</a>)</p><h3 id="HDFS常用命令"><a href="#HDFS常用命令" class="headerlink" title="HDFS常用命令"></a>HDFS常用命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">备注：Hadoop中有三种Shell命令方式：</span><br><span class="line">•hadoop fs适用于任何不同的文件系统，比如本地文件系统和HDFS文件系统</span><br><span class="line">hadoop dfs只能适用于HDFS文件系统</span><br><span class="line">hdfs dfs跟hadoop dfs的命令作用一样，也只能适用于HDFS文件系统</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls &lt;path&gt;:显示&lt;path&gt;指定的文件的详细信息</span><br><span class="line">hadoop fs -mkdir &lt;path&gt;:创建&lt;path&gt;指定的文件夹</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302252331868.png" alt="image-20230225233135783"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cat &lt;path&gt;:将&lt;path&gt;指定的文件的内容输出到标准输出（stdout）</span><br><span class="line">hadoop fs -copyFromLocal &lt;localsrc&gt; &lt;dst&gt;:将本地源文件&lt;localsrc&gt;复制到路径&lt;dst&gt;指定的文件或文件夹中</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302252333902.png" alt="image-20230225233301843"></p><h4 id="目录操作"><a href="#目录操作" class="headerlink" title="目录操作"></a>目录操作</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">./bin/hadoop fs  查看fs总共支持了哪些命令</span><br><span class="line"></span><br><span class="line">./bin/hadoop fs -help put  查看put命令如何使用，可以输入如下命令</span><br><span class="line"></span><br><span class="line">cd /usr/local/hadoop</span><br><span class="line">./bin/hdfs dfs –mkdir –p /user/hadoop  “–mkdir”是创建目录的操作，“-p”表示如果是多级目录，则父目录和子目录一起创建</span><br><span class="line"></span><br><span class="line">./bin/hdfs dfs –rm –r /input  “-r”参数表示如果删除“/input”目录及其子目录下的所有内容，如果要删除的一个目录包含了子目录，则必须使用“-r”参数，否则会执行失败。</span><br><span class="line"></span><br><span class="line">./bin/hdfs dfs –ls 目录</span><br></pre></td></tr></table></figure><h4 id="文件操作"><a href="#文件操作" class="headerlink" title="文件操作"></a>文件操作</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs dfs -put /home/hadoop/myLocalFile.txt  input</span><br><span class="line">可以使用如下命令把本地文件系统的“/home/hadoop/myLocalFile.txt”上传到HDFS中的当前用户目录的input目录下，也就是上传到HDFS的“/user/hadoop/input/”目录下</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs dfs –cat input/myLocalFile.txt</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs dfs -get input/myLocalFile.txt  /home/hadoop/下载</span><br><span class="line">把HDFS中的myLocalFile.txt文件下载到本地文件系统的“/home/hadoop/下载/”这个目录下</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs dfs -cp input/myLocalFile.txt  /input</span><br><span class="line"></span><br><span class="line">把文件从HDFS中的一个目录拷贝到HDFS中的另外一个目录</span><br></pre></td></tr></table></figure><h3 id="HDFS的Web界面"><a href="#HDFS的Web界面" class="headerlink" title="HDFS的Web界面"></a>HDFS的Web界面</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在配置好Hadoop集群之后，可以通过浏览器登录</span><br><span class="line">“http:&#x2F;&#x2F;[NameNodeIP]:50070”访问HDFS文件系统</span><br></pre></td></tr></table></figure><h3 id="HDFS常用Java-API及应用实例"><a href="#HDFS常用Java-API及应用实例" class="headerlink" title="HDFS常用Java API及应用实例"></a>HDFS常用Java API及应用实例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">利用Java API与HDFS进行交互</span><br><span class="line"></span><br><span class="line">实例：利用hadoop 的java api检测伪分布式文件系统HDFS上是否存</span><br><span class="line">在某个文件？</span><br><span class="line">准备工作：在Ubuntu系统中安装和配置Eclipse</span><br><span class="line">第一步:放置配置文件到当前工程下面（ eclipse工作目录的bin文件夹</span><br><span class="line">下面）</span><br><span class="line">第二步：编写实现代码</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面提供了Hadoop官方的Hadoop API文档，想要深入学习Hadoop，可以访问如下网站，查看各个API的功能。</span><br></pre></td></tr></table></figure><p><a href="http://hadoop.apache.org/docs/stable/api/" target="_blank" rel="external nofollow noopener noreferrer">Hadoop API文档</a></p><h4 id="在Eclipse创建项目"><a href="#在Eclipse创建项目" class="headerlink" title="在Eclipse创建项目"></a>在Eclipse创建项目</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在“Project name”后面输入工程名称“HDFSExample”，选中“Use default location”，让这个Java工程的所有文件都保存到“&#x2F;home&#x2F;hadoop&#x2F;workspace&#x2F;HDFSExample”目录下。在“JRE”这个选项卡中，可以选择当前的Linux系统中已经安装好的JDK，比如java-8-openjdk-amd64。然后，点击界面底部的“Next&gt;”按钮，进入下一步的设置。</span><br></pre></td></tr></table></figure><h4 id="为项目添加需要用到的JAR包"><a href="#为项目添加需要用到的JAR包" class="headerlink" title="为项目添加需要用到的JAR包"></a>为项目添加需要用到的JAR包</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">进入下一步的设置以后，会弹出如图4-5所示界面。</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302252355494.png" alt="img" style="zoom:80%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">需要在这个界面中加载该Java工程所需要用到的JAR包，这些JAR包中包含了可以访问HDFS的Java API。这些JAR包都位于Linux系统的Hadoop安装目录下，对于本教程而言，就是在“&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;share&#x2F;hadoop”目录下。点击界面中的“Libraries”选项卡，然后，点击界面右侧的“Add External JARs…”按钮，会弹出如图4-6所示界面。</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302252358567.png" alt="img" style="zoom:80%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">在该界面中，上面的一排目录按钮（即“usr”、“local”、“hadoop”、“share”、“hadoop”、“mapreduce”和“lib”），当点击某个目录按钮时，就会在下面列出该目录的内容。</span><br><span class="line">为了编写一个能够与HDFS交互的Java应用程序，一般需要向Java工程中添加以下JAR包：</span><br><span class="line">（1）&quot;&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;common”目录下的hadoop-common-2.7.1.jar和haoop-nfs-2.7.1.jar；</span><br><span class="line">（2）&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;lib”目录下的所有JAR包；</span><br><span class="line">（3）“&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;hdfs”目录下的haoop-hdfs-2.7.1.jar和haoop-hdfs-nfs-2.7.1.jar；</span><br><span class="line">（4）“&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;hdfs&#x2F;lib”目录下的所有JAR包。</span><br><span class="line">比如，如果要把“&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;common”目录下的hadoop-common-2.7.1.jar和haoop-nfs-2.7.1.jar添加到当前的Java工程中，可以在界面中点击目录按钮，进入到common目录，然后，界面会显示出common目录下的所有内容（如图4-7所示）。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302260000415.png" alt="img"></p><h4 id="编写Java应用程序代码"><a href="#编写Java应用程序代码" class="headerlink" title="编写Java应用程序代码"></a>编写Java应用程序代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HDFSFileIfExist</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">            String fileName = <span class="string">"test"</span>;</span><br><span class="line">            Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">            conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://localhost:9000"</span>);</span><br><span class="line">            conf.set(<span class="string">"fs.hdfs.impl"</span>, <span class="string">"org.apache.hadoop.hdfs.DistributedFileSystem"</span>);</span><br><span class="line">            FileSystem fs = FileSystem.get(conf);</span><br><span class="line">            <span class="keyword">if</span>(fs.exists(<span class="keyword">new</span> Path(fileName)))&#123;</span><br><span class="line">                System.out.println(<span class="string">"文件存在"</span>);</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                System.out.println(<span class="string">"文件不存在"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这行代码给出了需要被检测的文件名称是“test”，没有给出路径全称，表示是采用了相对路径，实际上就是测试当前登录Linux系统的用户hadoop，在HDFS中对应的用户目录下是否存在test文件，也就是测试HDFS中的“&#x2F;user&#x2F;hadoop&#x2F;”目录下是否存在test文件。</span><br></pre></td></tr></table></figure><h4 id="编译运行程序"><a href="#编译运行程序" class="headerlink" title="编译运行程序"></a>编译运行程序</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd &#x2F;usr&#x2F;local&#x2F;hadoop</span><br><span class="line">.&#x2F;sbin&#x2F;start-dfs.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">现在就可以编译运行上面编写的代码。可以直接点击Eclipse工作界面上部的运行程序的快捷按钮，当把鼠标移动到该按钮上时，在弹出的菜单中选择“Run As”，继续在弹出来的菜单中选择“Java Application”，如图4-12所示。</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302260010905.png" alt="img" style="zoom:80%;"><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302260011816.png" alt="img" style="zoom:80%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在该界面中，需要在“Select type”下面的文本框中输入“HDFSFileIfExist”，Eclipse就会自动找到相应的类“HDFSFileIfExist-(default package)”（注意：这个类在后面的导出JAR包操作中的Launch configuration中会被用到），然后，点击界面右下角的“OK”按钮，开始运行程序。程序运行结束后，会在底部的“Console”面板中显示运行结果信息（如图4-14所示）。由于目前HDFS的“&#x2F;user&#x2F;hadoop”目录下还没有test文件，因此，程序运行结果是“文件不存在”。同时，“Console”面板中还会显示一些类似“log4j:WARN…”的警告信息，可以不用理会。</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302260012071.png" alt="img" style="zoom:80%;"><h4 id="应用程序的部署"><a href="#应用程序的部署" class="headerlink" title="应用程序的部署"></a>应用程序的部署</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面介绍如何把Java应用程序生成JAR包，部署到Hadoop平台上运行。首先，在Hadoop安装目录下新建一个名称为myapp的目录，用来存放我们自己编写的Hadoop应用程序</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">然后，请在Eclipse工作界面左侧的“Package Explorer”面板中，在工程名称“HDFSExample”上点击鼠标右键，在弹出的菜单中选择“Export”，如图4-15所示。</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302260015871.png" alt="img" style="zoom:80%;"><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302260015002.png" alt="img" style="zoom:80%;"><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302260015648.png" alt="img" style="zoom:80%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在该界面中，“Launch configuration”用于设置生成的JAR包被部署启动时运行的主类，需要在下拉列表中选择刚才配置的类“HDFSFileIfExist-HDFSExample”。在“Export destination”中需要设置JAR包要输出保存到哪个目录，比如，这里设置为“&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;myapp&#x2F;HDFSExample.jar”。在“Library handling”下面选择“Extract required libraries into generated JAR”。然后，点击“Finish”按钮，会出现如图4-18所示界面。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302260017891.png" alt="img"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">可以忽略该界面的信息，直接点击界面右下角的“OK”按钮。至此，已经顺利把HDFSExample工程打包生成了HDFSExample.jar。</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">现在，就可以在Linux系统中，使用hadoop jar命令运行程序，命令如下：</span><br><span class="line"></span><br><span class="line">cd /usr/local/hadoop</span><br><span class="line">./bin/hadoop jar ./myapp/HDFSExample.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">或者也可以使用如下命令运行程序：</span><br><span class="line"></span><br><span class="line">cd &#x2F;usr&#x2F;local&#x2F;hadoop</span><br><span class="line">java -jar .&#x2F;myapp&#x2F;HDFSExample.jar</span><br></pre></td></tr></table></figure><h2 id="附录：自己练习用的代码文件"><a href="#附录：自己练习用的代码文件" class="headerlink" title="附录：自己练习用的代码文件"></a>附录：自己练习用的代码文件</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;  </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Chapter3</span> </span>&#123;    </span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123; </span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                        Configuration conf = <span class="keyword">new</span> Configuration();  </span><br><span class="line">                        conf.set(<span class="string">"fs.defaultFS"</span>,<span class="string">"hdfs://localhost:9000"</span>);</span><br><span class="line">                        conf.set(<span class="string">"fs.hdfs.impl"</span>,<span class="string">"org.apache.hadoop.hdfs.DistributedFileSystem"</span>);</span><br><span class="line">                        FileSystem fs = FileSystem.get(conf);</span><br><span class="line">                        <span class="keyword">byte</span>[] buff = <span class="string">"Hello world"</span>.getBytes(); <span class="comment">// 要写入的内容</span></span><br><span class="line">                        String filename = <span class="string">"test"</span>; <span class="comment">//要写入的文件名</span></span><br><span class="line">                        FSDataOutputStream os = fs.create(<span class="keyword">new</span> Path(filename));</span><br><span class="line">                        os.write(buff,<span class="number">0</span>,buff.length);</span><br><span class="line">                        System.out.println(<span class="string">"Create:"</span>+ filename);</span><br><span class="line">                        os.close();</span><br><span class="line">                        fs.close();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;  </span><br><span class="line">                        e.printStackTrace();  </span><br><span class="line">                &#125;  </span><br><span class="line">        &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Chapter3</span> </span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                            String filename = <span class="string">"test"</span>;</span><br><span class="line"> </span><br><span class="line">                            Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">                            conf.set(<span class="string">"fs.defaultFS"</span>,<span class="string">"hdfs://localhost:9000"</span>);</span><br><span class="line">                            conf.set(<span class="string">"fs.hdfs.impl"</span>,<span class="string">"org.apache.hadoop.hdfs.DistributedFileSystem"</span>);</span><br><span class="line">                            FileSystem fs = FileSystem.get(conf);</span><br><span class="line">                            <span class="keyword">if</span>(fs.exists(<span class="keyword">new</span> Path(filename)))&#123;</span><br><span class="line">                                    System.out.println(<span class="string">"文件存在"</span>);</span><br><span class="line">                            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                                    System.out.println(<span class="string">"文件不存在"</span>);</span><br><span class="line">                            &#125;</span><br><span class="line">                            fs.close();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Chapter3</span> </span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">                        conf.set(<span class="string">"fs.defaultFS"</span>,<span class="string">"hdfs://localhost:9000"</span>);</span><br><span class="line">                        conf.set(<span class="string">"fs.hdfs.impl"</span>,<span class="string">"org.apache.hadoop.hdfs.DistributedFileSystem"</span>);</span><br><span class="line">                        FileSystem fs = FileSystem.get(conf);</span><br><span class="line">                        Path file = <span class="keyword">new</span> Path(<span class="string">"test"</span>); </span><br><span class="line">                        FSDataInputStream getIt = fs.open(file);</span><br><span class="line">                        BufferedReader d = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(getIt));</span><br><span class="line">                        String content = d.readLine(); <span class="comment">//读取文件一行</span></span><br><span class="line">                        System.out.println(content);</span><br><span class="line">                        d.close(); <span class="comment">//关闭文件</span></span><br><span class="line">                        fs.close(); <span class="comment">//关闭hdfs</span></span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">注意！！！！！！！！！！！！！！！！</span><br><span class="line">第一步:放置配置文件到当前工程下面</span><br><span class="line">需要把集群上的core-site.xml和hdfs-site.xml(这两文件存在</span><br><span class="line">&#x2F;hadoop&#x2F;etc&#x2F;hadoop目录下)放到当前工程项目下，即eclipse工作目录的</span><br><span class="line">bin文件夹下面</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">这样就不需要写：~！！！！！！！！！！！！！！</span><br><span class="line">conf.set(&quot;fs.defaultFS&quot;,&quot;hdfs:&#x2F;&#x2F;localhost:9000&quot;);</span><br><span class="line">                                conf.set(&quot;fs.hdfs.impl&quot;,&quot;org.apache.hadoop.hdfs.DistributedFileSystem&quot;);</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Chapter3</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">String filename = <span class="string">"hdfs://localhost:9000/user/hadoop/test.txt"</span>;</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(conf);</span><br><span class="line"><span class="keyword">if</span>(fs.exists(<span class="keyword">new</span> Path(filename)))&#123;</span><br><span class="line">System.out.println(<span class="string">"文件存在"</span>);</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">System.out.println(<span class="string">"文件不存在"</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第二章 大数据处理框架Hadoop</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6Hadoop.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6Hadoop.html</id>
    <published>2023-02-23T19:03:02.000Z</published>
    <updated>2023-03-14T09:11:47.255Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第二章-大数据处理框架Hadoop"><a href="#第二章-大数据处理框架Hadoop" class="headerlink" title="第二章 大数据处理框架Hadoop"></a>第二章 大数据处理框架Hadoop</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">主页：http:&#x2F;&#x2F;www.cs.xmu.edu.cn&#x2F;linziyu</span><br><span class="line"></span><br><span class="line">欢迎访问《大数据技术原理与应用》教材官方网站：</span><br><span class="line">http:&#x2F;&#x2F;dblab.xmu.edu.cn&#x2F;post&#x2F;bigdata</span><br></pre></td></tr></table></figure><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="Hadoop简介"><a href="#Hadoop简介" class="headerlink" title="Hadoop简介"></a>Hadoop简介</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">• Hadoop是Apache软件基金会旗下的一个开源分布式计算平台</span><br><span class="line">•Hadoop是基于Java语言开发的，具有很好的跨平台特性，并且可以部署在廉价的计算机集群中</span><br><span class="line">•Hadoop的核心是分布式文件系统HDFS（Hadoop Distributed File System）和MapReduce</span><br><span class="line">•Hadoop被公认为行业大数据标准开源软件，在分布式环境下提供了海量数据的处理能力</span><br><span class="line">•几乎所有主流厂商都围绕Hadoop提供开发工具、开源软件、商业化工具和技术服务，如谷歌、雅虎、微软、思科、淘宝等，都支持Hadoop</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302240310867.png" alt="image-20230224031009725"></p><h3 id="Hadoop发展简史"><a href="#Hadoop发展简史" class="headerlink" title="Hadoop发展简史"></a>Hadoop发展简史</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">• Hadoop最初是由Apache Lucene项目的创始人Doug Cutting开发的文本搜索库。Hadoop源自始于2002年的Apache Nutch项目——一个开源的网络搜索引擎并且也是Lucene项目的一部分</span><br><span class="line">• 在2004年，Nutch项目也模仿GFS开发了自己的分布式文件系统NDFS（Nutch Distributed File System），也就是HDFS的前身</span><br><span class="line">• 2004年，谷歌公司又发表了另一篇具有深远影响的论文，阐述了MapReduce分布式编程思想</span><br><span class="line">• 2005年，Nutch开源实现了谷歌的MapReduce</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">• 到了2006年2月，Nutch中的NDFS和MapReduce开始独立出来，成为Lucene项目的一个子项目，称为Hadoop，同时，Doug Cutting加盟雅虎</span><br><span class="line">• 2008年1月，Hadoop正式成为Apache顶级项目，Hadoop也逐渐开始被雅虎之外的其他公司使用</span><br><span class="line">• 2008年4月，Hadoop打破世界纪录，成为最快排序1TB数据的系统，它采用一个由910个节点构成的集群进行运算，排序时间只用了209秒</span><br><span class="line">•在2009年5月，Hadoop更是把1TB数据排序时间缩短到62秒。Hadoop从此名声大震，迅速发展成为大数据时代最具影响力的开源分布式开发平台，并成为事实上的大数据处理标准</span><br></pre></td></tr></table></figure><h3 id="Hadoop的特性"><a href="#Hadoop的特性" class="headerlink" title="Hadoop的特性"></a>Hadoop的特性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">• 高可靠性</span><br><span class="line">• 高效性</span><br><span class="line">• 高可扩展性</span><br><span class="line">• 高容错性</span><br><span class="line">• 成本低</span><br><span class="line">• 运行在Linux平台上</span><br><span class="line">• 支持多种编程语言</span><br></pre></td></tr></table></figure><h3 id="Hadoop的应用现状"><a href="#Hadoop的应用现状" class="headerlink" title="Hadoop的应用现状"></a>Hadoop的应用现状</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• Facebook作为全球知名的社交网站，Hadoop是非常理想的选择，Facebook主要将Hadoop平台用于日志处理、推荐系统和数据仓库等</span><br><span class="line">方面</span><br><span class="line">• 国内采用Hadoop的公司主要有百度、淘宝、网易、华为、中国移动等，其中，淘宝的Hadoop集群比较大</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302240322387.png" alt="image-20230224032203188"></p><h3 id="Apache-Hadoop版本演变"><a href="#Apache-Hadoop版本演变" class="headerlink" title="Apache Hadoop版本演变"></a>Apache Hadoop版本演变</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">•Apache Hadoop版本分为两代，我们将第一代Hadoop称为Hadoop 1.0，第二代Hadoop称为Hadoop 2.0</span><br><span class="line">•第一代Hadoop包含三个大版本，分别是0.20.x，0.21.x和0.22.x，其中，0.20.x最后演化成1.0.x，变成了稳定版，而0.21.x和0.22.x则增加了NameNode HA等新的重大特性</span><br><span class="line">•第二代Hadoop包含两个版本，分别是0.23.x和2.x，它们完全不同于Hadoop 1.0，是一套全新的架构，均包含HDFS Federation和YARN两个系统，相比于0.23.x，2.x增加了NameNode HA和Wire-compatibility两个重大特性</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">有3.0了,大数据开发工程师</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302240330058.png" alt="image-20230224033028837"></p><h3 id="Hadoop各种版本"><a href="#Hadoop各种版本" class="headerlink" title="Hadoop各种版本"></a>Hadoop各种版本</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302241621593.png" alt="image-20230224162152527"></p><h3 id="2-1-5-Hadoop各种版本"><a href="#2-1-5-Hadoop各种版本" class="headerlink" title="2.1.5 Hadoop各种版本"></a>2.1.5 Hadoop各种版本</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302241635397.png" alt="image-20230224163550332"></p><h2 id="Hadoop项目结构"><a href="#Hadoop项目结构" class="headerlink" title="Hadoop项目结构"></a>Hadoop项目结构</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hadoop的项目结构不断丰富发展，已经形成一个丰富的Hadoop生态系统</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302241637513.png" alt="image-20230224163755829"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">组件 功能</span><br><span class="line">HDFS 分布式文件系统</span><br><span class="line">MapReduce 分布式并行编程模型</span><br><span class="line">YARN 资源管理和调度器</span><br><span class="line">Tez 运行在YARN之上的下一代Hadoop查询处理框架</span><br><span class="line">Hive Hadoop上的数据仓库</span><br><span class="line">HBase Hadoop上的非关系型的分布式数据库</span><br><span class="line">Pig 一个基于Hadoop的大规模数据分析平台，提供类似SQL的查询语言Pig Latin</span><br><span class="line">Sqoop 用于在Hadoop与传统数据库之间进行数据传递</span><br><span class="line">Oozie Hadoop上的工作流管理系统</span><br><span class="line">Zookeeper 提供分布式协调一致性服务</span><br><span class="line">Storm 流计算框架</span><br><span class="line">Flume 一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统</span><br><span class="line">Ambari Hadoop快速部署工具，支持Apache Hadoop集群的供应、管理和监控</span><br><span class="line">Kafka 一种高吞吐量的分布式发布订阅消息系统，可以处理消费者规模的网站中的所有动作流数据</span><br><span class="line">Spark 类似于Hadoop MapReduce的通用并行框架</span><br></pre></td></tr></table></figure><h2 id="Hadoop的安装与使用"><a href="#Hadoop的安装与使用" class="headerlink" title="Hadoop的安装与使用"></a>Hadoop的安装与使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">详细安装教程请参考厦门大学数据实验室出品的《大数据技术原理与</span><br><span class="line">应用 第二章 大数据处理架构Hadoop 学习指南》</span><br><span class="line">访问地址：http:&#x2F;&#x2F;dblab.xmu.edu.cn&#x2F;blog&#x2F;285&#x2F;</span><br></pre></td></tr></table></figure><h3 id="Hadoop安装之前的预备知识"><a href="#Hadoop安装之前的预备知识" class="headerlink" title="Hadoop安装之前的预备知识"></a>Hadoop安装之前的预备知识</h3><h4 id="Linux的选择"><a href="#Linux的选择" class="headerlink" title="Linux的选择"></a>Linux的选择</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">（1）选择哪个Linux发行版？</span><br><span class="line">•在Linux系统各个发行版中，CentOS系统和Ubuntu系统在服务端和桌面端使用占比最高，网络上资料最是齐全，所以建议使用CentOS 或Ubuntu</span><br><span class="line">•在学习Hadoop方面，虽然两个系统没有多大区别，但是推荐使用Ubuntu操作系统</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">（2）选择32位还是64位？</span><br><span class="line">•如果电脑比较老或者内存小于2G，那么建议选择32位系统版本的Linux</span><br><span class="line">•如果内存大于4G，那么建议选择64位系统版本的Linux</span><br></pre></td></tr></table></figure><h4 id="系统安装方式：选择虚拟机安装还是双系统安装"><a href="#系统安装方式：选择虚拟机安装还是双系统安装" class="headerlink" title="系统安装方式：选择虚拟机安装还是双系统安装"></a>系统安装方式：选择虚拟机安装还是双系统安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">•建议电脑比较新或者配置内存4G以上的电脑可以选择虚拟机安装</span><br><span class="line">•电脑较旧或配置内存小于等于4G的电脑强烈建议选择双系统安装，否则，在配置较低的计算机上运行LInux虚拟机，系统运行速度会非常慢</span><br><span class="line">•鉴于目前教师和学生的计算机硬件配置一般不高，建议在实践教学中采用双系统安装，确保系统运行速度</span><br></pre></td></tr></table></figure><h4 id="关于Linux的一些基础知识"><a href="#关于Linux的一些基础知识" class="headerlink" title="关于Linux的一些基础知识"></a>关于Linux的一些基础知识</h4><h4 id="Hadoop安装方式"><a href="#Hadoop安装方式" class="headerlink" title="Hadoop安装方式"></a>Hadoop安装方式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">•单机模式：Hadoop 默认模式为非分布式模式（本地模式），无需进行其他配置即可运行。非分布式即单 Java 进程，方便进行调试</span><br><span class="line">•伪分布式模式：Hadoop 可以在单节点上以伪分布式的方式运行，Hadoop 进程以分离的 Java 进程来运行，节点既作为 NameNode 也作为 DataNode，同时，读取的是 HDFS 中的文件</span><br><span class="line">•分布式模式：使用多个节点构成集群环境来运行Hadoop</span><br></pre></td></tr></table></figure><h3 id="安装Linux虚拟机"><a href="#安装Linux虚拟机" class="headerlink" title="安装Linux虚拟机"></a>安装Linux虚拟机</h3><h3 id="安装双操作系统"><a href="#安装双操作系统" class="headerlink" title="安装双操作系统"></a>安装双操作系统</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">•第一步：制作安装U盘</span><br><span class="line">•具体可参考百度经验文章</span><br><span class="line">•http:&#x2F;&#x2F;jingyan.baidu.com&#x2F;article&#x2F;59703552e0a6e18fc007409f.html</span><br><span class="line">•第二步：双系统安装</span><br><span class="line">•具体可参考百度经验文章</span><br><span class="line">•http:&#x2F;&#x2F;jingyan.baidu.com&#x2F;article&#x2F;dca1fa6fa3b905f1a44052bd.html</span><br></pre></td></tr></table></figure><h3 id="详解Hadoop的安装与使用"><a href="#详解Hadoop的安装与使用" class="headerlink" title="详解Hadoop的安装与使用"></a>详解Hadoop的安装与使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Hadoop基本安装配置主要包括以下几个步骤：</span><br><span class="line">• 创建Hadoop用户</span><br><span class="line">• SSH登录权限设置</span><br><span class="line">• 安装Java环境</span><br><span class="line">• 单机安装配置</span><br><span class="line">• 伪分布式安装配置</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">详细安装配置过程请参考厦门大学数据库实验室出品教程</span><br><span class="line">《Hadoop安装教程_单机&#x2F;伪分布式配置_Hadoop2.6.0&#x2F;Ubuntu14.04》</span><br><span class="line">http:&#x2F;&#x2F;dblab.xmu.edu.cn&#x2F;blog&#x2F;install-hadoop&#x2F;</span><br></pre></td></tr></table></figure><h4 id="创建Hadoop用户"><a href="#创建Hadoop用户" class="headerlink" title="创建Hadoop用户"></a>创建Hadoop用户</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">如果安装 Ubuntu 的时候不是用的 “hadoop” 用户，那么需要增加一个名为</span><br><span class="line">hadoop 的用户</span><br><span class="line">首先按 ctrl+alt+t 打开终端窗口，输入如下命令创建新用户 :</span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo useradd –m hadoop –s /bin/bash</span></span><br><span class="line">上面这条命令创建了可以登陆的 hadoop 用户，并使用 /bin/bash 作为 shell</span><br><span class="line">接着使用如下命令设置密码，可简单设置为 hadoop，按提示输入两次密码：</span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo passwd hadoop</span></span><br><span class="line">可为 hadoop 用户增加管理员权限，方便部署，避免一些对新手来说比较</span><br><span class="line">棘手的权限问题：</span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo adduser hadoop sudo</span></span><br></pre></td></tr></table></figure><h4 id="SSH登录权限设置"><a href="#SSH登录权限设置" class="headerlink" title="SSH登录权限设置"></a>SSH登录权限设置</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SSH 为 Secure Shell 的缩写，是建立在应用层和传输层基础上的安全协议。SSH 是目前较可靠、专为远程登录会话和其他网络服务提供安全性的协议。利用 SSH 协议可以有效防止远程管理过程中的信息泄露问题。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SSH是由客户端和服务端的软件组成，服务端是一个守护进程(daemon)，它在后台运行并响应来自客户端的连接请求，</span><br><span class="line">客户端包含ssh程序以及像scp（远程拷贝）、slogin（远程登陆）、sftp（安全文件传输）等其他的应用程序</span><br></pre></td></tr></table></figure><h5 id="配置SSH的原因"><a href="#配置SSH的原因" class="headerlink" title="配置SSH的原因"></a>配置SSH的原因</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hadoop名称节点（NameNode）需要启动集群中所有机器的Hadoop守护进程，这个过程需要通过SSH登录来实现。Hadoop并没有提供SSH输入密码登录的形式，因此，为了能够顺利登录每台机器，需要将所有机器配置为名称节点可以无密码登录它们</span><br></pre></td></tr></table></figure><h4 id="安装Java环境"><a href="#安装Java环境" class="headerlink" title="安装Java环境"></a>安装Java环境</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 具体请参考网络教程：http:&#x2F;&#x2F;dblab.xmu.edu.cn&#x2F;blog&#x2F;install-hadoop&#x2F;</span><br></pre></td></tr></table></figure><h5 id="第1种安装方式"><a href="#第1种安装方式" class="headerlink" title="第1种安装方式"></a>第1种安装方式</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">下面有三种安装JDK的方式，可以任选一种。推荐直接使用第1种安装方式。</span><br><span class="line"></span><br><span class="line">（1）第1种安装JDK方式（手动安装，推荐采用本方式）</span><br><span class="line">cd /usr/lib</span><br><span class="line">sudo mkdir jvm #创建/usr/lib/jvm目录用来存放JDK文件</span><br><span class="line">cd ~ #进入hadoop用户的主目录</span><br><span class="line">cd Downloads  #注意区分大小写字母，刚才已经通过FTP软件把JDK安装包jdk-8u162-linux-x64.tar.gz上传到该目录下</span><br><span class="line">sudo tar -zxvf ./jdk-8u162-linux-x64.tar.gz -C /usr/lib/jvm  #把JDK文件解压到/usr/lib/jvm目录下</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/lib/jvm</span><br><span class="line">ls</span><br><span class="line">可以看到，在/usr/lib/jvm目录下有个jdk1.8.0_162目录。</span><br><span class="line">下面继续执行如下命令，设置环境变量：</span><br><span class="line">cd ~</span><br><span class="line">vim ~/.bashrc</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">添加如下几行内容：</span><br><span class="line"></span><br><span class="line">export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_162</span><br><span class="line">export JRE_HOME=$&#123;JAVA_HOME&#125;/jre</span><br><span class="line">export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib</span><br><span class="line">export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">保存.bashrc文件并退出vim编辑器。然后，继续执行如下命令让.bashrc文件的配置立即生效：</span><br><span class="line"></span><br><span class="line">source ~/.bashrc</span><br><span class="line"></span><br><span class="line">java -version</span><br></pre></td></tr></table></figure><h5 id="第2种安装方式"><a href="#第2种安装方式" class="headerlink" title="第2种安装方式"></a>第2种安装方式</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install openjdk-7-jre openjdk-7-jdk</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JRE和JDK的区别: JRE（Java Runtime Environment，Java运行环境），是运行 Java 所需的环境。JDK（Java Development Kit，Java软件开发工具包）即包括 JRE，还包括开发 Java 程序所需的工具和类库。</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">安装好 OpenJDK 后，需要找到相应的安装路径，这个路径是用于配置 JAVA_HOME 环境变量的。执行如下命令：</span><br><span class="line"></span><br><span class="line">dpkg -L openjdk-7-jdk | grep '/bin/javac'</span><br><span class="line"></span><br><span class="line">该命令会输出一个路径，除去路径末尾的 “/bin/javac”，剩下的就是正确的路径了。如输出路径为 /usr/lib/jvm/java-7-openjdk-amd64/bin/javac，则我们需要的路径为 /usr/lib/jvm/java-7-openjdk-amd64。</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">接着需要配置一下 JAVA_HOME 环境变量，为方便，我们在 ~/.bashrc 中进行设置</span><br><span class="line">vim ~/.bashrc</span><br><span class="line"></span><br><span class="line">在文件最前面添加如下单独一行（注意 = 号前后不能有空格），将“JDK安装路径”改为上述命令得到的路径，并保存：</span><br><span class="line">export JAVA_HOME=JDK安装路径</span><br><span class="line"></span><br><span class="line">source ~/.bashrc    # 使变量设置生效</span><br><span class="line"></span><br><span class="line">设置好后我们来检验一下是否设置正确：</span><br><span class="line">echo $JAVA_HOME     # 检验变量值</span><br><span class="line">java -version</span><br><span class="line"><span class="meta">$</span><span class="bash">JAVA_HOME/bin/java -version  <span class="comment"># 与直接执行 java -version 一样</span></span></span><br></pre></td></tr></table></figure><h5 id="第3种安装方式"><a href="#第3种安装方式" class="headerlink" title="第3种安装方式"></a>第3种安装方式</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">根据大量电脑安装Java环境的情况我们发现，部分电脑按照上述的第一种安装方式会出现安装失败的情况，这时，可以采用这里介绍的另外一种安装方式，命令如下：</span><br><span class="line">sudo apt-get install default-jre default-jdk</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">上述安装过程需要访问网络下载相关文件，请保持联网状态。安装结束以后，需要配置JAVA_HOME环境变量，请在Linux终端中输入下面命令打开当前登录用户的环境变量配置文件.bashrc：</span><br><span class="line">vim ~/.bashrc</span><br><span class="line"></span><br><span class="line">在文件最前面添加如下单独一行（注意，等号“=”前后不能有空格），然后保存退出：</span><br><span class="line">export JAVA_HOME=/usr/lib/jvm/default-java</span><br><span class="line"></span><br><span class="line">接下来，要让环境变量立即生效，请执行如下代码：</span><br><span class="line">source ~/.bashrc    # 使变量设置生效</span><br><span class="line">执行上述命令后，可以检验一下是否设置正确：</span><br><span class="line">echo $JAVA_HOME     # 检验变量值</span><br><span class="line">java -version</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash">JAVA_HOME/bin/java -version  <span class="comment"># 与直接执行java -version一样</span></span></span><br><span class="line">至此，就成功安装了Java环境。下面就可以进入Hadoop的安装。</span><br></pre></td></tr></table></figure><h5 id="设置Linux环境变量的方法和区别"><a href="#设置Linux环境变量的方法和区别" class="headerlink" title="设置Linux环境变量的方法和区别"></a>设置Linux环境变量的方法和区别</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">设置 Linux 环境变量可以通过 export 实现，也可以通过修改几个文件来实现，有必要弄清楚这两种方法以及这几个文件的区别。</span><br></pre></td></tr></table></figure><p><a href="https://dblab.xmu.edu.cn/blog/linux-environment-variable/" target="_blank" rel="external nofollow noopener noreferrer">设置Linux环境变量的方法和区别</a></p><h6 id="对所有用户都会生效"><a href="#对所有用户都会生效" class="headerlink" title="对所有用户都会生效"></a>对所有用户都会生效</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">etc&#x2F;profile: 此文件为系统的每个用户设置环境信息。当用户登录时，该文件被执行一次，并从 &#x2F;etc&#x2F;profile.d 目录的配置文件中搜集shell 的设置。一般用于设置所有用户使用的全局变量。</span><br><span class="line"></span><br><span class="line">&#x2F;etc&#x2F;bashrc: 当 bash shell 被打开时，该文件被读取。也就是说，每次新打开一个终端 shell，该文件就会被读取。</span><br></pre></td></tr></table></figure><h6 id="对单个用户生效"><a href="#对单个用户生效" class="headerlink" title="对单个用户生效"></a>对单个用户生效</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">~&#x2F;.bash_profile 或 ~&#x2F;.profile: 只对单个用户生效，当用户登录时该文件仅执行一次。用户可使用该文件添加自己使用的 shell 变量信息。另外在不同的LINUX操作系统下，这个文件可能是不同的，可能是 ~&#x2F;.bash_profile， ~&#x2F;.bash_login 或 ~&#x2F;.profile 其中的一种或几种，如果存在几种的话，那么执行的顺序便是：~&#x2F;.bash_profile、 ~&#x2F;.bash_login、 ~&#x2F;.profile。比如 Ubuntu 系统一般是~&#x2F;.profile 文件。</span><br><span class="line"></span><br><span class="line">~&#x2F;.bashrc: 只对单个用户生效，当登录以及每次打开新的 shell 时，该文件被读取。</span><br></pre></td></tr></table></figure><h6 id="etc-environment"><a href="#etc-environment" class="headerlink" title="/etc/environment"></a>/etc/environment</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此外，修改 &#x2F;etc&#x2F;environment 这个文件也能实现环境变量的设置。&#x2F;etc&#x2F;environment 设置的也是全局变量，从文件本身的作用上来说， &#x2F;etc&#x2F;environment 设置的是整个系统的环境，而&#x2F;etc&#x2F;profile是设置所有用户的环境。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">有几点需注意：</span><br><span class="line">系统先读取 etc&#x2F;profile 再读取 &#x2F;etc&#x2F;environment（还是反过来？）</span><br><span class="line">&#x2F;etc&#x2F;environment 中不能包含命令，即直接通过 VAR&#x3D;&quot;...&quot; 的方式设置，不使用 export 。</span><br><span class="line">使用 source &#x2F;etc&#x2F;environment 可以使变量设置在当前窗口立即生效，需注销&#x2F;重启之后，才能对每个新终端窗口都生效。</span><br></pre></td></tr></table></figure><h6 id="修改-Linux-环境变量实例"><a href="#修改-Linux-环境变量实例" class="headerlink" title="修改 Linux 环境变量实例"></a>修改 Linux 环境变量实例</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.profile</span><br><span class="line"></span><br><span class="line">如果该文件存在，则在文件的最后看到如下代码，PATH 变量的值使用冒号(:)隔开的：</span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">set</span> PATH so it includes user<span class="string">'s private bin if it exists</span></span></span><br><span class="line">if [ -d "$HOME/bin" ] ; then</span><br><span class="line">    PATH="$HOME/bin:$PATH"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在最后加上代码 PATH="$PATH:/usr/local/hadoop/bin"，注意等号(=)两边不要有空格，即：</span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">set</span> PATH so it includes user<span class="string">'s private bin if it exists</span></span></span><br><span class="line">if [ -d "$HOME/bin" ] ; then</span><br><span class="line">    PATH="$HOME/bin:$PATH"</span><br><span class="line">fi</span><br><span class="line">PATH="$PATH:/usr/local/hadoop/bin"</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">因为这个文件是在用户登陆是才读取一次的，所以需要重启才会生效（修改 /etc/profile、/etc/environment 也是如此）。但可以使用命令 source ~/.profile 使其立即生效。通过 echo $PATH 可以看到修改后的变量值：</span><br><span class="line">source ~/.profile</span><br><span class="line">echo $PATH</span><br></pre></td></tr></table></figure><h6 id="通过-Shell-命令-export-修改-Linux-环境变量"><a href="#通过-Shell-命令-export-修改-Linux-环境变量" class="headerlink" title="通过 Shell 命令 export 修改 Linux 环境变量"></a>通过 Shell 命令 export 修改 Linux 环境变量</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">另一种修改 Linux 环境变量的方式就是通过 Shell 命令 export，注意变量名不要有美元号 $，赋值语句中才需要有：</span><br><span class="line">export PATH=$PATH:/usr/local/hadoop/bin</span><br><span class="line"></span><br><span class="line">export 方式只对当前终端 Shell 有效: 使用 export 设置的变量，只对当前终端 Shell 有效，也就是说如果新打开一个终端，那这个 export 设置的变量在新终端中使无法读取到的。适合设置一些临时变量。</span><br><span class="line"></span><br><span class="line">根据变量所需，选择设置方式，例如 JAVA_HOME 这类变量，就适合将其设为为全局变量，可在 /etc/environment 中设置。</span><br></pre></td></tr></table></figure><h4 id="安装-Hadoop"><a href="#安装-Hadoop" class="headerlink" title="安装 Hadoop"></a>安装 Hadoop</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">一般选择下载最新的稳定版本，即下载 &quot;stable&quot; 下的 hadoop-2.x.y.tar.gz 这个格式的文件，这是编译好的，另一个包含 src 的则是 Hadoop 源代码，需要进行编译才可使用。</span><br><span class="line"></span><br><span class="line">下载完 Hadoop 文件后一般就可以直接使用。但是如果网络不好，可能会导致下载的文件缺失，可以使用 md5 等检测工具可以校验文件是否完整。</span><br><span class="line"></span><br><span class="line">下载官方网站提供的 hadoop-2.x.y.tar.gz.mds 这个文件，该文件包含了检验值可用于检查 hadoop-2.x.y.tar.gz 的完整性，否则若文件发生了损坏或下载不完整，Hadoop 将无法正常运行。</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cat ~/下载/hadoop-2.6.0.tar.gz.mds | grep 'MD5' # 列出md5检验值</span><br><span class="line"><span class="meta">#</span><span class="bash"> head -n 6 ~/下载/hadoop-2.7.1.tar.gz.mds <span class="comment"># 2.7.1版本格式变了，可以用这种方式输出</span></span></span><br><span class="line">md5sum ~/下载/hadoop-2.6.0.tar.gz | tr "a-z" "A-Z" # 计算md5值，并转化为大写，方便比较</span><br><span class="line"></span><br><span class="line">若文件不完整则这两个值一般差别很大，可以简单对比下前几个字符跟后几个字符是否相等即可，如下图所示，如果两个值不一样，请务必重新下载。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302250135408.png" alt="检验文件完整性"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo tar -zxf ~&#x2F;下载&#x2F;hadoop-2.6.0.tar.gz -C &#x2F;usr&#x2F;local    # 解压到&#x2F;usr&#x2F;local中</span><br><span class="line">cd &#x2F;usr&#x2F;local&#x2F;</span><br><span class="line">sudo mv .&#x2F;hadoop-2.6.0&#x2F; .&#x2F;hadoop            # 将文件夹名改为hadoop</span><br><span class="line">sudo chown -R hadoop .&#x2F;hadoop       # 修改文件权限</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Hadoop 解压后即可使用。输入如下命令来检查 Hadoop 是否可用，成功则会显示 Hadoop 版本信息：</span><br><span class="line">cd &#x2F;usr&#x2F;local&#x2F;hadoop</span><br><span class="line">.&#x2F;bin&#x2F;hadoop version</span><br></pre></td></tr></table></figure><h5 id="单机安装配置"><a href="#单机安装配置" class="headerlink" title="单机安装配置"></a>单机安装配置</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https:&#x2F;&#x2F;dblab.xmu.edu.cn&#x2F;blog&#x2F;7&#x2F;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Hadoop 默认模式为非分布式模式（本地模式），无需进行其他配置即可运行。非分布式即单 Java 进程，方便进行调试。</span><br><span class="line"></span><br><span class="line">现在我们可以执行例子来感受下 Hadoop 的运行。Hadoop 附带了丰富的例子（运行 ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar 可以看到所有例子），包括 wordcount、terasort、join、grep 等。</span><br><span class="line"></span><br><span class="line">在此我们选择运行 grep 例子，我们将 input 文件夹中的所有文件作为输入，筛选当中符合正则表达式 dfs[a-z.]+ 的单词并统计出现的次数，最后输出结果到 output 文件夹中。</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop</span><br><span class="line">mkdir ./input</span><br><span class="line">cp ./etc/hadoop/*.xml ./input   # 将配置文件作为输入文件</span><br><span class="line">./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar grep ./input ./output 'dfs[a-z.]+'</span><br><span class="line">cat ./output/*          # 查看运行结果</span><br><span class="line"></span><br><span class="line">执行成功后如下所示，输出了作业的相关信息，输出的结果是符合正则的单词 dfsadmin 出现了1次</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302250151792.png" alt="Hadoop单机模式运行grep的输出结果"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意，Hadoop 默认不会覆盖结果文件，因此再次运行上面实例会提示出错，需要先将 ./output 删除。</span><br><span class="line"></span><br><span class="line">rm -r ./output</span><br></pre></td></tr></table></figure><h5 id="伪分布式安装配置"><a href="#伪分布式安装配置" class="headerlink" title="伪分布式安装配置"></a>伪分布式安装配置</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https:&#x2F;&#x2F;dblab.xmu.edu.cn&#x2F;blog&#x2F;7&#x2F;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Hadoop 可以在单节点上以伪分布式的方式运行，Hadoop 进程以分离的 Java 进程来运行，节点既作为 NameNode 也作为 DataNode，同时，读取的是 HDFS 中的文件。</span><br><span class="line"></span><br><span class="line">Hadoop 的配置文件位于 /usr/local/hadoop/etc/hadoop/ 中，伪分布式需要修改2个配置文件 core-site.xml 和 hdfs-site.xml 。Hadoop的配置文件是 xml 格式，每个配置以声明 property 的 name 和 value 的方式来实现。</span><br><span class="line"></span><br><span class="line">修改配置文件 core-site.xml (通过 gedit 编辑会比较方便: gedit ./etc/hadoop/core-site.xml)，将当中的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br><span class="line"></span><br><span class="line">修改为下面配置：</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;file:&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;tmp&lt;&#x2F;value&gt;</span><br><span class="line">        &lt;description&gt;Abase for other temporary directories.&lt;&#x2F;description&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;hdfs:&#x2F;&#x2F;localhost:9000&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br><span class="line"></span><br><span class="line">•hadoop.tmp.dir表示存放临时数据的目录，即包括NameNode的数据，也</span><br><span class="line">包括DataNode的数据。该路径任意指定，只要实际存在该文件夹即可</span><br><span class="line">•name为fs.defaultFS的值，表示hdfs路径的逻辑名称</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">同样的，修改配置文件 hdfs-site.xml：</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line">•dfs.replication表示副本的数量，伪分布式要设置为1</span><br><span class="line">•dfs.namenode.name.dir表示本地磁盘目录，是存储fsimage文件的地方</span><br><span class="line">•dfs.datanode.data.dir表示本地磁盘目录，HDFS数据存放block的地方</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Hadoop配置文件说明:</span><br><span class="line"></span><br><span class="line">Hadoop 的运行方式是由配置文件决定的（运行 Hadoop 时会读取配置文件），因此如果需要从伪分布式模式切换回非分布式模式，需要删除 core-site.xml 中的配置项。</span><br><span class="line"></span><br><span class="line">此外，伪分布式虽然只需要配置 fs.defaultFS 和 dfs.replication 就可以运行（官方教程如此），不过若没有配置 hadoop.tmp.dir 参数，则默认使用的临时目录为 &#x2F;tmp&#x2F;hadoo-hadoop，而这个目录在重启时有可能被系统清理掉，导致必须重新执行 format 才行。所以我们进行了设置，同时也指定 dfs.namenode.name.dir 和 dfs.datanode.data.dir，否则在接下来的步骤中可能会出错</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">配置完成后，执行 NameNode 的格式化:</span><br><span class="line"></span><br><span class="line">cd /usr/local/hadoop</span><br><span class="line">./bin/hdfs namenode -format</span><br><span class="line"></span><br><span class="line">成功的话，会看到 "successfully formatted" 和 "Exitting with status 0" 的提示，若为 "Exitting with status 1" 则是出错。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果已经按照前面教程在.bashrc文件中设置了JAVA_HOME，还是出现 Error: JAVA_HOME is not set and could not be found. 的错误，那么，请到hadoop的安装目录修改配置文件“&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;etc&#x2F;hadoop&#x2F;hadoop-env.sh”，在里面找到“export JAVA_HOME&#x3D;$&#123;JAVA_HOME&#125;”这行，然后，把它修改成JAVA安装路径的具体地址，比如，“export JAVA_HOME&#x3D;&#x2F;usr&#x2F;lib&#x2F;jvm&#x2F;default-java”，然后，再次启动Hadoop。</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">接着开启 NameNode 和 DataNode 守护进程。</span><br><span class="line"></span><br><span class="line">cd /usr/local/hadoop</span><br><span class="line">./sbin/start-dfs.sh  #start-dfs.sh是个完整的可执行文件，中间没有空格</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">启动时可能会出现如下 WARN 提示：WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable WARN 提示可以忽略，并不会影响正常使用。</span><br><span class="line"></span><br><span class="line">启动 Hadoop 时提示 Could not resolve hostname:</span><br><span class="line"></span><br><span class="line">如果启动 Hadoop 时遇到输出非常多“ssh: Could not resolve hostname xxx”的异常情况，如下图所示：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302250211763.png" alt="启动Hadoop时的异常提示"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这个并不是 ssh 的问题，可通过设置 Hadoop 环境变量来解决。首先按键盘的 ctrl + c 中断启动，然后在 ~&#x2F;.bashrc 中，增加如下两行内容（设置过程与 JAVA_HOME 变量一样，其中 HADOOP_HOME 为 Hadoop 的安装目录）：</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;hadoop</span><br><span class="line">export HADOOP_COMMON_LIB_NATIVE_DIR&#x3D;$HADOOP_HOME&#x2F;lib&#x2F;native</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">保存后，务必执行 source ~&#x2F;.bashrc 使变量设置生效，然后再次执行 .&#x2F;sbin&#x2F;start-dfs.sh 启动 Hadoop。</span><br><span class="line"></span><br><span class="line">启动完成后，可以通过命令 jps 来判断是否成功启动，若成功启动则会列出如下进程: &quot;NameNode&quot;、&quot;DataNode&quot; 和 &quot;SecondaryNameNode&quot;（如果 SecondaryNameNode 没有启动，请运行 sbin&#x2F;stop-dfs.sh 关闭进程，然后再次尝试启动尝试）。如果没有 NameNode 或 DataNode ，那就是配置不成功，请仔细检查之前步骤，或通过查看启动日志排查原因。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Hadoop无法正常启动的解决方法: 一般可以查看启动日志来排查原因，注意几点：</span><br><span class="line"></span><br><span class="line">启动时会提示形如 &quot;DBLab-XMU: starting namenode, logging to &#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;logs&#x2F;hadoop-hadoop-namenode-DBLab-XMU.out&quot;，其中 DBLab-XMU 对应你的机器名，但其实启动日志信息是记录在 &#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;logs&#x2F;hadoop-hadoop-namenode-DBLab-XMU.log 中，所以应该查看这个后缀为 .log 的文件；</span><br><span class="line">每一次的启动日志都是追加在日志文件之后，所以得拉到最后面看，对比下记录的时间就知道了。</span><br><span class="line">一般出错的提示在最后面，通常是写着 Fatal、Error、Warning 或者 Java Exception 的地方。</span><br><span class="line">可以在网上搜索一下出错信息，看能否找到一些相关的解决方法。</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">此外，若是 DataNode 没有启动，可尝试如下的方法（注意这会删除 HDFS 中原有的所有数据，如果原有的数据很重要请不要这样做）</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 针对 DataNode 没法启动的解决方法</span></span><br><span class="line">cd /usr/local/hadoop</span><br><span class="line">./sbin/stop-dfs.sh   # 关闭</span><br><span class="line">rm -r ./tmp     # 删除 tmp 文件，注意这会删除 HDFS 中原有的所有数据</span><br><span class="line">./bin/hdfs namenode -format   # 重新格式化 NameNode</span><br><span class="line">./sbin/start-dfs.sh  # 重启</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">成功启动后，可以访问 Web 界面 http:&#x2F;&#x2F;localhost:50070 查看 NameNode 和 Datanode 信息，还可以在线查看 HDFS 中的文件。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302250216043.png" alt="Hadoop的Web界面"></p><h6 id="运行Hadoop伪分布式实例"><a href="#运行Hadoop伪分布式实例" class="headerlink" title="运行Hadoop伪分布式实例"></a>运行Hadoop伪分布式实例</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">上面的单机模式，grep 例子读取的是本地数据，伪分布式读取的则是 HDFS 上的数据。要使用 HDFS，首先需要在 HDFS 中创建用户目录：</span><br><span class="line"></span><br><span class="line">./bin/hdfs dfs -mkdir -p /user/hadoop</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">注意: 教材《大数据技术原理与应用》的命令是以&quot;.&#x2F;bin&#x2F;hadoop dfs&quot;开头的Shell命令方式，实际上有三种shell命令方式。</span><br><span class="line">1. hadoop fs</span><br><span class="line">2. hadoop dfs</span><br><span class="line">3. hdfs dfs</span><br><span class="line"></span><br><span class="line">hadoop fs适用于任何不同的文件系统，比如本地文件系统和HDFS文件系统</span><br><span class="line">hadoop dfs只能适用于HDFS文件系统</span><br><span class="line">hdfs dfs跟hadoop dfs的命令作用一样，也只能适用于HDFS文件系统</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">接着将 ./etc/hadoop 中的 xml 文件作为输入文件复制到分布式文件系统中，即将 /usr/local/hadoop/etc/hadoop 复制到分布式文件系统中的 /user/hadoop/input 中。我们使用的是 hadoop 用户，并且已创建相应的用户目录 /user/hadoop ，因此在命令中就可以使用相对路径如 input，其对应的绝对路径就是 /user/hadoop/input:</span><br><span class="line"></span><br><span class="line">./bin/hdfs dfs -mkdir input</span><br><span class="line">./bin/hdfs dfs -put ./etc/hadoop/*.xml input</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">复制完成后，可以通过如下命令查看文件列表：</span><br><span class="line"></span><br><span class="line">./bin/hdfs dfs -ls input</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">伪分布式运行 MapReduce 作业的方式跟单机模式相同，区别在于伪分布式读取的是HDFS中的文件（可以将单机步骤中创建的本地 input 文件夹，输出结果 output 文件夹都删掉来验证这一点）。</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar grep input output 'dfs[a-z.]+'</span><br><span class="line"></span><br><span class="line">查看运行结果的命令（查看的是位于 HDFS 中的输出结果）：</span><br><span class="line">./bin/hdfs dfs -cat output/*</span><br><span class="line"></span><br><span class="line">结果如下，注意到刚才我们已经更改了配置文件，所以运行结果不同。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302250229448.png" alt="Hadoop伪分布式运行grep结果"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">我们也可以将运行结果取回到本地：</span><br><span class="line">rm -r ./output    # 先删除本地的 output 文件夹（如果存在）</span><br><span class="line">./bin/hdfs dfs -get output ./output     # 将 HDFS 上的 output 文件夹拷贝到本机</span><br><span class="line">cat ./output/*</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Hadoop 运行程序时，输出目录不能存在，否则会提示错误 "org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://localhost:9000/user/hadoop/output already exists" ，因此若要再次执行，需要执行如下命令删除 output 文件夹:</span><br><span class="line"></span><br><span class="line">./bin/hdfs dfs -rm -r output    # 删除 output 文件夹</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">运行程序时，输出目录不能存在: 运行 Hadoop 程序时，为了防止覆盖结果，程序指定的输出目录（如 output）不能存在，否则会提示错误，因此运行前需要先删除输出目录。在实际开发应用程序时，可考虑在程序中加上如下代码，能在每次运行时自动删除输出目录，避免繁琐的命令行操作：</span><br><span class="line"></span><br><span class="line">Configuration conf = new Configuration();</span><br><span class="line">Job job = new Job(conf);</span><br><span class="line"> </span><br><span class="line">/* 删除输出目录 */</span><br><span class="line">Path outputPath = new Path(args[1]);</span><br><span class="line">outputPath.getFileSystem(conf).delete(outputPath, true);</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">若要关闭 Hadoop，则运行</span><br><span class="line">./sbin/stop-dfs.sh</span><br><span class="line"></span><br><span class="line">注意: 下次启动 hadoop 时，无需进行 NameNode 的初始化，只需要运行 ./sbin/start-dfs.sh 就可以！</span><br></pre></td></tr></table></figure><h2 id="Hadoop集群的部署与使用"><a href="#Hadoop集群的部署与使用" class="headerlink" title="Hadoop集群的部署与使用"></a>Hadoop集群的部署与使用</h2><h3 id="集群节点类型"><a href="#集群节点类型" class="headerlink" title="集群节点类型"></a>集群节点类型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">•Hadoop框架中最核心的设计是为海量数据提供存储的HDFS和对数据进行计算的MapReduce</span><br><span class="line">•MapReduce的作业主要包括：（1）从磁盘或从网络读取数据，即IO密集工作；（2）计算数据，即CPU密集工作</span><br><span class="line">•Hadoop集群的整体性能取决于CPU、内存、网络以及存储之间的性能平衡。因此运营团队在选择机器配置时要针对不同的工作节点选择合适硬件类型</span><br><span class="line"></span><br><span class="line">•一个基本的Hadoop集群中的节点主要有</span><br><span class="line">•NameNode：负责协调集群中的数据存储</span><br><span class="line">•DataNode：存储被拆分的数据块</span><br><span class="line">•JobTracker：协调数据计算任务</span><br><span class="line">•TaskTracker：负责执行由JobTracker指派的任务</span><br><span class="line">•SecondaryNameNode：帮助NameNode收集文件系统运行的状态信息</span><br></pre></td></tr></table></figure><h3 id="集群规模"><a href="#集群规模" class="headerlink" title="集群规模"></a>集群规模</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•Hadoop集群规模可大可小，初始时，可以从一个较小规模的集群开始，比如包含10个节点，然后，规模随着存储器和计算需求的扩大而扩大</span><br><span class="line">•如果数据每周增大1TB，并且有三个HDFS副本，然后每周需要一个额外的3TB作为原始数据存储。要允许一些中间文件和日志（假定30%）的空间，由此，可以算出每周大约需要增加一台新机器。存储两年数据的集群，大约需要100台机器</span><br><span class="line">•对于一个小的集群，名称节点（NameNode）和JobTracker运行在单个节点上，通常是可以接受的。但是，随着集群和存储在HDFS中的文件数量的增加，名称节点需要更多的主存，这时，名称节点和JobTracker就需要运行在不同的节点上</span><br><span class="line">•第二名称节点（SecondaryNameNode）会和名称节点可以运行在相同的机器上，但是，由于第二名称节点和名称节点几乎具有相同的主存需求，因此，二者最好运行在不同节点上</span><br></pre></td></tr></table></figure><h3 id="集群硬件配置"><a href="#集群硬件配置" class="headerlink" title="集群硬件配置"></a>集群硬件配置</h3><h3 id="集群网络拓扑"><a href="#集群网络拓扑" class="headerlink" title="集群网络拓扑"></a>集群网络拓扑</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">•普通的Hadoop集群结构由一个两阶网络构成</span><br><span class="line">•每个机架（Rack）有30-40个服务器，配置一个1GB的交换机，并向上传输到一个核心交换机或者路由器（1GB或以上）</span><br><span class="line">•在相同的机架中的节点间的带宽的总和，要大于不同机架间的节点间的带宽总和</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302251651696.png" alt="image-20230225165105566"></p><h3 id="集群的建立与安装"><a href="#集群的建立与安装" class="headerlink" title="集群的建立与安装"></a>集群的建立与安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">采购好相关的硬件设备后，就可以把硬件装入机架，安装并运行Hadoop安装Hadoop有多种方法：</span><br><span class="line">•（1）手动安装</span><br><span class="line">•（2）自动化安装</span><br><span class="line">•为了缓解安装和维护每个节点上相同的软件的负担，可以使用一个自动化方法实现完全自动化安装，比如Red Hat Linux’ Kickstart、Debian或者Docker</span><br><span class="line">•自动化安装部署工具，会通过记录在安装过程中对于各个选项的回答来完成自动化安装过程</span><br></pre></td></tr></table></figure><h3 id="集群基准测试"><a href="#集群基准测试" class="headerlink" title="集群基准测试"></a>集群基准测试</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•如何判断一个Hadoop集群是否已经正确安装？可以运行基准测试</span><br><span class="line">•Hadoop自带有一些基准测试程序，被打包在测试程序JAR文件中</span><br><span class="line">•用TestDFSIO基准测试，来测试HDFS的IO性能</span><br><span class="line">•用排序测试MapReduce：Hadoop自带一个部分排序的程序，这个测试过程的整个数据集都会通过洗牌（Shuffle）传输至Reducer，可以充分测试MapReduce的性能</span><br></pre></td></tr></table></figure><h3 id="在云计算环境中使用Hadoop"><a href="#在云计算环境中使用Hadoop" class="headerlink" title="在云计算环境中使用Hadoop"></a>在云计算环境中使用Hadoop</h3><h2 id="Ubuntu命令"><a href="#Ubuntu命令" class="headerlink" title="Ubuntu命令"></a>Ubuntu命令</h2><h3 id="sudo"><a href="#sudo" class="headerlink" title="sudo"></a>sudo</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">添加用户</span><br><span class="line">sudo useradd -m hadoop -s /bin/bash</span><br><span class="line">为用户设置密码</span><br><span class="line">sudo passwd hadoop</span><br><span class="line">给普通用户权限</span><br><span class="line">sudo adduser hadoop sudo</span><br></pre></td></tr></table></figure><h3 id="apt-get"><a href="#apt-get" class="headerlink" title="apt-get"></a>apt-get</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">更新一下 apt，后续我们使用 apt 安装软件，如果没更新可能有一些软件安装不了。</span><br><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;Hash校验和不符&quot; 的提示，可通过更改软件源来解决 的解决方法</span><br></pre></td></tr></table></figure><h3 id="vim"><a href="#vim" class="headerlink" title="vim"></a>vim</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install vim  vim（vi增强版，基本用法相同），建议安装一下</span><br><span class="line">（如果你实在还不会用 vi&#x2F;vim 的，请将后面用到 vim 的地方改为 gedit，这样可以使用文本编辑器进行修改，并且每次文件更改完成后请关闭整个 gedit 程序，否则会占用终端）</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">正常模式</span><br><span class="line">正常模式主要用来浏览文本内容。一开始打开vim都是正常模式。在任何模式下按下Esc键就可以返回正常模式</span><br><span class="line">插入编辑模式</span><br><span class="line">插入编辑模式则用来向文本中添加内容的。在正常模式下，输入i键即可进入插入编辑模式</span><br><span class="line">退出vim</span><br><span class="line">如果有利用vim修改任何的文本，一定要记得保存。Esc键退回到正常模式中，然后输入:wq即可保存文本并退出vim</span><br></pre></td></tr></table></figure><h3 id="安装SSH、配置SSH无密码登陆"><a href="#安装SSH、配置SSH无密码登陆" class="headerlink" title="安装SSH、配置SSH无密码登陆"></a>安装SSH、配置SSH无密码登陆</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">集群、单节点模式都需要用到 SSH 登陆（类似于远程登陆，你可以登录某台 Linux 主机，并且在上面运行命令），Ubuntu 默认已安装了 SSH client，此外还需要安装 SSH server</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install openssh-server</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">安装后，可以使用如下命令登陆本机：</span><br><span class="line">ssh localhost</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">但这样登陆是需要每次输入密码的，我们需要配置成SSH无密码登陆比较方便。</span><br><span class="line"></span><br><span class="line">首先退出刚才的 ssh，就回到了我们原先的终端窗口，然后利用 ssh-keygen 生成密钥，并将密钥加入到授权中：</span><br><span class="line">exit                           # 退出刚才的 ssh localhost</span><br><span class="line">cd ~&#x2F;.ssh&#x2F;                     # 若没有该目录，请先执行一次ssh localhost</span><br><span class="line">ssh-keygen -t rsa              # 会有提示，都按回车就可以</span><br><span class="line">cat .&#x2F;id_rsa.pub &gt;&gt; .&#x2F;authorized_keys  # 加入授权</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">~的含义: 在 Linux 系统中，~ 代表的是用户的主文件夹，即 "/home/用户名" 这个目录，如你的用户名为 hadoop，则 ~ 就代表 "/home/hadoop/"。 此外，命令中的 # 后面的文字是注释，只需要输入前面命令即可。</span><br><span class="line"></span><br><span class="line">此时再用 ssh localhost 命令，无需输入密码就可以直接登陆了</span><br></pre></td></tr></table></figure><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">命令 含义</span><br><span class="line">cd &#x2F;home&#x2F;hadoop #把&#x2F;home&#x2F;hadoop设置为当前目录</span><br><span class="line">cd .. #返回上一级目录</span><br><span class="line">cd ~ #进入到当前Linux系统登录用户的主目录（或主文件夹）。在 Linux 系统中，~代表的是用户的主文件夹，即“&#x2F;home&#x2F;用户名”这个目录，如果当前登录用户名为 hadoop，则~就代表“&#x2F;home&#x2F;hadoop&#x2F;”这个目录</span><br><span class="line">ls #查看当前目录中的文件</span><br><span class="line">ls -l #查看文件和目录的权限信息</span><br><span class="line">mkdir input #在当前目录下创建input子目录</span><br><span class="line">mkdir -p src&#x2F;main&#x2F;scala #在当前目录下，创建多级子目录src&#x2F;main&#x2F;scala</span><br><span class="line">cat &#x2F;proc&#x2F;version #查看Linux系统内核版本信息</span><br><span class="line">cat &#x2F;home&#x2F;hadoop&#x2F;word.txt #把&#x2F;home&#x2F;hadoop&#x2F;word.txt这个文件全部内容显示到屏幕上</span><br><span class="line">cat file1 file2 &gt; file3 #把当前目录下的file1和file2两个文件进行合并生成文件file3</span><br><span class="line">head -5 word.txt #把当前目录下的word.txt文件中的前5行内容显示到屏幕上</span><br><span class="line">cp &#x2F;home&#x2F;hadoop&#x2F;word.txt &#x2F;usr&#x2F;local&#x2F; #把&#x2F;home&#x2F;hadoop&#x2F;word.txt文件复制到“&#x2F;usr&#x2F;local”目录下</span><br><span class="line">rm .&#x2F;word.txt #删除当前目录下的word.txt文件</span><br><span class="line">rm –r .&#x2F;test #删除当前目录下的test目录及其下面的所有文件</span><br><span class="line">rm –r test* #删除当面目录下所有以test开头的目录和文件</span><br><span class="line"></span><br><span class="line">tar -zxf ~&#x2F;下载&#x2F;spark-2.1.0.tgz -C &#x2F;usr&#x2F;local&#x2F; #把spark-2.1.0.tgz这个压缩文件解压到&#x2F;usr&#x2F;local目录下</span><br><span class="line">mv spark-2.1.0 spark #把spark-2.1.0目录重新命名为spark</span><br><span class="line">chown -R hadoop:hadoop .&#x2F;spark # hadoop是当前登录Linux系统的用户名，把当前目录下的spark子目录的所有权限，赋予给用户hadoop</span><br><span class="line">ifconfig #查看本机IP地址信息</span><br><span class="line">exit #退出并关闭Linux终端</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第一章 大数据概述</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A6%82%E8%BF%B0.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A6%82%E8%BF%B0.html</id>
    <published>2023-02-23T13:09:33.000Z</published>
    <updated>2023-02-24T08:08:46.184Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第一章-大数据概述"><a href="#第一章-大数据概述" class="headerlink" title="第一章 大数据概述"></a>第一章 大数据概述</h1><h2 id="大数据时代"><a href="#大数据时代" class="headerlink" title="大数据时代"></a>大数据时代</h2><h3 id="三次信息化浪潮"><a href="#三次信息化浪潮" class="headerlink" title="三次信息化浪潮"></a>三次信息化浪潮</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">根据IBM前首席执行官郭士纳的观点，IT领域每隔十五年就会迎来一次重大变革</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第一次  1980前后 个人计算机 信息处理</span><br><span class="line">第二次  1995前后 互联网  信息传输</span><br><span class="line">第三次  2010前后 物联网、大数据、云计算 信息爆炸</span><br></pre></td></tr></table></figure><h3 id="信息科技为大数据时代提供技术支撑"><a href="#信息科技为大数据时代提供技术支撑" class="headerlink" title="信息科技为大数据时代提供技术支撑"></a>信息科技为大数据时代提供技术支撑</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">存储成本降低</span><br><span class="line">cpu计算速度提升</span><br><span class="line">网络带宽不断提升</span><br></pre></td></tr></table></figure><h3 id="信息产生方式的变革促成大数据时代的来临"><a href="#信息产生方式的变革促成大数据时代的来临" class="headerlink" title="信息产生方式的变革促成大数据时代的来临"></a>信息产生方式的变革促成大数据时代的来临</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">运营式系统阶段(数据库管理数据)&#x3D;》用户原创内容阶段&#x3D;》感知式系统阶段</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302241553509.png" alt="image-20230224155322411"></p><h3 id="大数据的发展历程"><a href="#大数据的发展历程" class="headerlink" title="大数据的发展历程"></a>大数据的发展历程</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302241554293.png" alt="image-20230224155412228"></p><h2 id="大数据概念"><a href="#大数据概念" class="headerlink" title="大数据概念"></a>大数据概念</h2><h3 id="Volume大量化"><a href="#Volume大量化" class="headerlink" title="Volume大量化"></a>Volume大量化</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1byte&#x3D;8bit</span><br><span class="line">1kb&#x3D;1024byte</span><br><span class="line">1mb&#x3D;1024kb</span><br><span class="line">1GB&#x3D;1024mb</span><br><span class="line">1TB&#x3D;1024GB</span><br><span class="line">1PB&#x3D;1024TB</span><br><span class="line">1EB&#x3D;1024PB</span><br><span class="line">1ZB&#x3D;1024EB</span><br><span class="line">1YB&#x3D;1024ZB</span><br></pre></td></tr></table></figure><h3 id="Velocity迅速化"><a href="#Velocity迅速化" class="headerlink" title="Velocity迅速化"></a>Velocity迅速化</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">从数据的生成到消耗，时间窗口非常小，可用于生成决策的时间非常少</span><br><span class="line"></span><br><span class="line">1秒定律：这一点也是和传统的数据挖掘技术有着本质的不同</span><br></pre></td></tr></table></figure><h3 id="Variety多样化"><a href="#Variety多样化" class="headerlink" title="Variety多样化"></a>Variety多样化</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">大数据是由结构化和非结构化数据组成的</span><br><span class="line">– 10%的结构化数据，存储在数据库中</span><br><span class="line">– 90%的非结构化数据，它们与人类信</span><br><span class="line">息密切相关</span><br></pre></td></tr></table></figure><h3 id="Value价值密度低，商业价值高"><a href="#Value价值密度低，商业价值高" class="headerlink" title="Value价值密度低，商业价值高"></a>Value价值密度低，商业价值高</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">以视频为例，连续不间断监控过程中，可能有用的数据仅仅有一两秒，但是具有很高的商业价值</span><br></pre></td></tr></table></figure><h2 id="大数据的影响"><a href="#大数据的影响" class="headerlink" title="大数据的影响"></a>大数据的影响</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在思维方式方面，大数据完全颠覆了传统的思维方式：</span><br><span class="line">– 全样而非抽样</span><br><span class="line">– 效率而非精确</span><br><span class="line">– 相关而非因果</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在社会发展方面，大数据决策逐渐成为一种新的决策方式，大数据应用有力促进了信息技术与各行业的深度融合，大数据开发大大推动了新技术和新应用的不断涌现</span><br><span class="line"></span><br><span class="line">在就业市场方面，大数据的兴起使得数据科学家成为热门职业</span><br><span class="line"></span><br><span class="line">在人才培养方面，大数据的兴起，将在很大程度上改变中国高校信息技术相关专业的现有教学和科研体制</span><br></pre></td></tr></table></figure><h2 id="大数据关键技术"><a href="#大数据关键技术" class="headerlink" title="大数据关键技术"></a>大数据关键技术</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302240257655.png" alt="image-20230224025717471"></p><h3 id="两大核心技术"><a href="#两大核心技术" class="headerlink" title="两大核心技术"></a>两大核心技术</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">分布式存储</span><br><span class="line"></span><br><span class="line">分布式处理</span><br></pre></td></tr></table></figure><h2 id="大数据计算模式及其产品"><a href="#大数据计算模式及其产品" class="headerlink" title="大数据计算模式及其产品"></a>大数据计算模式及其产品</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302240256592.png" alt="image-20230224025628429"></p><h2 id="大数据产业"><a href="#大数据产业" class="headerlink" title="大数据产业"></a>大数据产业</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">大数据产业是指一切与支撑大数据组织管理和价值发现相关的企业经济活动的集合</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302240250827.png" alt="image-20230224025055674"></p><h2 id="大数据与云计算、物联网的关系"><a href="#大数据与云计算、物联网的关系" class="headerlink" title="大数据与云计算、物联网的关系"></a>大数据与云计算、物联网的关系</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">云计算、大数据和物联网代表了IT领域最新的技术发展趋势，三者相辅相成，既有联系又有区别</span><br></pre></td></tr></table></figure><h3 id="云计算"><a href="#云计算" class="headerlink" title="云计算"></a>云计算</h3><h4 id="云计算概念"><a href="#云计算概念" class="headerlink" title="云计算概念"></a>云计算概念</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">云计算实现了通过网络提供可伸缩的、廉价的分布式计算能力，用户只需要在具备网络接入条件的地方，就可以随时随地获得所需的各种IT资源</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302240218423.png" alt="image-20230224021822364"></p><h4 id="云计算关键技术"><a href="#云计算关键技术" class="headerlink" title="云计算关键技术"></a>云计算关键技术</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">云计算关键技术包括：虚拟化、分布式存储、分布式计算、多租户等</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302240220141.png" alt="image-20230224022054090"></p><h4 id="云计算数据中心"><a href="#云计算数据中心" class="headerlink" title="云计算数据中心"></a>云计算数据中心</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">云计算数据中心是一整套复杂的设施，包括刀片服务器、宽带网络连接、环境控制设备、监控设备以及各种安全装置等</span><br><span class="line">• 数据中心是云计算的重要载体，为云计算提供计算、存储、带宽等各种硬件资源，为各种平台和应用提供运行支撑环境</span><br><span class="line">• 全国各地推进数据中心建设</span><br></pre></td></tr></table></figure><h4 id="云计算应用"><a href="#云计算应用" class="headerlink" title="云计算应用"></a>云计算应用</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">• 政务云上可以部署公共安全管理、容灾备份、城市管理、应急管理、智能交通、社会保障等应用，通过集约化建设、管理和运行，可以实现信</span><br><span class="line">息资源整合和政务资源共享，推动政务管理创新，加快向服务型政府转</span><br><span class="line">型</span><br><span class="line">• 教育云可以有效整合幼儿教育、中小学教育、高等教育以及继续教育等优质教育资源，逐步实现教育信息共享、教育资源共享及教育资源深度</span><br><span class="line">挖掘等目标</span><br><span class="line">• 中小企业云能够让企业以低廉的成本建立财务、供应链、客户关系等管理应用系统，大大降低企业信息化门槛，迅速提升企业信息化水平，增</span><br><span class="line">强企业市场竞争力</span><br><span class="line">• 医疗云可以推动医院与医院、医院与社区、医院与急救中心、医院与家庭之间的服务共享，并形成一套全新的医疗健康服务系统，从而有效地</span><br><span class="line">提高医疗保健的质量</span><br></pre></td></tr></table></figure><h3 id="物联网"><a href="#物联网" class="headerlink" title="物联网"></a>物联网</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">物联网是物物相连的互联网，是互联网的延伸，它利用局部网络或互联网等通信技术把传感器、控制器、机器、人员和物等通过新的方式联在一起，形成人与物、物与物相联，实现信息化和远程管理控制</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302240232718.png" alt="image-20230224023256443"></p><h4 id="物联网关键技术"><a href="#物联网关键技术" class="headerlink" title="物联网关键技术"></a>物联网关键技术</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">物联网中的关键技术包括识别和感知技术（二维码、RFID、传感器等）、网络与通信技术、数据挖掘与融合技术等</span><br></pre></td></tr></table></figure><h4 id="物联网的应用"><a href="#物联网的应用" class="headerlink" title="物联网的应用"></a>物联网的应用</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">物联网已经广泛应用于智能交通、智慧医疗、智能家居、环保监测、智能安防、智能物流、智能电网、智慧农业、智能工业等领域，对国民经济与社会发展起到了重要的推动作用</span><br></pre></td></tr></table></figure><h4 id="物联网产业"><a href="#物联网产业" class="headerlink" title="物联网产业"></a>物联网产业</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">完整的物联网产业链主要包括核心感应器件提供商、感知层末端设备提供商、网络提供商、软件与行业解决方案提供商、系统集成商、运</span><br><span class="line">营及服务提供商等六大环节</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302240242090.png" alt="image-20230224024224902"></p><h3 id="大数据与云计算、物联网的关系-1"><a href="#大数据与云计算、物联网的关系-1" class="headerlink" title="大数据与云计算、物联网的关系"></a>大数据与云计算、物联网的关系</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">云计算、大数据和物联网代表了IT领域最新的技术发展趋势，三者既</span><br><span class="line">有区别又有联系</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302240244662.png" alt="image-20230224024448474"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>找工作遇到的常识问题</title>
    <link href="http://tianyong.fun/%E6%89%BE%E5%B7%A5%E4%BD%9C%E9%81%87%E5%88%B0%E7%9A%84%E5%B8%B8%E8%AF%86%E9%97%AE%E9%A2%98.html"/>
    <id>http://tianyong.fun/%E6%89%BE%E5%B7%A5%E4%BD%9C%E9%81%87%E5%88%B0%E7%9A%84%E5%B8%B8%E8%AF%86%E9%97%AE%E9%A2%98.html</id>
    <published>2023-02-05T07:16:09.000Z</published>
    <updated>2023-02-23T12:30:20.885Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="找工作遇到的常识问题"><a href="#找工作遇到的常识问题" class="headerlink" title="找工作遇到的常识问题"></a>找工作遇到的常识问题</h1><h2 id="大数据"><a href="#大数据" class="headerlink" title="大数据"></a>大数据</h2><h3 id="OLAP"><a href="#OLAP" class="headerlink" title="OLAP"></a>OLAP</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">online analytical processing</span><br><span class="line">操作主体一般是运营、销售、市场等团队人员(通过对数据库数据得出结论性的东西)</span><br></pre></td></tr></table></figure><h3 id="OLTP"><a href="#OLTP" class="headerlink" title="OLTP"></a>OLTP</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">online transaction processing</span><br><span class="line">操作主体一般是用户(主要是对数据库数据的增删改查)</span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/weixin_44087159/article/details/124477313" target="_blank" rel="external nofollow noopener noreferrer">参考来源</a></p><h3 id="ETL"><a href="#ETL" class="headerlink" title="ETL"></a>ETL</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">extract transform load</span><br></pre></td></tr></table></figure><h3 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">分布式的</span><br><span class="line">日志采集 聚集 传输系统</span><br></pre></td></tr></table></figure><h3 id="kafka"><a href="#kafka" class="headerlink" title="kafka"></a>kafka</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">分布式的</span><br><span class="line">流处理平台</span><br></pre></td></tr></table></figure><h3 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">历史数据 批处理</span><br><span class="line">实时数据 流处理</span><br><span class="line">flink支持这两种的计算引擎</span><br></pre></td></tr></table></figure><h3 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">是建立在hadoop之上的数据仓库基础框架</span><br></pre></td></tr></table></figure><h3 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">natural language processing</span><br></pre></td></tr></table></figure><h3 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">学它是因为spark要用到，其实也支持java，但scala比java更优</span><br><span class="line">与java类似</span><br></pre></td></tr></table></figure><h3 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HBase是从hadoop中分离出来的nosql系统</span><br></pre></td></tr></table></figure><h3 id="CDH、HDP、CDP"><a href="#CDH、HDP、CDP" class="headerlink" title="CDH、HDP、CDP"></a>CDH、HDP、CDP</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CDH:cloudera&#39;s distribution including apache hadoop</span><br><span class="line">HDP:Hortonworks Data Platform</span><br><span class="line">CDP:cloudera data platform(cloudera和hortonworks合并和推出的产品)</span><br></pre></td></tr></table></figure><p><a href="https://www.cnblogs.com/chhyan-dream/p/16226081.html" target="_blank" rel="external nofollow noopener noreferrer">参考来源</a></p><h3 id="ES"><a href="#ES" class="headerlink" title="ES"></a>ES</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">elasticsearch弹性搜索 搜索和分析引擎</span><br></pre></td></tr></table></figure><h3 id="Aireflow"><a href="#Aireflow" class="headerlink" title="Aireflow"></a>Aireflow</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Airflow是一个以编程方式编写，安排和监视工作流的平台。</span><br><span class="line">使用Airflow将工作流编写任务的有向无环图(DAG)。</span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/weixin_39658434/article/details/100897245" target="_blank" rel="external nofollow noopener noreferrer">参考来源</a></p><p><a href="https://baijiahao.baidu.com/s?id=1727510089901712755&wfr=spider&for=pc" target="_blank" rel="external nofollow noopener noreferrer">参考来源</a></p><h3 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用于解决单点故障的</span><br></pre></td></tr></table></figure><h3 id="Sqoop"><a href="#Sqoop" class="headerlink" title="Sqoop"></a>Sqoop</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">专门将关系型数据库中的数据导入到hadoop平台</span><br></pre></td></tr></table></figure><h3 id="DW、BI"><a href="#DW、BI" class="headerlink" title="DW、BI"></a>DW、BI</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DW:data warehouse</span><br><span class="line">BI:business intelligence 使决策者能够对企业信息进行有效、合理分析和处理、为决策者提供可靠的依据。</span><br></pre></td></tr></table></figure><h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">support vector machine 支持向量机</span><br><span class="line">是一类按监督学习（supervised learning）方式对数据进行二元分类的广义线性分类器（generalized linear classifier），其决策边界是对学习样本求解的最大边距超平面（maximum-margin hyperplane）</span><br></pre></td></tr></table></figure><h3 id="Storm"><a href="#Storm" class="headerlink" title="Storm"></a>Storm</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Storm是Twitter开源的分布式实时大数据处理框架，被业界称为实时版Hadoop。</span><br></pre></td></tr></table></figure><p><a href="https://www.jianshu.com/p/017980b9a25d" target="_blank" rel="external nofollow noopener noreferrer">参考来源</a></p><h3 id="Impala"><a href="#Impala" class="headerlink" title="Impala"></a>Impala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Impala是Cloudera公司主导开发的新型查询系统，它提供SQL语义，能查询存储在Hadoop的HDFS和HBase中的PB级大数据。已有的Hive系统虽然也提供了SQL语义，但由于Hive底层执行使用的是MapReduce引擎，仍然是一个批处理过程，难以满足查询的交互性。相比之下，Impala的最大特点也是最大卖点就是它的快速。</span><br></pre></td></tr></table></figure><h3 id="Elasticsearch"><a href="#Elasticsearch" class="headerlink" title="Elasticsearch"></a>Elasticsearch</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.1普通搜索：搜索，就是在任何场景下，找寻你想要的信息，这个时候，会输入一段你要搜索的关键字，然后就期望找到这个关键字相关的有些信息</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1.2 用数据库做搜索</span><br><span class="line">用数据库来实现搜索，是不太靠谱的。通常来说，性能会很差的。</span><br><span class="line"></span><br><span class="line">会逐条扫描</span><br><span class="line"></span><br><span class="line">拿着关键词去搜索，第一一般无法将关键词拆分开来(如生化机，无法搜索出生化危机)，第二，当记录很多，且数据描述很长的话，效率会非常低</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.3 全文检索和Lucene</span><br><span class="line">(1)全文检索是指计算机索引程序通过扫描文章中的每一个词，对每一个词建立一个索引，指明该词在文章中出现的次数和位置，当用户查询时，检索程序就根据事先建立的索引进行查找(这个过程类似于通过字典中的检索字表查字的过程。)</span><br><span class="line"></span><br><span class="line">(2)lucene，就是一个jar包，里面包含了封装好的各种建立倒排索引，以及进行搜索的代码，包括各种算法。我们就用java开发的时候，引入lucene jar，然后基于lucene的api进行去进行开发就可以了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1.4 什么是Elasticsearch</span><br><span class="line">Elasticsearch，基于lucene，隐藏复杂性，提供简单易用的restful api接口、java api接口（还有其他语言的api接口）。</span><br><span class="line"></span><br><span class="line">Elasticsearch是一个实时分布式搜索和分析引擎。它用于全文搜索、结构化搜索、分析。</span><br><span class="line"></span><br><span class="line">全文检索：将非结构化数据中的一部分信息提取出来,重新组织,使其变得有一定结构,然后对此有一定结构的数据进行搜索,从而达到搜索相对较快的目的。</span><br><span class="line"></span><br><span class="line">结构化检索：我想搜索商品分类为日化用品的商品都有哪些，select * from products where category_id&#x3D;&#39;日化用品&#39;。</span><br></pre></td></tr></table></figure><p><a href="https://zhuanlan.zhihu.com/p/496682762" target="_blank" rel="external nofollow noopener noreferrer">参考</a></p><h3 id="Druid"><a href="#Druid" class="headerlink" title="Druid"></a>Druid</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Druid是一个专为大型数据集上的高性能切片和OLAP分析而设计的数据存储。</span><br><span class="line"></span><br><span class="line">Druid提供低延时的数据插入，实时的数据查询。</span><br></pre></td></tr></table></figure><p><a href="https://baijiahao.baidu.com/s?id=1715905517657199904&wfr=spider&for=pc" target="_blank" rel="external nofollow noopener noreferrer">参考</a></p><h3 id="presto"><a href="#presto" class="headerlink" title="presto"></a>presto</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Presto是一个开源的分布式SQL查询引擎，它以集群的方式运行，采用MPP架构，用在交互式分析查询场景下，可以将多种不同数据量级（从GB到PB）的数据源组合起来进行统一计算。Presto本身只是一个查询引擎，它通过connector的方式完成外部数据源的接入；也就是说通过使用Presto提供的ANSI标准SQL，可以完成多种数据源的标准化计算工作。</span><br></pre></td></tr></table></figure><h3 id="clickhouse"><a href="#clickhouse" class="headerlink" title="clickhouse"></a>clickhouse</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">clickHouse是俄罗斯的 Yandex 公司于 2016 年开源的列式存储数据库，使用 C++ 语言编写；</span><br><span class="line"></span><br><span class="line">一款面向 OLAP 的数据库</span><br><span class="line">ClickHouse支持类SQL语言，提供了传统关系型数据的便利</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">专门用于 OLAP（联机分析处理），其性能惊人；</span><br><span class="line"></span><br><span class="line">高性能面向 OLAP 的数据库，不擅长的方面：</span><br><span class="line"></span><br><span class="line">不支持事务</span><br><span class="line">不擅长根据主键按行粒度进行查询（虽然支持），所以不应该把 ClickHouse 当做键值对数据库使用</span><br><span class="line">不擅长按行删除数据（虽然支持）</span><br><span class="line">对于 OLAP 数据库而言，上述这些能力不是重点，只能说这是为了极致的查询性能所做的权衡。</span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/huazhongkejidaxuezpp/article/details/128457477" target="_blank" rel="external nofollow noopener noreferrer">参考</a></p><h2 id="互联网"><a href="#互联网" class="headerlink" title="互联网"></a>互联网</h2><h3 id="ERP"><a href="#ERP" class="headerlink" title="ERP"></a>ERP</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">eterprise resource planning</span><br></pre></td></tr></table></figure><h3 id="BP"><a href="#BP" class="headerlink" title="BP"></a>BP</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">business planning</span><br></pre></td></tr></table></figure><h3 id="BI"><a href="#BI" class="headerlink" title="BI"></a>BI</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">business intelligence</span><br></pre></td></tr></table></figure><h3 id="Tableau、PowerBi"><a href="#Tableau、PowerBi" class="headerlink" title="Tableau、PowerBi"></a>Tableau、PowerBi</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">一款可视化分析软件</span><br></pre></td></tr></table></figure><h3 id="Flask"><a href="#Flask" class="headerlink" title="Flask"></a>Flask</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">是python编写的web微框架</span><br></pre></td></tr></table></figure><h3 id="OA"><a href="#OA" class="headerlink" title="OA"></a>OA</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Office Automation 办公自动化</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="常识" scheme="http://tianyong.fun/categories/%E5%B8%B8%E8%AF%86/"/>
    
    
      <category term="常识" scheme="http://tianyong.fun/tags/%E5%B8%B8%E8%AF%86/"/>
    
  </entry>
  
</feed>
